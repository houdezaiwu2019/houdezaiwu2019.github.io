{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/01/18/hello-world/"},{"title":"Typora入门","text":"本文主要从目录设置、标题设置、特殊的标记符号、导入公式和列表以及图片图床的设置与排版来进行整理。 目录篇代码：[TOC] 标题篇对于Typora来说，主要分为快捷键方法和一般的直接写代码的方法，这里我比较推荐的还是直接写代码的方法，但是新手入门还是比较推荐先使用快捷键来进行入门。 标题的代码：==#+空格+内容== 这里有几个#就是几级标题 123# 一级标题## 二级标题### 三级标题 标题的快捷键：Ctrl+(数字) Ctrl+0 &lt;=&gt; 一般段落 Ctrl+1 &lt;=&gt; 一级标题 Ctrl+2 &lt;=&gt; 二级标题 连续两次Ctrl+同一个数字：取消标题操作 Ctrl+ +/-：代表标题的升级和降级 标记符号篇标记符号这里主要讲的是加粗、斜体、下滑线、高亮和内置公式与代码。 加粗：快捷键==Ctrl+B==，代码**text** 斜体：快捷键==Ctrl+I==，代码*text* 下滑线：快捷键==Ctrl+U==，代码**&lt;u&gt;text&lt;/u&gt;** 高亮：代码==text== 内置代码：快捷键==Ctrl+Shift+`==，代码`` 代码块：快捷键==Ctrl+Shift+K==， 代码: “```”+语言(c++/python/java) “```” 内置公式：代码\\aplha，这里使用的是latex代码进行标记的。 公式块：快捷键==Ctrl+Shift+M==，代码$$$ $$$ 1234$$ E_k = \\frac{1}{2}mv^2 \\tag{1.1}$$ E_k = \\frac{1}{2}mv^2 \\tag{1.1}引用：快捷键==Ctrl+Shift+Q，代码&gt;+空格+text 书籍是人类进步的阶梯。——高尔基 超级链接：代码[blibli](https://www.bilibili.com) [text](#name) eg1. blibli 的链接 eg2.目录 的链接 脚注：代码text[^name],之后在后面[^name]:text 1,这里添加成功后，后面会出现回车 ↩符号。 上标：代码X^2 下标：代码H~2~o 注意这里需要的一些额外的设置： 列表篇有序列表：快捷键==Ctrl+Shift+[==，代码1.+空格+text 无序列表：快捷键==Ctrl+Shift+]==，代码-+空格+text 注意：无序列表如果想切换到下一级时可以直接按==tab== 表格：快捷键==Ctrl+T==，之后弹出对话框，自己来选择需要的行数和列数。 今天 明天 早饭 午饭 晚饭 上述表格的源代码： 12345| | 今天 | 明天 || :--: | :--- | :--: || 早饭 | | || 午饭 | | || 晚饭 | | | 这里还是用快捷键吧，使用代码太麻烦了。 图片篇本地路径导入图片 一般来说，图片使用的是绝对路径，如下图所示为直接截图并粘贴等到的图片的路径。但这种语言不是md的语言。 &lt;img src=&quot;C:\\Users\\dell\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220118215115730.png&quot; alt=&quot;image-20220118215115730&quot; style=&quot;zoom:50%;&quot; /&gt;。 这里的路径是Typora自行生成的，但实际输入的时候要输入代码![]()。 也可以用相对路径，代码是![](./+name.png/jpg) ./代表的是当前的文件夹。 ../代表的是上一级文件夹。 ![](./pic01.png) 对于.md来说，保存图片一般使用的是保存图片的链接，这样可以减小文件的空间（相比Word）。但这样会造成修改了文件或者图片的路径以及清除的C盘的缓存文件时会造成无法显示的问题。 利用图床导入图片这里有个比较矛盾的问题，使用GitHub的图床的话会造成国内没有VPN的用户无法访问，但是使用一般的七牛云的话要花钱，因此再三考虑我们使用码云gitee来作图床（弊端是图片的大小不能超过1M）。 将插入图片的无任何操作-&gt;上传图片 根据picgo的位置来配置路径。并且这里还要下载一个node.js，下载过程这里省略。 到picgo中下载gitee的插件，这里我们使用的是第三个，当然其他的两个也是可以的，但以下将围绕第三个gitee-uploader来配置。 在gitee里面创建仓库，注意创建时一定要点击初始化，只有这样才可以上传图片。如果没有点，那么创建好仓库时也可以点初始化。 要生成一个token，打开gitee的设置，点击生成令牌。点击设置-&gt;安全设置-&gt;私人令牌 设置权限，起个名字点击生成。注意令牌只弹出一次，最好找个记事本记录下来，防止自己忘记。 配置picgo，打开图床设置-&gt;gitee。 注意，这里自己仓库命名repo格式：用户名/仓库名。 但是之前遇到了问题，上传的图片在本地无法显示，其中的原因是gitee设置的是私有的，可以在仓库的管理界面更改。 Typora图片排版这里图片排版需要我们掌握一点点HTML(Hyper Text Markup Language)的语法知识。 图注 12345678# 这里为上面图片的HTML代码&lt;center&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;br&gt; // 这里&lt;br&gt;的意思是回车，如果没有br会和图片是并排的 图注&lt;/center&gt; 因此，要实现下面两个图片并排的效果，就可以直接将图片的放一起即可，如果实现连个图片的上下排列，这是需要在之间加一个&lt;br&gt;即可 这里的&lt;center&gt;是默认图片居中的意思。导入图片默认的情况就是都是居中的，如果要改成左对齐则将&lt;center&gt;--&gt;&gt;&lt;left&gt;即可实现，同理右对齐就用&lt;right&gt;。 图 晚安 123456789&lt;center&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;br&gt; 图 晚安 &lt;img src=&quot;http://latex.codecogs.com/gif.latex? a^2&quot;&gt;&lt;/center&gt; 这里使用的是latex代码的形式来显示图注，虽然不是很好看，但是只是可以显示。","link":"/2022/01/21/Typora%E5%85%A5%E9%97%A8/"},{"title":"张量的创建与运算","text":"张量是一种特殊的数据结构，它于数组和矩阵很相似。我们用张量来标记模型的输入、输出和模型的参数。 张量（Tensor）和数组非常的相似，但是Tensor可以实现在硬件的加速。 tensors and NumPy 数组可以共享同一个内存。 hljs.initHighlightingOnLoad(); 初始化张量12import torchimport numpy as np 通过列表初始化 12345data = [ [1,2],[3,4] ]x_data = torch.tensor(data)type(data) # &lt;class 'list'&gt;type(x_data) # &lt;class 'torch.Tensor'&gt; 通过数组初始化 1234np_array = np.array(data)x_np = torch.from_numpy(np_array)# 默认的数据类型为 torch.float32 从另一个张量中初始化一个新的张量 1234567891011121314x_ones = torch.ones_like(x_data) # retains the properties of x_dataprint(f&quot;Ones Tensor: \\n {x_ones} \\n&quot;)x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_dataprint(f&quot;Random Tensor: \\n {x_rand} \\n&quot;)output:Ones Tensor: tensor([[1, 1], [1, 1]])Random Tensor: tensor([[0.4557, 0.7406], [0.5935, 0.1859]]) 用随机数或常值生成 12345678shape = ( 2,3,)rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)print(f&quot;Random Tensor: \\n {rand_tensor} \\n&quot;)print(f&quot;Ones Tensor: \\n {ones_tensor} \\n&quot;)print(f&quot;Zeros Tensor: \\n {zeros_tensor}&quot;) 这里的shape可以(2,3) or (2,3,) or [2,3] or [2,3,] Tensor的属性123456789101112tensor = torch.rand(3,4)tensor.shape # 形状 torch.Size([3, 4])tensor.dtype # 数据类型 torch.float32tensor.device # 使用的设备 device(type='cpu')print(f&quot;Shape of tensor: {tensor.shape}&quot;)print(f&quot;Datatype of tensor: {tensor.dtype}&quot;)print(f&quot;Device tensor is stored on: {tensor.device}&quot;)Shape of tensor: Datatype of tensor: torch.float32Device tensor is stored on: cpu Tensor的操作Tensor的主要操作包含100种包含矩阵运算、切片等。具体的操作API点击这里。 常用的操作如下： 关于Tensor的操作 is_tensor(x)判断是否为Tensor 123&gt;&gt;&gt; x=torch.tensor([1,2,3])&gt;&gt;&gt; torch.is_tensor(x)True is_nonzero(x)判断单一元素的Tensor是否为0 12345678910111213141516&gt;&gt;&gt; torch.is_nonzero(torch.tensor([0.]))False&gt;&gt;&gt; torch.is_nonzero(torch.tensor([1.5]))True&gt;&gt;&gt; torch.is_nonzero(torch.tensor([False]))False&gt;&gt;&gt; torch.is_nonzero(torch.tensor([3]))True&gt;&gt;&gt; torch.is_nonzero(torch.tensor([1, 3, 5]))Traceback (most recent call last):...RuntimeError: bool value of Tensor with more than one value is ambiguous&gt;&gt;&gt; torch.is_nonzero(torch.tensor([]))Traceback (most recent call last):...RuntimeError: bool value of Tensor with no values is ambiguous nueml(x)统计张量里的元素数目 123456&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)&gt;&gt;&gt; torch.numel(a)120&gt;&gt;&gt; a = torch.zeros(4,4)&gt;&gt;&gt; torch.numel(a)16 创建Tensor操作 tensor(x)创建张量 from_numpy(a)也是创建张量，但是不同的是此方法的创建的tensor和array是共享内存的。 1234567&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.from_numpy(a)&gt;&gt;&gt; ttensor([ 1, 2, 3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1, 2, 3]) zeros() 创建全0矩阵 torch.zeros(size, , out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 主要参数： *size：形状可以是元组、列表、切片等。 requires_grad=False：是否要求梯度。 dtype=None:默认是None,因为set_default_tensor_type()函数可以设置默认的dtype()类型。 arange()和range()函数都是用于一维计数，配合for来使用，基本的功能也是继承numpy和python的基本语法。 arange-&gt;[start, end)，size=$\\lceil \\frac{end-start}{step} \\rceil$，并且start、end、step —&gt;&gt; int range-&gt;[start, end]，size=$\\lfloor \\frac{end-start}{step} \\rfloor+1$，并且start、end、step —&gt;&gt; float eye(n,m=None) 单位矩阵 full(size,fill_value ) 就是用某个数值来填充某个size的张量。同样的还有full_like() 索引、切片、旋转以及聚合的操作 cat(tensors, dim=0) 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; x1 = torch.randn(2, 3)&gt;&gt;&gt; x2 = torch.randn(2, 2)&gt;&gt;&gt; s = torch.cat((x1,x2),dim=1)&gt;&gt;&gt; stensor([[-0.5980, 0.2570, 1.1660, -0.0306, -1.0363], [ 0.2817, -0.1498, -0.6464, -0.3204, -1.6839]])# 这里dim=1上是不同的dim=0上是相同的，因此可以实现在dim=1上的切片，dim=0是用来对齐的。&gt;&gt;&gt; x3 = torch.randn(3, 2)&gt;&gt;&gt; s = torch.cat([x2,x3],dim=0)&gt;&gt;&gt; stensor([[-0.0306, -1.0363], [-0.3204, -1.6839], [-0.0858, 0.9804], [ 0.1920, -0.1728], [ 1.0318, -1.6507]])","link":"/2022/01/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BF%90%E7%AE%97/"},{"title":"第一章绪论","text":"振动力学是力学专业的重要专业基础课，由此开始进入动力学的研究范畴，在航空航天、机械、土木等许多工程领域有着重要的应用背景。本文为第一章绪论，初步的介绍基本概念、学习目的、问题的提法、振动模型的建立以及系统的分类。 1.1 基本概念与学习目的 一般力学的对象主要是有限自由度系统的运动及其控制，有时它包含一个或多个无限自由度子系统。它包括运动稳定性理论、振动理论、动力系统理论、多体系统力学、机械动力学等。包括理论力学、振动力学、非线性力学。 固体力学是研究可变形固体在外界因素作用下所产生的应力、应变、位移和破坏等的力学分支。固体力学在力学中形成较早，应用也较广。包括材料力学、弹性力学、塑性力学、非线性介质力学。 流体力学是研究流体{液体和气体)的力学运动规律及其应用的学科。主要研究在各种力的作用下，流体本身的状态，以及流体和固体壁面,流体和流体间、流体与其他运动形态之间的相互作用的力学分支。包括流体力学、高等流体力学。 定义： 从广义上讲，如果表征一种运动的物理量作时而增大时而减小的反复变化，就可以称这种运动为振动 如果变化的物理量是一些机械量或力学量，例如物体的位移、速度、加速度、应力及应变等等，这种振动便称为机械振动 振动是自然界最普遍的现象之一 各种物理现象，诸如声、光、热等都包含振动 (1）心脏的搏动、耳膜和声带的振动，(2）桥梁和建筑物在风和地震作用下的振动,(3)飞机和轮船航行中的振动，（4）机床和刀具在加工时的振动 振动力学：借助数学、物理、实验和计算技术，探讨各种振动现象，阐明振动的基本规律，以便克服振动的消极因素，利用其积极因素，为合理解决各种振动问题提供理论依据。 学习的目的： 运用振动理论去创造和设计新型的振动设备、仪器及自动化装置 掌握振动的基本理论和分析方法，用以确定和限制振动对工程结构和机械产品的性能、寿命和安全的有害影响 1.2振动问题的提法 通常的研究对象被称作系统 它可以是一个零部件、一台机器、一个完整的工程结构 外部的激振力等因素被称作激励（输入） 系统发生的振动称为响应（输出） 振动的问题可以分为三类： 已知激励和系统，求响应（正问题） 动力响应分析，一般是建立微分方程，数值方法求解即可。 主要任务在于验算结构、产品等在工作时的动力响应（如变形、位移、应力等）是否满足预定的安全要求和其它要求 在产品设计阶段，对具体设计方案进行动力响应验算，若不符合要求再作修改，直到达到要求而最终确定设计方案，这一过程就是所谓的振动设计 这里比如响应中的反应设计的安全，加速度反应设计的舒适性特性。 高层建筑要有抗震要求，潜艇要有声学要求。 火箭-&gt;细长杆，尾端燃料推进器会给火箭本体造成振动（轴向、弯曲、扭振），从而影响卫星，因此火箭和推进器之间要加减震适配器。 第二类：已知和响应，求系统（第一个问题） 系统识别，系统辨识 求系统，主要是指获得对于系统的物理参数（如质量、刚数等）和系统关于振动的固有特性（如固有频率、主振型等）的认识 以估计物理参数为任务的叫做物理参数辨识，以估计系统振动固有特性为任务的叫做模态参数辨识或者试验模态分析 激励-&gt;卫星的姿态调整；响应-&gt;卫星的传感器（包括陀螺仪、端板上的加速度传感器） 机械臂本身的参数辨识 ; 空间机械臂抓取,输入(激励)为抓取前各个关节的力矩和加速度信息,输出(响应)为抓取后的力矩和加速度—&gt;&gt;得出被抓取的碎片的质量和惯量. 第三类：已知系统和响应，求激励（第二个逆问题） 环境预测 例如:为了避免产品在公路运输中的损坏，需要通过实地行车记录汽车振动和产品振动，以估计运输过程中是怎样的一种振动环境，运输过程对于产品是怎样的一种激励，这样才能有根据地为产品设计可靠的减震包装 对于火箭来说，系统的响应为传感器的输出，系统为火箭，反推激励-&gt;升空过程中的空气振动。 对于坦克来说，发射的炮弹为响应，坦克系统，反推地面环境对系统发射炮弹的激励。 相比之下，逆问题的求解要比正问题复杂，正问题建立ODE和PDE，再用数值方法求解即可。 1.3力学模型 振动系统的三要素：质量、刚度、阻尼 质量是感受惯性（包括转动惯量）的元件 刚度是感受弹性的元件 阻尼是耗能元件 描述振动系统的两类力学模型:（一般是建立数学模型） 连续系统模型（无限多自由度系统，分布参数系统） 结构参数（质量，刚度，阻尼等）在空间上连续分布 数学工具：偏微分方程 求解是将PDE离散化转化为ODE 离散系统模型（多自由度系统，单自由度系统） 结构参数为集中参量 数学工具：常微分方程 1.4振动及系统的分类按照微分方程的形式可以分为： 线性振动：描述其运动的方程为线性微分方程，相应的系统称为线性系统。线性系统的一个重要特性是线性叠加原理成立 非线性振动：描述其运动的方程为非线性微分方程，相应的系统称为非线性系统。对于非线性振动，线性叠加原理不成立 理论上，线性振动有统一的解决方法；而非线性振动就没有统一的方法。 按照激励的有无和性质可以分为： 固有振动：无激励时系统所有可能的运动集合（不是现实的振动，仅反映系统关于振动的固有属性) 自由振动：激励消失后系统所做的振动（现实的振动) 强迫振动：系统在外部激励作用下所做的振动 随机振动：系统在非确定性的随机激励下所做的振动，例如行驶在公路上的汽车的振动，激励无法使用函数来表示，只能用一个随机过程来表示 自激振动：系统受其自身运动诱发出来的激励作用而产生和维持的振动例如提琴发出的乐声，切削加工的高频振动，机翼的颤振等 参数振动：激励以系统本身的参数随时间变化的形式出现的振动，例如秋千被越荡越高。秋千受到的激励以摆长随时间变化的形式出现，而摆长的变化由人体的下蹲及站立造成。","link":"/2022/01/23/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%BB%AA%E8%AE%BA/"},{"title":"Latex公式排版快速教程","text":"src=\"//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js\"> hljs.initHighlightingOnLoad(); 本文主要关注Latex在公式排版的技巧，并且配合Markdown更好的编辑公式。主要包含希腊字母、上下标、分式与根式、运算符、标注、箭头、括号与定界符、多行公式、大括号、矩阵以及实战书写几个公式。 typora有些latex字符是不识别的，因此可能出现一些乱码问题。 行内公式创建行内的公式的方法有三种： 美元号：a+b=b+a 小括号：\\( a+b=b+a\\) math环境：\\begin{math} a+b=b+a \\end{math} 行间公式创建行间公式的主要方法有： 双美元号：$$ $$ 中括号：\\[a+b=b+a\\] displaymath环境：\\begin{displaymath} a+b=b+a \\end{displaymath} equation环境(自动生成编号)：\\begin{equation} a+b=b+a \\end{equation}，同时可以对生成的公式用\\label进行交叉引用，如下图所示。 equation*环境(不自动生成编号)：\\begin{equation*} a+b=b+a \\end{equation*}，同时可以对生成的公式用\\label进行交叉引用，如下图所示。 此时交叉引用的编号为小结的编号。 带星号的equation环境需要使用amsmath宏包。 1\\usepackage{amsmath} 希腊字母输入方法：\\+字母的拼写 常用的希腊字母表 对于有大写的字母来说，直接将首字母大写即可得到大写的希腊字母，对于没有大写的字母如$\\alpha$和$\\beta$，如果首字母大写对应的是$\\Alpha$和$\\Beta$即AB。 对于变体字的输入，直接在前面加var即可。 上下标 一般来说，上下标直接使用^和_即可。 对于斜体的要加大括号，否则就会只对第一个有效。 1234$ a^2 , a_2 x^{y+z},p_{ij},p_ij$ a^2 , a_2, \\, x^{y+z},p_{ij},p_ij 英文字母只有在表示变量(或单一字符的函数名称，如f(r))时才可使用斜体，其余情况都应使用罗马体(直立体)。 1234567$ x_i 此时为斜体 x_{\\rm i} 此时为直立体roman x_{\\mathrm i} \\rm is short for \\mathrm x_{\\text i} \\text{e} ,\\text{i} 自然对数和虚数单位也要为直立体$ x_i, i=1,2...n\\\\ x_{\\rm i} ,i是input\\\\ x_{\\mathrm i},x_{\\text i}\\\\ \\text{A B},\\rm{AB}\\\\ \\text{e} 自然对数,\\text{i} 虚数单位 \\text{}和\\rm{}的区别是前者支持空格，后者不支持空格。 分式于根式\\frac(fraction,分数) 12345$ \\frac{1}{2}, \\frac 1 2 \\frac{1}{x+y} \\frac{\\frac 1 x +1}{y+1},\\frac{\\dfrac 1 x +1}{y+1}$ 单个字符可以不用大括号，原理与上面相似。 如果感觉嵌套时的字符比较小的话，可以用\\dfrac{}{}. \\frac{1}{2}, \\frac 1 2 \\frac{1}{x+y}\\\\ \\frac{\\frac 1 x +1}{y+1},\\frac{\\dfrac 1 x +1}{y+1}\\sqrt[]{}(square root,平方根) 123$ \\sqrt 2, \\sqrt{x+y}, \\sqrt[3]{x+z}$ \\sqrt 2, \\sqrt{x+y}, \\sqrt[3]{x+z} 对于非平方根的数在\\sqrt后加[]来自定义。 普通运算符12345678910$ +- \\times,\\cdot,\\div \\pm,\\mp &gt;&lt;,\\ge,\\le,\\gg,\\ll,\\ne,\\approx,\\equiv \\cap,\\cup,\\in,\\notin,\\subseteq,\\subsetneqq,\\subnsetneqq \\varnothing,\\forall,\\exists,\\nexists\\because,\\therefore \\mathbb R,\\R,\\Q,\\N,\\Z_+ \\mathcal F,\\mathscr F$ $\\pm$ ：\\pm(plus-mius 正负号) $\\mp$：\\mp(mius-plus 负正号) $\\ge$：\\ge(greater than or equal 大于等于) $\\le$：\\le(less than or equal 大于等于) $\\gg,\\ll$：\\gg,\\ll（远大于,远小于） $\\ne$：\\ne（not equal, 不等于） $\\approx$：\\approx(approximate, 约等于) $\\equiv$：\\equiv(equivalent, 恒等的) $\\cap,\\cup$：\\cap，\\cup（交集，并集） $\\in,\\notin$：\\in,\\notin（属于，不属于） $\\subseteq,\\subsetneqq,\\subsetneq$：\\subseteq,\\subsetneqq,\\subsetneq（子集，真子集） $\\supseteq,\\supsetneqq,\\supsetneq$：\\suqseteq,\\suqsetneqq,\\suqsetneq（与上面相反） $\\varnothing,\\forall$：\\varnothing（空集）,\\forall(与任意的) $\\exists,\\nexists$：\\exists,\\nexists（存在，不存在） $\\because,\\therefore$：\\because,\\therefore（因为，所以） $\\R,\\Q,\\N,\\Z$：\\R,\\Q,\\N,\\Z对于数集来说特有的字符，可以直接使用\\转义即可，但是除了数集以外的其他字符就无法使用了。完整写法为：\\mathbb R $\\mathcal F,\\mathscr F$：\\mathcal F,\\mathscr F（花体字符） 以下为上面字符的运行结果。 +-\\\\ \\times,\\cdot,\\div\\\\ \\pm,\\mp\\\\ >","link":"/2022/01/24/Latex/Latex%E5%85%AC%E5%BC%8F%E6%8E%92%E7%89%88%E5%BF%AB%E9%80%9F%E6%95%99%E7%A8%8B/"},{"title":"Latex期刊论文的排版","text":"本文主要针对与IEEE的一个Latex模板，主要从模板下载、语句解释、插入图片、插入公式、插入算法图、插入引用等方面来讲解如何使用模板。该教程属于简易教程，深入学习请看后续的详细教程。视频参考 Latex模板下载 模板下载:IEEE模板:http://www.ieee.org/publications_standards/publications/authors/author_templates.html 通用模板:https://www.overleaf.com/ 其他方法:百度,csdn等网页找 Latex的核心思想就是内容与格式分离 模板的基本语句解释 \\begin{document}是文件的开始，之前为导言区，可以加入各种各样的宏包。 \\begin{document}之后到\\begin{abstract}之前用来写作者信息，联系方式以及致谢等信息。 \\begin{IEEEkeywords} 后面写关键词。 \\section{Introduction} 之后为介绍综述部分。 \\subsection{name} 为子标题部分。 \\section{References Section} 参考文献部分。 \\begin{IEEEbiography} 开始作者生平部分。 插入图片 首先，导入宏包 \\usepackage{graphicx}。 其次，导入代码块，其中的后缀名可以去查看详细的教程。 最后，编辑图片的大小样式。 插入公式公式部分主要的内容参考之前的专门写公式那篇博客。 首先，导入宏包 \\usepackage{amsmath,amsfonts} 其次，行间公式如图，文章终会自行标号，也可以自行标号\\tag{1.1}。 \\begin{equation} \\label{deqn_ex1} x = \\sum_{i=0}^{n} 2{i} Q. \\tag{1.1} \\end{equation} 最后，行内的公式的写法是在$ $之间直接写即可，如$x^2+y^2=0$ 插入参考文献实用办法插入参考文献 首先，引入宏包 \\usepackage[colorlinks,linkeolor-black,anchorcolor-black,citecolor-black]{hyperref }%用于调整多个参考文献的间距。直接用就好。 模板的插入方法，直接插入。 也可以自己使用BibTex格式的文件来插入，但是在实际编译的过程中要记得将BibTex文件也进行编译。 到百度上下载一个BibTex的文件。 引用BIb文件。 以下为BibTex文件的内容。 12345678@article{2021Exploiting,title={Exploiting Vector Attention and Contextual Prior for Ultrasound Image Segmentation},author={ Xu, L. and Gao, S. and Shi, L. and Wei, B. and He, Y. },journal={Neurocomputing},volume={454},number={1},year={2021},} 引用，BibTex也要编译，否则出现的是？，但是目前TexStduio我还没有找到在哪里编译BibTex文件。 插入算法图 首先，导入宏包 \\usepackage[ruled,linesnumbered]{algorithm2e}。 之后，导入算法块即可。 工具补充 补充好用网页:手写公式转latex https://editor.codecogs.com/ 手写符号，转latex表达 https://detexify.kirelabs.org/classify/html 截图看公式 https://mathpix.com/ 手写表格，转latex表达 https://www.tablesgenerator.com/","link":"/2022/01/25/Latex/Latex%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87%E7%9A%84%E6%8E%92%E7%89%88/"},{"title":"AlexNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); 本文首先详解AlexNet网络，计算每一层网络的维度并且整理了花数据集。之后，通过Pytorch对网络进行复现，主要从模型搭建、设置GPU、数据预处理、加载数据、配置损失函数和优化器、模型保存、模型训练验证与测试等方面进行实现。 AlexNet的详解AlexNet是2012年ISLVRC 2012 (lmageNet Large Scale Visual RecognitionChallenge）竞赛的冠军网络，分类准确率由传统的70%+提升到80%+。它是由Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，深度学习开始迅速发展。 AlexNet网络简介ISLVRC 2012训练集:1,281,167张已标注图片；验证集:50,000张已标注图片；测试集:100,000张未标注图片。 AlexNet的结构图 该网络的亮点在于: 首次利用GPU进行网络加速训练。 使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数。 因为Sigmoid函数会出现梯度消失，梯度爆炸。 使用了LRN局部响应归一化。 在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合。 过拟合：根本原因是特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。过度的拟合了训练数据，而没有考虑到泛化能力。 使用Dropout的方式在网络正向传播过程中随机失活一部分神经元。 回顾：经过卷积后的矩阵尺寸大小计算公式为： 其中输入图片的大小为$W\\times W$； Filter的大小为$F\\times F$，即kernel_size为F； 步长Stirde=S padding的像素数为P N=\\dfrac{(W-F+2P)}{S}+1网络分层计算 第一层（Conv1）： 这里是运用了GPU的并行计算的功能，将96层分成了两部分进行计算。 通道数：input_channels = 3；output_channels = 48$\\times$2=96 卷积核参数：kernel_size = 11; stride = 4; padding = [1,2] 输入输出的尺寸数：input_size=[224,224,3]; output_size=[55,55,96] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(224-11+(1+2))}{4}+1=55 第二层（Maxpool1）：只是改变高度宽度，不改变深度。 通道数：input_channels = 96；output_channels = 48$\\times$2=96 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[55,55,96]; output_size=[27,27,96] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(55-3+0)}{2}+1=27 第三层（Conv2）： 通道数：input_channels = 96；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 5; stride = 1; padding = 2 输入输出的尺寸数：input_size=[27,27,96]; output_size=[27,27,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(27-5+4)}{1}+1=27 第四层（Maxpool2）： 通道数：input_channels = 256；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[27,27,256]; output_size=[13,13,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(27-3+0)}{2}+1=13 第五层（Conv3）： 通道数：input_channels = 256；output_channels = 192$\\times$2=384 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,256]; output_size=[13,13,384] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第六层（Conv4）： 通道数：input_channels = 384；output_channels = 192$\\times$2=384 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,256]; output_size=[13,13,384] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第七层（Conv5）： 通道数：input_channels = 384；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,384]; output_size=[13,13,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第八层（Conv6）： 通道数：input_channels = 256；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[13,13,256]; output_size=[6,6,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+0)}{2}+1=6 后三层接了三个全连接层，注意最后数据集有1000类，因此最后一个全连接层有1000个节点，如果我们的数据集有10类，最后的全连接层就10个节点。 layer_name kernel_size output_channels padding stride Conv1 11 96 [1,2] 4 Maxpool1 3 None 0 2 Conv2 5 256 [2,2] 1 Maxpool2 3 None 0 2 Conv3 3 384 [1,1] 1 Conv4 3 384 [1,1] 1 Conv5 3 256 [1,1] 1 Maxpool3 3 None 0 2 FC1 2048 None None None FC2 2048 None None None FC3 1000 None None None 数据集(flowers) （1）在data_set文件夹下创建新文件夹”flower_data” （2）点击链接下载花分类数据集 （3）解压数据集到flower_data文件夹下 （4）执行”split_data.py”脚本自动将数据集划分成训练集train和验证集val 1234├── flower_data ├── flower_photos（解压的数据集文件夹，3670个样本） ├── train（生成的训练集，3306个样本） └── val（生成的验证集，364个样本） AlexNet网络的pytorch实现模型的搭建 对于网络层比较多的结构来说，一般用在网络比较多的时候，且网络结构比较简单，没有跳跃连接等复杂的结构。 padding=tuple/int：如果tuple(1,2),代表上下方各补一行0，左右补两列0。但是，如果出现上述的计算出来的下一层的[H,W]不为0时，自动舍弃一行或一列零来满足最后结果为整数。 1nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2) 如果要左侧补一列，右侧补两列，上侧补一行，右侧补两行。要使用nn.ZeroPad((1，2，1，2)) nn.ReLU(inplace=True): 减小使用内存，提高性能的参数。 self.modules()：返回一个迭代器，遍历网络中所有的模块； 关于初始化，目前版本的pytorch其实是可以自动进行初始化的。初始化时，对于卷积层使用恺明初始化，对于一般的全连接层使用正态分布初始化，偏差全部初始化为0。 1234567891011121314def _initialize_weights(self): for m in self.modules(): # module()返回一个迭代器 遍历一个类中所有的模块类 if isinstance(m, nn.Conv2d): # 参数1为对象(迭代器) 参数2为要作比较的对象 # 如果参数1对应的对象和参数2对应的类相同，返回True nn.init.kaiming_normal_(m.weight, mode='fan_out') # 对于卷积的参数使用恺明初始化 if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): # 对于全连接层，使用的正态分布初始化权重 nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.constant_(m.bias, 0) 对于展开成为全连接层来说，保证第0位batch不变, start_dim=1$[batch\\ , channel\\ , weight\\ , height] \\rightarrow [batch\\ , num]$ 1x = torch.flatten(x, start_dim=1) # 也可以用view 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class AlexNet(nn.Module): def __init__(self, num_classes=1000, init_weight=False): super(AlexNet, self).__init__() # 卷积层合集 self.feature = nn.Sequential( # input [3,224,224] nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2), # output [48,55,55] # 如果出现上述的计算出来的下一层的[H,W]不为0时， # 自动舍弃一行零来满足最后结果为整数 # 或者使用nn.ZeroPad((1，2，1，2)) nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [48,27,27] nn.Conv2d(48, 128, kernel_size=5, padding=2), # output [128,27,27] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [128,13,13] nn.Conv2d(128, 192, kernel_size=3, padding=1), # output [192,13,13] nn.ReLU(inplace=True), nn.Conv2d(192, 192, kernel_size=3, padding=1), # output [192,13,13] nn.ReLU(inplace=True), nn.Conv2d(192, 128, kernel_size=3, padding=1), # output [128,13,13] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [128,6,6] ) # 全连接层合集 self.classifier = nn.Sequential( nn.Dropout(p=0.5), nn.Linear(128*6*6, 2048), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(2048, 2048), nn.ReLU(inplace=True), nn.Linear(2048, num_classes), ) if init_weight: self._initialize_weights() # 一般情况下时自动进行初始化的 def _initialize_weights(self): for m in self.modules(): # module()返回一个迭代器 遍历一个类中所有的模块类 if isinstance(m, nn.Conv2d): # 参数1为对象(迭代器) 参数2为要作比较的对象 # 如果参数1对应的对象和参数2对应的类相同，返回True nn.init.kaiming_normal_(m.weight, mode='fan_out') # 对于卷积的参数使用恺明初始化 if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): # 对于全连接层，使用的正态分布初始化权重 nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.constant_(m.bias, 0) def forward(self, x): x = self.feature(x) x = torch.flatten(x, start_dim=1) # [batch channel weight height] start_dim=1 # 就是保证第0位batch不变-&gt;[batch num] # 也可以用view x = self.classifier(x) return x 设置GPU设备1device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 数据预处理12345678910111213data_transform = { &quot;train&quot;: transforms.Compose([ transforms.RandomResizedCrop(224), # 随机截取大小为224的图片 transforms.RandomHorizontalFlip(), # 随机翻转 transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]), &quot;val&quot;: transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])} 这里我们将预处理部分分成“train”和”val”两部分，将其封装称为一个字典。 transforms.RandomResizedCrop(224)：随机截取一定大小的图片。 transforms.RandomHorizontalFlip()：对图片数据进行随机反转。 transforms.ToTensor()：将数据转化为Tensor，维度按照torch要求的排列$[batch\\ , channel\\ , weight\\ , height]$。 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))：将图片数据标准化。 加载数据1234567891011# 获得绝对路径data_root = os.path.abspath(os.path.join(os.getcwd(), &quot;../..&quot;))# os.getcwd()获得当前位置的目录# ./.代表返回上一层目录 ../..代表返回上两层目录# 获得花的数据data_root = os.path.abspath(os.getcwd())image_path = data_root + &quot;/data_set/flower_data/&quot;train_dataset = datasets.ImageFolder(root=image_path + &quot;/train&quot;, transform=data_transform[&quot;train&quot;])# 求数据即得数据个数train_num = len(train_dataset) os.getcwd()：获得当前位置的目录。 ./.代表返回上一层目录； ../..代表返回上两层目录 os.path.join(os.getcwd(), “../..”)：代表返回上两层目录，其中join()的作用就是将内部路径进行合并，也可以进入当前目录的下的某个文件，join(os.getcwd(), “/xxx/xxx”) ImageFolder：用来以一定方式整理图片的类。 123456789# flower_list得到字典，但是得出的结果是反的flower_list = train_dataset.class_to_idx# 是的字典的val和key交换为位置的一种方法cla_dict = dict((val, key) for key, val in flower_list.items())# 将字典写入到Json文件中json_str = json.dumps(cla_dict, indent=4)with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_dataset.class_to_idx：得到类别和索引值的字典，此处预测得到的是索引值，想要得到花的名字就需要将key（名称）和value（索引值）交换位置。 json.dumps(cla_dict, indent=4)：将字典值写入到json文件中。 1234567{ &quot;0&quot;: &quot;daisy&quot;, &quot;1&quot;: &quot;dandelion&quot;, &quot;2&quot;: &quot;roses&quot;, &quot;3&quot;: &quot;sunflowers&quot;, &quot;4&quot;: &quot;tulips&quot;} 数据的导入和加载123456789101112batch_size = 32train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)val_dataset = datasets.ImageFolder(root=image_path + &quot;val&quot;, transform=data_transform[&quot;val&quot;])val_num = len(val_dataset)val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0) 这边和上一章相同，此处不在赘述。 损失函数的计算与优化器配置12345678net = AlexNet(num_classes=5, init_weight=True)net.to(device) # 启动GPUloss_function = torch.nn.CrossEntropyLoss()optimizer = optim.Adam(net.parameters(), lr=0.0002)save_path = '.AlexNet.pth'best_acc = 0.0 # 设置保存最高准确率所对应的参数 模型的训练与验证 net.train(),net.eval()：主要的作用是用来管理Dropout层和BN层，在训练的过程启用上述层，但是在预测的时候还是正常计算，不进行随机失活和BN操作。 训练过程 t1 = time.perf_counter()：用来记录训练的时间。 用来打印进度条的程序。其中，end=’’空字符。 123456# 打印训练的过程进度条 rate = (step + 1) / len(train_loader) a = &quot;*&quot; * int(rate * 50) b = &quot;.&quot; * int((1-rate) * 50) print(&quot;\\rtrain loss: {:^3.0f}%[{}-&gt;{}]{:.3f}&quot;.format(int(rate*100), a, b, loss), end='') # 这里end=''相当于是一个换行符 以下是训练过程的代码实现。 123456789101112131415161718192021222324for epoch in range(15): net.train() # 主要针对的Dropout方法 running_loss = 0.0 t1 = time.perf_counter() for step, data in enumerate(train_loader, start=0): images, labels = data images, labels = images.to(device), labels.to(device) # 将梯度初始化为0 optimizer.zero_grad() outputs = net(images) loss = loss_function(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() # 打印训练的过程进度条 rate = (step + 1) / len(train_loader) a = &quot;*&quot; * int(rate * 50) b = &quot;.&quot; * int((1-rate) * 50) print(&quot;\\rtrain loss: {:^3.0f}%[{}-&gt;{}]{:.3f}&quot;.format(int(rate*100), a, b, loss), end='') # 这里end=''相当于是一个换行符 print() print(time.perf_counter()-t1) 验证过程1234567891011121314151617# 直接接在上一个for循环中net.eval()acc = 0.0with torch.no_grad(): for data_test in val_loader: test_images, test_labels = data_test test_images, test_labels = test_images.to(device), test_labels.to(device) outputs = net(test_images) predict_y = torch.max(outputs, dim=1)[1] print(predict_y == test_labels) acc += (predict_y == test_labels).sum().item() accurate_test = acc / val_num # 只保存最好的结果 if accurate_test &gt; best_acc: best_acc = accurate_test torch.save(net.state_dict(), save_path) print('[epoch %d] train_loss: %.3f test_accuracy: %3f' % (epoch + 1, running_loss/step, acc / val_num)) 模型测试导入数据1234567891011121314151617181920data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# load imageimg = Image.open(&quot;4.jpg&quot;)plt.imshow(img)# [N C H w]img = data_transform(img)# 增加一个维度img = torch.unsqueeze(img, dim=0)# 读取字典try: json_file = open('./class_indices.json', 'r') class_indict = json.load(json_file)except Exception as e: print(e) exit(-1) 导入模型12345678910111213141516# 建立模型model = AlexNet(num_classes=5)# load 权重model_weigh_path = &quot;.AlexNet.pth&quot;model.load_state_dict(torch.load(model_weigh_path))model.eval()with torch.no_grad(): # predict class output = torch.squeeze(model(img)) # 降低维度，将batch一维去掉 predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy()print(class_indict[str(predict_cla)], predict[predict_cla].item())plt.show()","link":"/2022/01/28/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/AlexNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"Pytorch入门教程及LeNet网络","text":"hljs.initHighlightingOnLoad(); 本篇为Pytorch入门教程，使用的网络为LeNet(1998)网络。主要从网络搭建、导入数据、定义损失函数与优化器、训练与验证模型、模型测试与保存方面进行展示。 建立LeNet网络模型123456789101112131415161718192021222324252627import torchimport torch.nn as nnimport torch.nn.functional as Fclass LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5) self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5) self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(32 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) # pytorch [batch, channel, height, width] def forward(self, x): #input(3,32,32) N=(32-5+0)/1+1=28 x = F.relu(self.conv1(x)) #output(16,28,28) N=28/2=14 x = self.pool1(x) #output(16,14,14) N=(14-5+0)/1+1=10 x = F.relu(self.conv2(x)) #output(32,10,10) N=10/2=5 x = self.pool2(x) #output(32,5,5) x = x.view(-1, 32*5*5) #output=32*5*5=800 x = F.relu(self.fc1(x)) #output=120 x = F.relu(self.fc2(x)) #output=84 out = F.relu((self.fc3(x)))#output=10 return out 经过卷积后的矩阵尺寸大小计算公式为： 其中输入图片的大小为$W\\times W$； Filter的大小为$F\\times F$，即kernel_size为F； 步长Stirde=S padding的像素数为P N=\\dfrac{(W-F+2P)}{S}+1以下我们进行模型的测试和调试： 12345678910111213141516# 模型的测试import torchX = torch.rand(size=(32, 3, 32, 32), dtype=torch.float32)model = LeNet()print(model)# 打印的结果如下：LeNet( (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1)) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1)) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=800, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 导入数据 这里我们使用的CIFAR-10数据集，一共由10类。 首先，下载数据集。 torchvision.datasets.xxx() 里面有我们需要很多数据集。 123456# 下载训练数据trainset = torchvision.datasets.CIFAR10( root=&quot;./data&quot;, #数据集下载的位置 train=True, #是否为训练集 transform=transforms,#图像处理函数 download=True) #是否下载 图像的处理函数集 1234transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) Compose([ ])函数的作用是将所有的处理函数合并、打包。 ToTensor()函数的作用是将图片数据转换为张量，维度转化为pytorch标准的维度，同时数据的范围由原来的$[0,255]\\rightarrow[0.0,1.0]$。 Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))的作用是将图片的张量数据进行标准化。 之后，导入数据集。 123456# 导入训练数据trainloader = utils.data.DataLoader( trainset, # 总的训练数据集 batch_size=50, # 一个batch里的样本数 shuffle=True, # 是否打乱顺序 num_workers=0) # 线程数 由于一次训练不可能将训练集所有的参数都选进来进行反向传播，因此我们把一次喂入数据的个数称为batch_size。 数据的测试，使用迭代器获得数据集中的图片和标签。 1234567891011121314151617181920test_data_iter = iter(test_loader)# 迭代器，用来获取test_image, test_label = test_data_iter.next()# 用来获取下一个参数# 显示函数def imshow(img): img = img / 2 + 0.5 # unnormalize 反标准化 npimg = img.numpy() # 转化为numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # 由于ToTensor()改变了数据的维度排列，转化为原来的维度排列 plt.show()# # get some random training imagesdataiter = iter(trainloader)images, labels = dataiter.next()# show imagesimshow(torchvision.utils.make_grid(images))# print labelsprint(' '.join('%5s' % classes[labels[j]] for j in range(4))) 定义损失函数12# 定义损失函数为交叉熵损失函数loss_function = nn.CrossEntropyLoss() 如图，在CrossEntropyLoss()里面已经包含了交叉熵的计算公式，因此就不在需要在网络里定义softmax()层了。 定义优化器这里使用的Adam() 优化器。 1234# 定义优化器（训练参数，学习率）optimizer = optim.Adam(net.parameters(), lr=0.01)# 可变学习率scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1) 训练模型1234567891011121314151617181920212223242526272829303132for epoch in range(5): # 一个epoch即对整个训练集进行一次训练,这里对训练集循5轮 running_loss = 0.0 # 训练误差初始化为0 for step, train_data in enumerate(trainloader, start=0): inputs, labels = train_data inputs, labels = inputs.to(device), labels.to(device) # 清除历史梯度,每一轮训练都要 optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) # 正向传播 loss = loss_function(outputs, labels) # 计算损失 loss.backward() # 反向传播 optimizer.step() # 优化器更新参数 # 打印训练结果 running_loss += loss.item() accuray = 0 test_image, test_label = test_image.to(device), test_label.to(device) if step % 500 == 499: print('******************') with torch.no_grad(): # 代表以下的代码都是在torch.no_grad()下进行运算的 outputs = net(test_image) predit_y = torch.max(outputs, dim=1)[1] accuray = (predit_y == test_label).sum().item() / test_label.shape[0] print('[%d, %5d] loss: %.3f test_accuray: %.3f' % (epoch + 1, step + 1, running_loss / 500, accuray)) running_loss = 0.0 enumerate(trainloader, start=0)：返回的是数据集和对应的步数。 关于optimizer.zero_grad() 进行梯度的清零。 12345678910# 传统的方法for i, (image, label) in enumerate(train_loader): # 1. input output pred = model(image) loss = criterion(pred, label) # 2. backward optimizer.zero_grad() # reset gradient loss.backward() optimizer.step() 获取 loss：输入图像和标签，通过infer计算得到预测值，计算损失函数； optimizer.zero_grad() 清空过往梯度； loss.backward() 反向传播，计算当前梯度； optimizer.step() 根据梯度更新网络参数 1234567891011121314151617# 梯度累加方法for i,(image, label) in enumerate(train_loader): # 1. input output pred = model(image) loss = criterion(pred, label) # 2.1 loss regularization loss = loss / accumulation_steps # 2.2 back propagation loss.backward() # 3. update parameters of net if (i+1) % accumulation_steps == 0: # optimizer the net optimizer.step() # update parameters of net optimizer.zero_grad() # reset gradient 获取 loss：输入图像和标签，通过infer计算得到预测值，计算损失函数； loss.backward() 反向传播，计算当前梯度； 多次循环步骤 1-2，不清空梯度，使梯度累加在已有梯度上； 梯度累加了一定次数后，先optimizer.step() 根据累计的梯度更新网络参数，然后optimizer.zero_grad() 清空过往梯度，为下一波梯度累加做准备； 总结来说：梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环。对于一些GPU内存不足的实验室，这是一个可以扩大batch_size的trick。 模型测试12345678910111213141516# 打印训练结果running_loss += loss.item()accuray = 0# 使用GPUtest_image, test_label = test_image.to(device), test_label.to(device)if step % 500 == 499: print('******************') with torch.no_grad(): # 代表以下的代码都是在torch.no_grad()下进行运算的 outputs = net(test_image) predit_y = torch.max(outputs, dim=1)[1] # _,predit_y = torch.max(outputs, dim=1) # max()返回的是两个维度，首先的最大值，之后是index accuray = (predit_y == test_label).sum().item() / test_label.shape[0] print('[%d, %5d] loss: %.3f test_accuray: %.3f' % (epoch + 1, step + 1, running_loss / 500, accuray)) running_loss = 0.0 with torch.no_grad():这里的with是一个上下文管理器，其中的作用是下面的代码都遵循torch.no_grad()。 其次，在预测过程中，不用进行梯度计算，用了反而内存不够，因此使用torch.no_grad()。 tensor.item()：用来获取tensor的数值。 保存模型12save_path = './LeNet.pth'torch.save(net.state_dict(), save_path) 模型测试 首先，对图片进行处理。 123456789101112# 数据预处理transform = transforms.Compose( [transforms.Resize((32, 32)), # 首先需resize成跟训练集图像一样的大小 transforms.ToTensor(), # 转换为torch的对的张量维度 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# 导入要测试的图像（自己找的，不在数据集中），放在源文件目录下im = Image.open('4.jpg')im = transform(im) # 转换成[C, H, W]im = torch.unsqueeze(im, dim=0) # 对数据增加一个新维度，# 因为tensor的维度是[batch, channel, height, width] torch.unsqueeze(im, dim=0)：对数据增加一个新维度，即加一个batch的维度。 之后，实例化网络 123# 实例化网络，加载训练好的模型参数net = LeNet()net.load_state_dict(torch.load('Lenet.pth')) 最后，预测类别 1234567# 预测classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')with torch.no_grad(): outputs = net(im) predict = torch.max(outputs, dim=1)[1].data.numpy()print(classes[int(predict)]) 这里，我们将outputs丢进torch.softmax() ，可得到属于每个类别的概率。 1234567predict = torch.softmax(outputs, dim=1)# dim=1的原因，是因为outputs为两维的，dim=0只有一个# outputs = tensor([[0.0000, 0.0000, 5.4191, 0.4988, 0.0000, 1.3599, 0.0000, 0.2233, 0.0000, 0.0000]])print(predict)# 输出:tensor([[0.0042, 0.0042, 0.9464, 0.0069, 0.0042, 0.0163, 0.0042, 0.0052, 0.0042, 0.0042]])","link":"/2022/01/28/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/Pytorch%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8F%8ALeNet%E7%BD%91%E7%BB%9C/"},{"title":"GoogLeNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); GoogLeNet详解简介GoogLeNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task (分类任务)第一名，本文主要从模型的搭建以及pytorch实现上进行介绍。 网络中的亮点： 引入了Inception结构（融合不同尺度的特征信息) 使用1x1的卷积核进行降维以及映射处理 添加两个辅助分类器帮助训练 AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层(其中两个辅助分类层) 丢弃全连接层，使用平均池化层（大大减少模型参数) Inception结构 如图为原始的一种Inception结构； 主要的区别是之前的VGG主要采取串联的结构，增加模型的深度， Inception主要采取并行的结构，增加的是模型的宽度。 注意：每个分支所得的特征矩阵的高和宽必须是相同的。 如图为改进之后的网络图，这里1$\\times$1的卷积核的主要作用是用来降维的。 这里降维就是将深度降低，即channels数，通过减少深度，进而减少参数量，减少计算量。 辅助分类器(Auxiliary Classifier)The exact structure of the extra network on the side, including the auxiliary classifier, is as follows: An average pooling layer with 5x5 filter size and stride 3, resulting in an 4x4×512 outputfor the (4a), and 4×4×528 for the (4d) stage. $input_{size}=14,\\ output_{size}=\\dfrac{14-5+0}{3}+1=4$ A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.· A fully connected layer（全连接层） with 1024 units and rectified linear activation（RELU). A dropout layer with 70% ratio of dropped outputs. A linear layer with softmax loss as the classifier (predicting the same 1000 classes as themain classifier, but removed at inference time). 在3$\\times$3卷积之前要进行一个1$\\times$1卷积的降维处理。 上图为下面表格中对于的结构在网络中的位置。 如图，在实际的操作过程中，LocalRespNorm(LPN)层的作用不大，因此可以去掉。 如图相比之下发现，VGG的参数参数过多，是GoogLeNet的20倍。 但VGG的模型搭建简单，并且GoogLeNet辅助分类器的使用和修改比较麻烦，不容易调试，因此一般来说VGG的使用较多。 GoogLeNet的pytorch实现模型搭建因为GoogLeNet的一些结构块的代码复用比较多，因此这里首先创建几个模板文件，以方便之后模型的搭建。 基本卷积块12345678910class BasicCon2vd(nn.Module): def __init__(self, in_channels, out_channels, **kwargs): super(BasicCon2vd, self).__init__() self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, **kwargs) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.conv(x) x = self.relu(x) return x 由于卷积层都是伴随着ReLU激活函数来使用的，因此首先建立将二者合并的模板。 通过使用模板搭建的方式，使得网络得结构一目了然，很清晰。 Inception模板的搭建 1234567891011121314151617181920212223242526272829class Inception(nn.Module): def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj): super(Inception, self).__init__() self.branch1 = BasicCon2vd(in_channels, ch1x1, kernel_size=1) self.branch2 = nn.Sequential( BasicCon2vd(in_channels, ch3x3red, kernel_size=1), BasicCon2vd(ch3x3red, ch3x3, kernel_size=3, padding=1) # 将padding=1 使得branch的输入和输出是相同的 ) self.branch3 = nn.Sequential( BasicCon2vd(in_channels, ch5x5red, kernel_size=1), BasicCon2vd(ch5x5red, ch5x5, kernel_size=5, padding=2) # 将padding=2 使得branch的输入和输出是相同的 ) self.branch4 = nn.Sequential( nn.MaxPool2d(kernel_size=3, stride=1, padding=1), BasicCon2vd(in_channels, pool_proj, kernel_size=5, padding=2) # 将padding=2 使得branch的输入和输出是相同的 ) def forward(self, x): branch1 = self.branch1(x) branch2 = self.branch2(x) branch3 = self.branch3(x) branch4 = self.branch4(x) output = [branch1, branch2, branch3, branch4] return torch.cat(output, 1) # 在维度1上面进行叠加和整合, 即在channel上进行拼接 通过不同的padding参数，保证每个branch的输入和输出总是相同的。 最后，在维度dim=1上面进行叠加和整合, 即在channel上进行拼接。 辅助分类器的搭建123456789101112131415161718192021222324252627class InceptionAux(nn.Module): def __init__(self, in_channels, num_classes): super(InceptionAux, self).__init__() self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3) self.conv = BasicConv2d(in_channels, 128, kernel_size=1) # output[batch, 128, 4, 4] self.fc1 = nn.Linear(2048, 1024) self.fc2 = nn.Linear(1024, num_classes) def forward(self, x): # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14 x = self.averagePool(x) # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4 x = self.conv(x) # N x 128 x 4 x 4 x = torch.flatten(x, 1) # [batch channel width height] 1 代表在1的维度下进行展平 x = F.dropout(x, 0.5, training=self.training) # self.training model.train()模式下为true 在model.eval()下为False # N x 2048 x = F.relu(self.fc1(x), inplace=True) x = F.dropout(x, 0.5, training=self.training) # N x 1024 x = self.fc2(x) # N x num_classes return x 开始时，在平均池化下采样和卷积之前，一个辅助分类器与第二个辅助分类器的深度是不同的。 dropout层的随机概率p=0.7，但是实际的效果不一定好，因此我们将调整为p=0.5。 当我们实例化个模型model后，可以通过model.train()和model.eval()来控制模型的状态。在model.train()模式下self.training=True，在model.eval()模式下self.training=False GoogLeNet搭建123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101class GoogLeNet(nn.Module): def __init__(self, num_classes=1000, aux_logits=True, init_weight = False): super(GoogLeNet, self).__init__() self.aux_logits = aux_logits self.conv1 = BasicCon2vd(3, 64, kernel_size=7, stride=7, padding=3) # 计算默认为下取整 该层的作用是将H,W缩小为原来的一半 self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True) # ceil_mode=True时 取整方式变为了上取整 self.conv2 = BasicCon2vd(64, 64, kernel_size=1) self.conv3 = BasicCon2vd(64, 192, kernel_size=3, padding=1) self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32) self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64) self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64) self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64) self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64) self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64) self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128) self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128) self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128) if self.aux_logits: self.aux1 = InceptionAux(512, num_classes) self.aux2 = InceptionAux(528, num_classes) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # (1, 1) 这里的(1, 1) &lt;=&gt; (width, Height) 无论输入是多少 输出都为1*1 self.dropout = nn.Dropout(p=0.4) self.fc = nn.Linear(1024, num_classes) if init_weight: self._initialize_weights() def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) def forward(self, x): # N x 3 x 224 x 224 x = self.conv1(x) # N x 64 x 112 x 112 x = self.maxpool1(x) # N x 64 x 56 x 56 x = self.conv2(x) # N x 64 x 56 x 56 x = self.conv3(x) # N x 192 x 56 x 56 x = self.maxpool2(x) # N x 192 x 28 x 28 x = self.inception3a(x) # N x 256 x 28 x 28 x = self.inception3b(x) # N x 480 x 28 x 28 x = self.maxpool3(x) # N x 480 x 14 x 14 x = self.inception4a(x) # N x 512 x 14 x 14 if self.training and self.aux_logits: # eval model lose this layer aux1 = self.aux1(x) x = self.inception4b(x) # N x 512 x 14 x 14 x = self.inception4c(x) # N x 512 x 14 x 14 x = self.inception4d(x) # N x 528 x 14 x 14 if self.training and self.aux_logits: # eval model lose this layer aux2 = self.aux2(x) x = self.inception4e(x) # N x 832 x 14 x 14 x = self.maxpool4(x) # N x 832 x 7 x 7 x = self.inception5a(x) # N x 832 x 7 x 7 x = self.inception5b(x) # N x 1024 x 7 x 7 x = self.avgpool(x) # N x 1024 x 1 x 1 x = torch.flatten(x, 1) # N x 1024 x = self.dropout(x) x = self.fc(x) # N x 1000 (num_classes) if self.training and self.aux_logits: # eval model lose this layer return x, aux2, aux1 return x self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)：ceil_mode=True时 取整方式变为了上取整。 nn.AdaptiveAvgPool2d((1, 1)) ：自适应的池化下采样操作，其中输入参数为一个元组，大小为目标输出的[H,w]值。 模型的训练但部分还是和之前的模型相似的，以下为不同的部分。 1234net = GoogLeNet(num_classes=5, aux_logits=True, init_weights=True) net.to(device) loss_function = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=0.0003) 首先是定义的模型为GoogLeNet与原来不同。 123456789101112131415for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits, aux_logits2, aux_logits1 = net(images.to(device)) loss0 = loss_function(logits, labels.to(device)) loss1 = loss_function(aux_logits1, labels.to(device)) loss2 = loss_function(aux_logits2, labels.to(device)) loss = loss0 + loss1 * 0.3 + loss2 * 0.3 loss.backward() optimizer.step() # print statistics running_loss += loss.item() train_bar.desc = &quot;train epoch[{}/{}] loss:{:.3f}&quot;.format(epoch + 1, epochs, loss) 其次是治理的损失函数用三个，训练时分别辅助分类器进行计算，之后再将计算得到的辅助分类器的值进行加权处理。 在训练是，会去计算各种辅助分类器的输出，计算损失函数，之后在验证和测试过程中，就不会再去计算辅助分类器了。 模型测试1234567# create modelmodel = GoogLeNet(num_classes=5, aux_logits=False).to(device)# load model weightsweights_path = &quot;./googleNet.pth&quot;assert os.path.exists(weights_path), &quot;file: '{}' dose not exist.&quot;.format(weights_path)missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device), strict=False) 这里，我们在训练过程中是保存了辅助分类器的参数的，但是在测试时，没有定义辅助分类器，因此将strict=False，默认为True，此时model.load_state_dict() 会返回两个结果，unexpected_keys保留的是之前辅助分类器的层。 最好的结果达到了86%左右。","link":"/2022/01/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"VGG网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); VGG网络详解简述VGG在2014年由牛津大学著名研究组VGG (Visual GeometryGroup)提出，斩获该年ImageNet竞赛中Localization Task (定位任务)第一名和Classification Task (分类任务)第二名。本文从模型的搭建与pytorch的实现进行具体的介绍。 VGG网络的几种配置 一般使用的时候，多用D网络。 网络中的亮点: 通过堆叠多个3x3的卷积核来替代大尺度卷积核——(减少所需参数) 论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核，代替后拥有相同的感受野。 CNN的感受野在卷积神经网络中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野(receptive field)。通俗的解释是，输出feature map上的一个单元对应输入层上的区域大小。 感受野的计算公式： F(i)=(F(i+1)-1)\\times Stride+K_{size} $F(i)$为第i层感受野，$Stride$为第i层的步距，$K_{size}$为卷积核或采样核尺寸。 Pool1：Size=2$\\times$2，Stride=2；Conv1：size=3$\\times$3，Stride=2 Feature map：$F=1$ Pool1：$F=(1-1)\\times2+2=2$ Conv1：$F=(2-1)\\times2+3=5$ 验证两个3$\\times$3的卷积核可以代替一个7$\\times$7(Stride=1): Feature map：$F=1$ Conv3$\\times$3(3)：$F=(1-1)\\times1+3=3$ Conv3$\\times$3(2)：$F=(3-1)\\times1+3=5$ Conv3$\\times$3(1)：$F=(5-1)\\times1+3=7$ 论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核。使用7x7卷积核所需参数，与堆叠三个3x3卷积核所需参数(假设输入输出channel为C) $7\\times7\\times C \\times C = 49C^2$ $3\\times3\\times C \\times C+3\\times3\\times C \\times C+3\\times3\\times C \\times C=27C^2$ 卷积层的作用是用来提取特征，全连接层的作用的用来分类。 VGG的pytorch实现模型搭建123456cfgs = { 'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],} 配置VGG的模型list，里面的数字代表Conv层的channel数，“M”代表Maxpool层。 其中cfg为一个dict的数据，Key为不同VGG的类型，Value为list即不同的VGG网络的配置参数。 卷积层的实现12345678910111213def make_features(cfg: list): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) layers += [conv2d, nn.ReLU(True)] # 每个卷积函数跟ReLU激活函数都是绑定在一起的。 in_channels = v # 下一层的输入为本层的输出 return nn.Sequential(*layers) 该代码是用来读取配置文件的，由于输入为一张图片，因此输入的in_channels=3。 nn.Sequential(layers)*：实例化非关键字参数实现的。 全连接层的实现12345678910111213self.classifier = nn.Sequential( # 第一个全连接层 nn.Dropout(p=0.5), nn.Linear(512*7*7, 2048), nn.ReLU(True), # 第二个全连接层 nn.Dropout(p=0.5), nn.Linear(2048, 2048), # 原论文为4096，为了加快速度我们设置为2048 nn.ReLU(True), # 第三个全连接层 nn.Linear(2048, class_num) ) 全部的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738class VGG(nn.Module): def __init__(self, features, class_num=1000, init_weights=False): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Dropout(p=0.5), nn.Linear(512*7*7, 2048), nn.ReLU(True), nn.Dropout(p=0.5), nn.Linear(2048, 2048), # 原论文为4096，为了加快速度我们设置为2048 nn.ReLU(True), nn.Linear(2048, class_num) ) if init_weights: self._initialize_weights() def forward(self, x): # N x 3 x 224 x 224 x = self.features(x) # N x 512 x 7 x 7 # 展平操作，和之前相似 x = torch.flatten(x, start_dim=1) # N x 512*7*7 x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) # nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) 模型的实例化123456789def vgg(model_name=&quot;vgg16&quot;, **kwargs): try: cfg = cfgs[model_name] # 得出配置列表参数 except: print(&quot;Warning: model number {} not in cfgs dict!&quot;.format(model_name)) exit(-1) model = VGG(make_features(cfg), **kwargs) return model \\*kwargs*：代表可变字典长度的参数。 模型的训练训练的过程，基本和ALexNet相同。这里由于VGG网络的参数多，训练所需要的数据大，而现实过程中我们的参数不足，因此需要迁移学习的方法来解决比较好。","link":"/2022/01/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"ResNeXt网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); ResNeXt网络详解论文题目为Aggregated Residual Transformations for Deep Neural Networks，主要的创新点在于更新了block. 在宽度和深度上性能有了一定的提升。 ResNeXt网络在相同的计算量的情况下相比ResNet来说，错误率更小。 组卷积 本质上是将输入$C_{in}$划分为g个组，每个组$C_{in}/g$层，每个组通过卷积层之后将产生$n/g$层的一个卷积，再将g个组的卷积再拼接以下，就得出了n层的卷积网络。 极端情况$g=C_{in},n=C_{in}$，此时就是DW Conv. (c)首先$1 \\times 1$对进行降维处理，256-&gt;128；之后用组卷积group=32进行卷积，最后$1 \\times 1$进行升维，再和输入进行相加。 (b)先分组，分成32个组，每组channel=4，进行$1 \\times 1$卷积之后，再将每个组进行$3 \\times 3$的组卷积，归并之后其他跟(c)相同。 (a)前面和(b)相同，后面用一个相加来代替归+卷积操作。 理论上三者是等价的，(b)和(c)的等价比较显然，与(a)的等价不好理解。 相加的规则如下图所示，kernel=[1,2]卷积的运算结果如绿线所示。 分组卷积再相加的结果和直接卷积的结果相同。 利用组卷积代替他一般的卷积，得出ResNeXt网络的基本框架。下图的(32$\\times$4d)代表成32个组，每个组conv2_channel=4 如下图所示，作者通过实验来验证为什么要选分组数为32. 这里尽量保持计算量相同的情况下，通过改变不同的分组数和conv_channel，右图设计了几个不同的分组组合。 左面的图代表50和101层网络中，不同分组组合对应得错误率。 对于浅层得block来讲，效果没有很大的提升。 ResNeXt网络的pytorch实现网络的搭建基本和ResNet相同，基本是在ResNet上面的改进。 如右图所示，网络的框架的是相同的，改进主要发生在基本的block上面： 第二个卷积成换成了group=32的组卷积。 第一个卷积层的输入channel是原来ResNet的2倍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Bottleneck(nn.Module): &quot;&quot;&quot; 注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2， 这么做的好处是能够在top1上提升大概0.5%的准确率。 可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch &quot;&quot;&quot; expansion = 4 def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64): super(Bottleneck, self).__init__() width = int(out_channel * (width_per_group / 64.)) * groups self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width, kernel_size=1, stride=1, bias=False) # squeeze channels self.bn1 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups, kernel_size=3, stride=stride, bias=False, padding=1) self.bn2 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion, kernel_size=1, stride=1, bias=False) # unsqueeze channels self.bn3 = nn.BatchNorm2d(out_channel*self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out += identity out = self.relu(out) return out width = int(out_channel \\ (width_per_group / 64.)) * groups*计算的是翻倍以后的ResNeXt网络的out_channel数，如图所示，ResNeXt网络的第一层out_channel=128是ResNet的2倍，根据计算公式 width=(64*(32/64))*32=128 out_channel代表的是残差块的out_channel数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class ResNet(nn.Module): def __init__(self, block, blocks_num, num_classes=1000, include_top=True, groups=1, width_per_group=64): super(ResNet, self).__init__() self.include_top = include_top self.in_channel = 64 self.groups = groups self.width_per_group = width_per_group self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.in_channel) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) if self.include_top: self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # output size = (1, 1) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') def _make_layer(self, block, channel, block_num, stride=1): downsample = None if stride != 1 or self.in_channel != channel * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channel * block.expansion)) layers = [] layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride, groups=self.groups, width_per_group=self.width_per_group)) self.in_channel = channel * block.expansion for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x 相比之前的ResNet增加了groups=1, width_per_group=64 参数，在_make_layer()函数以及初始化的部分都增加上述参数，其余不变。 123456789def resnext50_32x4d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth groups = 32 width_per_group = 4 return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group) 这里我们采用第三种方法，仅仅训练最后一层全连接层的权重。 param.requires_grad = False 这样将前面的参数视之为不可训练的，这样就可以仅仅之训练最后一层的参数了，因为最后一层是重新定义的。 12345678net.load_state_dict(torch.load(model_weight_path, map_location=device))for param in net.parameters(): param.requires_grad = False # change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 数据预测数据的打包过程，这里的我们的首先将图片名称放到一个list中，之后使用batch_img = torch.stack(img_list, dim=0) 将list里面的图片打包成一个batch中。","link":"/2022/02/03/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/ResNeXt%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"ResNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); ResNet详解ResNet在2015年由微软实验室提出，斩获当年lmageNet竞赛中分类任务第一名，目标检测第一名。获得coco数据集中目标检测第一名，图像分割第一名。 网络中的亮点: 超深的网络结构(突破1000层) 提出residual模块 使用Batch Normalization加速训练(丢弃dropout) 随着网络的不断加深：梯度消失和梯度爆炸、退化问题。 通过残差结构可以很好的解决上述问题。 residual结构 如图，1x1的卷积层的主要作用是改变channel个数，即进行升维和降维； 注意，求和是时主分支和shoutcut的特征矩阵的形状[H,W]和深度channel必须相同。 左边这副图原本是以输入channel为64，3x3卷积层卷积核数也是64为例的，这里为了方便对比都改成了256。 表格中的乘积代表堆叠几层 下图所示，残差块的结构中残差有实线也有虚线。 实线连接和虚线连接的区别 其中的实线代表相加的二者形状和维度相同，即残差块的输入channel和输出的channel相同； 虚线表示残差块的输入channel和输出channel不同。因为虚线块首先要进行一个维度变化，导致input和shoutcut的维度不同，因此需要一个$kernel\\ size=1\\times1 \\, ,stride = 2$的卷积层来缩减矩阵形状，并且改变通道数来保证形状和维度相同。 一般在迭代的时候，将虚线层放在每次迭代的第一层来改变宽度和维度，第二层以后都为实线层，主要是用来将迭代，不改变任何参跨度和维度。 同理，另一个卷积的结构也适用。 注意原论文中，右侧虚线残差结构的主分支上，第一个1x1卷积层的步距是$stride=2$，第二个3x3卷积层步距是$stride=1$。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是$stride=1$，第二个3x3卷积层步距是$stride=2$，这样能够在ImageNet的top1上提升大概0.5%的准确率。详细可参考Resnet v1.5 综上，我们的残差块的主要的功能是降维缩减为原来的一半，之后将通道数变为原来的两倍。 Batch Normalization详解 Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。 Batch Normalization 的目的是使得我们一批(Batch)的feature map满足均值为0，方差为1的规律。 Batch Normalization原理 我们在图像预处理过程中通常会对图像进行标准化处理，这样能够加速网络的收敛，如下图所示。 对于Conv1来说输入的就是满足某一分布的特征矩阵，但对于Conv2而言输入的feature map就不一定满足某一分布规律了（注意这里所说满足某一分布规律并不是指某一个feature map的数据要满足分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律）。 也就是说，卷积Conv1输入以前的满足归一化，输出后不一定还满足之前输入的归一化。同时数据的Normalization是针对feature map的每一个维度在整个训练集上的数据进行的。 Batch Normalization的目的就是使我们的feature map满足均值为0，方差为1的分布规律。 “对于一个拥有d维的输入x，我们将对它的每一个维度进行标准化处理。” 假设我们输入的x是RGB三通道的彩色图像，那么这里的d就是输入图像的channels即d=3，$x^{(1)}$代表的是图片R通道的特征矩阵，依次类推分别是G通道和B通道的。 x = (x^{(1)}, x^{(2)}, x^{(3)}) 标准化处理也就是分别对我们的R通道，G通道，B通道进行处理。上面的公式不用看，原文提供了更加详细的计算公式如下： 由于Normalization后归一化成[0,1]并非是最好的结果，因此这里选择了引入$\\gamma;\\beta$两个超参数一个是 让feature map满足某一分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律，也就是说要计算出整个训练集的feature map然后在进行标准化处理，对于一个大型的数据集明显是不可能的，所以论文中说的是Batch Normalization，也就是我们计算一个Batch数据的feature map然后在进行标准化（batch越大越接近整个数据集的分布，效果越好）。 使用BN时需要注意的问题（1）训练时要将traning参数设置为True，在验证时将trainning参数设置为False。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。 （2）batch size尽可能设置大点，设置小后表现可能很糟糕，设置的越大求的均值和方差越接近整个训练集的均值和方差。 （3）建议将BN层放在卷积层（Conv）和激活层（例如Relu）之间，且卷积层不要使用偏置bias，因为没有用，参考下图推理，即使使用了偏置bias求出的结果也是一样的. 总结如下 迁移学习迁移学习的优势 能够快速训练出一个理想的结果。 当数据集较少时也能训练出理想的效果。传统情况下，网络越深，数据越少，越容易造成过拟合。 迁移学习可以将别人预训练好的模型拿来使用，大大的加速训练的过程。 注意：在使用别人的预训练模型参数时，要注意别人的预处理方式。自己的预处理的方式要与预训练模型的相同。 浅层网络的学习到的信息具有通用性，可以直接迁移到比较复杂的网络中使用，大大减小的训练的时间。 迁移学习的主要目的是，将预训练好的浅层网络的通用的识别信息迁移到我们的要训练的模型里去，使得新的网络也拥有识别底层通用特征得能力。 常见的迁移学习方式：1．载入权重后训练所有参数；2．载入权重后只训练最后几层参数；3．载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层(由于我们最后的一层输出的结点数和类别数可能不等于，因此最后加一层)。 一般情况下，第一种方法的效果是最好的，但相比第二和第三种方法，需要的运算资源较长、花费的时间也较长。 Resnet的pytorch实现模型搭建基本骨架的搭建18和34层结构上的主分支操残差结构的两层卷积网络是一模一样的，50、101、152层的残差结构中的三个卷积网络都不同。 这里现从基本的18层和34层的网络开始搭建。 1234567891011121314151617181920212223242526272829class BasicBlock(nn.Module): expansion = 1 # 通道数的变化，输出是输入的几倍 def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channel) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channel) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += identity out = self.relu(out) return out downsample的作用是区分结果是否为虚线上的的残差结构。 out_channel为主分支上的通道数。 bias=False使用BN层时，偏置是不需要的，设为False. expansion输出是输入的几倍的参数。 以下是50层、101层和152层网络的搭建： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Bottleneck(nn.Module): &quot;&quot;&quot; 注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2， 这么做的好处是能够在top1上提升大概0.5%的准确率。 可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch &quot;&quot;&quot; expansion = 4 def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64): super(Bottleneck, self).__init__() width = int(out_channel * (width_per_group / 64.)) * groups self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width, kernel_size=1, stride=1, bias=False) # squeeze channels self.bn1 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups, kernel_size=3, stride=stride, bias=False, padding=1) self.bn2 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion, kernel_size=1, stride=1, bias=False) # unsqueeze channels self.bn3 = nn.BatchNorm2d(out_channel * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out += identity out = self.relu(out) return out ResNet基本框架1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class ResNet(nn.Module): def __init__(self, block, blocks_num, groups=1, num_classes=1000, include_top=True, width_per_group=64): super(ResNet, self).__init__() self.include_top = include_top self.in_channel = 64 self.groups = groups self.width_per_group = width_per_group self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.in_channel) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) if self.include_top: self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # output size = (1, 1) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') def _make_layer(self, block, channel, block_num, stride=1): downsample = None if stride != 1 or self.in_channel != channel * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channel * block.expansion)) layers = [] layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride, groups=self.groups, width_per_group=self.width_per_group)) self.in_channel = channel * block.expansion for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: # include_top的主要的作用是对于需要在ResNet基础上做扩展的网络，则要将 # include_top设置为False x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x _make_layer(self, block, channel, block_num, stride=1): 输入为block(基本骨架)，channel(输入通道数)，block_num(基本骨架循环的层数) if stride != 1 or self.in_channel != channel $\\times$ block.expansion: 此处的判断逻辑是stride!=1(有H,W的改变)或者输入维度与输出维度不相等(不满足in_channel = channel $\\times$ block.expansion就代表不是第一层)，对于每一个残差结构的第一个来说，其改变的宽度和深度，因此要用考虑虚线连接。之后，几层为一般的实线的结构，不需要if分支设计虚线层。 设计堆叠的实线的残差层。第一层为实线，后几层都为实线。 12345for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) nn.Sequential(\\layers)* 返回值为非关键字参数。 第一个残差层不改变[H,W]，因此stride设计为1. 12345self.layer1 = self._make_layer(block, 64, blocks_num[0]) # 第一个残差层不改变[H,W]，因此stride设计为1 self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) 1234567891011121314151617181920212223242526272829303132333435def resnet34(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet34-333f7ec4.pth return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)def resnet50(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet50-19c8e357.pth return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)def resnet101(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)def resnext50_32x4d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth groups = 32 width_per_group = 4 return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group)def resnext101_32x8d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth groups = 32 width_per_group = 8 return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group) 定义不同类型的网络。 模型训练迁移学习1import torchvision.models.resnet ctrl+左键进入后可以直接看源码。可以下载预训练参数进行的前迁移学习，网址如下： 1234567891011model_urls = { 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',} 数据增强处理123456789data_transform = { &quot;train&quot;: transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), &quot;val&quot;: transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])} 载入预训练模型的方法12345678910111213net = resnet34()# load pretrain weights# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pthmodel_weight_path = &quot;./resnet34-pre.pth&quot;assert os.path.exists(model_weight_path), &quot;file {} does not exist.&quot;.format(model_weight_path)net.load_state_dict(torch.load(model_weight_path, map_location=device))# for param in net.parameters():# param.requires_grad = False# change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 首先，定义网络net = resnet34(); 之后，导入预训练的数据集 12model_weight_path = &quot;./resnet34-pre.pth&quot;net.load_state_dict(torch.load(model_weight_path, map_location=device)) 最后，改变模型最后一层结构。 1234# change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 也可以先载入到内存里面，之后将全连接层删去，将其载入到模型中去。 12torch.load(model_weight_path, map_location=device)# 载入到内存 其余的部分跟之前一样 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport jsonimport torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import transforms, datasetsfrom tqdm import tqdmfrom model import resnet34def main(): device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(&quot;using {} device.&quot;.format(device)) data_transform = { &quot;train&quot;: transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), &quot;val&quot;: transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])} data_root = os.path.abspath(os.path.join(os.getcwd(), &quot;../..&quot;)) # get data root path image_path = os.path.join(data_root, &quot;data_set&quot;, &quot;flower_data&quot;) # flower data set path assert os.path.exists(image_path), &quot;{} path does not exist.&quot;.format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;train&quot;), transform=data_transform[&quot;train&quot;]) train_num = len(train_dataset) # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4} flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) # write dict into json file json_str = json.dumps(cla_dict, indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) batch_size = 16 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # number of workers print('Using {} dataloader workers every process'.format(nw)) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw) validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;val&quot;), transform=data_transform[&quot;val&quot;]) val_num = len(validate_dataset) validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw) print(&quot;using {} images for training, {} images for validation.&quot;.format(train_num, val_num)) net = resnet34() # load pretrain weights # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth model_weight_path = &quot;./resnet34-pre.pth&quot; assert os.path.exists(model_weight_path), &quot;file {} does not exist.&quot;.format(model_weight_path) net.load_state_dict(torch.load(model_weight_path, map_location=device)) # for param in net.parameters(): # param.requires_grad = False # change fc layer structure in_channel = net.fc.in_features net.fc = nn.Linear(in_channel, 5) net.to(device) # define loss function loss_function = nn.CrossEntropyLoss() # construct an optimizer params = [p for p in net.parameters() if p.requires_grad] optimizer = optim.Adam(params, lr=0.0001) epochs = 3 best_acc = 0.0 save_path = './resNet34.pth' train_steps = len(train_loader) for epoch in range(epochs): # train net.train() running_loss = 0.0 train_bar = tqdm(train_loader) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits = net(images.to(device)) loss = loss_function(logits, labels.to(device)) loss.backward() optimizer.step() # print statistics running_loss += loss.item() train_bar.desc = &quot;train epoch[{}/{}] loss:{:.3f}&quot;.format(epoch + 1, epochs, loss) # validate net.eval() acc = 0.0 # accumulate accurate number / epoch with torch.no_grad(): val_bar = tqdm(validate_loader) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) # loss = loss_function(outputs, test_labels) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_bar.desc = &quot;valid epoch[{}/{}]&quot;.format(epoch + 1, epochs) val_accurate = acc / val_num print('[epoch %d] train_loss: %.3f val_accuracy: %.3f' % (epoch + 1, running_loss / train_steps, val_accurate)) if val_accurate &gt; best_acc: best_acc = val_accurate torch.save(net.state_dict(), save_path) print('Finished Training')if __name__ == '__main__': main() 视频参考","link":"/2022/02/03/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/ResNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"Latex基础教程（二）","text":"hljs.initHighlightingOnLoad(); 本文主主要包含Latex的表格操、作浮动体的设置、参考文献以及自定义命令的设置，主要参考了B站的latex中文教程-15集从入门到精通包含各种latex操作视频的后9讲。 Latex中的表格123\\begin{tabular}[&lt;垂直对齐方式&gt;]{&lt;列格式说明&gt;} &lt;表项&gt;&amp;&lt;表项&gt;&amp; ...&lt;表项入&gt; \\\\\\end{tabular} 用\\\\\\表示换行 用&amp;表示不同的列 对于格式说明参数主要有： l-本列左对齐 c-本列居中对齐 r-本列右对齐 p{&lt;宽&gt;}-本列宽度固定,能够自动换行 如图，一共五列，因此就有五个格式说明符。 其次，两个列之间加一个竖线加一个”|“，如果加一个双竖线就加一个”||“。 如果要加一个横线了，就在行前或行后加\\hline。 帮助文档的打开 123texdoc booktab # 三线表texdoc longtab # 长表格宏包texdoc tabu # 综合表格宏包 Latex中的浮动体浮动体的创建操作LATEX 预定义了两类浮动体环境figure 和table。习惯上figure 里放图片，table 里放表格，但并没有严格限制，可以在任何一个浮动体里放置文字、公式、表格、图片等等任意内容。 对于一般的图像，有figure浮动体环境。 对于表格，有table浮动体环境。 结果可以看出图像和表格的位置都发生了浮动效果。 给浮动体加属性以table 环境的用法举例： 123\\begin{table}[⟨placement⟩]...\\end{table} ⟨placement⟩ 参数提供了一些符号用来表示浮动体允许排版的位置，如hbp 允许浮动体排版在当前位置、底部或者单独成页。table 和figure 浮动体的默认设置为tbp。 \\caption{...}为设置浮动体的标题。 \\centering为设置图片的居中和偏左偏右。 \\label{...}为设置标签操作，方便后面的应用。 \\ref{...}为引用设置，和标签在一起来实现交叉引用。 这里浮动体的交叉引用和标号是自动完成的。 总结： 浮动体可以实现灵活的分页（避免无法分割的内容产生的页面六百留白） 可以给图表添加标题。 交叉引用。 123456789figure环境(table环境与之类似)\\begin{figure}[&lt;允许位置&gt;]&lt;任意内容&gt;...\\end{figure}&lt;允许位置&gt;参数(默认tbp)% h，此处( here)-代码所在的上下文位置% t，页顶(top)-代码所在页面或之后页面的顶部% b，页底( bottom)一代码所在页面或之后页面的底部% p，独立一页(page)-浮动页面 标题控制( caption、bicaption等宏包) 并排与子图表( subcaption、subfig、floatrow等宏包) 绕排(picinpar、 wrapfig等宏包) Latex中的参考文献thebibliography环境的使用 一次管理，一次使用参考文献格式: 12345\\begin{thebibliography}{编号样本}\\bibitem[记号]{引用标志}文献条目1\\bibitem[记号]{引用标志}文献条目2end{thebibliography}% 其中文献条目包括:作者，题目，出版社，年代，版本，页码等。 如下为一个具体的案例： 123456789101112引用文章\\cite{article1}，引用一本书\\cite{book1}\\begin{thebibliography}{99}\\bibitem{article1}陈立辉,苏伟,蔡川,陈晓云，\\emph{基于LaTex的Meb数堂公式提取方法研究}[7].计算机科学．2014(86)\\bibitem{book1}William H. Press,saul A. Ieukolsky,william T. Vetterling,Brian P. Elanneny.,\\emph{Numerical Recipes 3rd Edition:The Art of scientific Computing}Cambridge University Press,New York ,2007.\\bibitem{latexGuide} Kopka Helmut,w.Daly Patrick,\\emph{Guide to \\LaTeX}, $4^{th}$ Edition.Available at \\texttt{http://www.amazon.com}.\\bibitem{latexMath} Graetzer. George，\\emph{Math Into \\LaTeX},BirkhAtuser Boston;3 edition (June 22，2000).\\end{thebibliography} \\emph{}表示要强调的具体内容。 引用的时候可以采用：\\cite{引用标志1,引用标志2,...}。因此，这样管理参考文献需要给每个参考文献进行详细排版，比较的麻烦，同时也方便文献的跨文献的引用。 因此需要一个可以一次管理多次使用的包。 使用BibTex文件进行参考文献管理 bibtex文件的格式如下： 12345@⟨type⟩{⟨citation⟩, ⟨key1⟩ = {⟨value1⟩}, ⟨key2⟩ = {⟨value2⟩},...} 其中⟨type⟩ 为文献的类别，如article 为学术论文，book 为书籍，incollection 为论文集中的某一篇，等等。 ⟨citation⟩ 为\\cite 命令使用的文献标签。在⟨citation⟩ 之后为条目里的各个字段，以⟨key⟩ = {⟨value⟩} 的形式组织。 我们在此简单列举学术论文里使用较多的BIBTEX 文献条目类别： article 学术论文，必需字段有author, title, journal, year; 可选字段包括volume, number,pages, doi 等； book 书籍，必需字段有author/editor, title, publisher, year; 可选字段包括volume/number,series, address 等； incollection 论文集中的一篇，必需字段有author, title,booktitle, publisher, year; 可选字段包括editor, volume/number, chapter, pages, address 等； inbook 书中的一章，必需字段有author/editor, title,chapter/pages, publisher, year; 可选字段包括volume/number, series, address 等。 具体的使用： 在导言区设置参考文献的格式 1234\\bibliographystyle{plain}%plain unsrt alpha abbrv\\bibliographystyle[round]{plain}% 引用的括号变为圆括号 在正文区设置参考文献 1234567%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\bibliography{test} % 可以不写扩展名\\end{document} 然后bibtex.exe会编译.aux的辅助文件，根据\\citation在.bib文件中选择指定的参考文献，并指定的参考文献按照指定的样式进行排版，生成另一个.bbl辅助文件。 \\cite {xxx}，引用的标志为下图所示。 使用google的相应功能来实现数据库的维护。 在搜索结果中打开引用链接。 打开bibtex可以得到该文件对于的bibtex数据。 将生成的bibtex数据copy到文献数据库中即可。 使用知网导入数据，需要导入zotero的FireFox浏览器。 1.打开浏览器的关联插件； 2.选择需要的文献。 3.在Zotero里选择导出的文献，来执行导出条目的操作。 4.打开test.bib文件 5.引用之后的排版如下： 1234567%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\bibliography{test} % 可以不写扩展名\\end{document} 默认只能显示导入数据库内已经引用的文献。 对于还未引用的文献，可以使用\\nocite{}来显示。 12\\nocite{*} % 表示所有未引用的参考文献\\notice{xxx} % 特定的未引用的参考文献 特别注意，重新编译的时候要手动删除之前的参考文献，否则编译完之后没有变化。 12345678%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\nocite{*} \\bibliography{test} % 可以不写扩展名\\end{document} 输出的结果如下： 使用BibLaTex文件进行参考文献管理biblatex 宏包是一套基于LATEX 宏命令的参考文献解决方案，提供了便捷的格式控制和强大的排序、分类、筛选、多文献表等功能。biblatex 宏包也因其对UTF-8 和中文参考文献的良好支持，被国内较多LATEX 模板采用。 新的TEX参考文献排版引擎biblatex/ biber样式文件(参考文献样式文件—bbx文件，引用样式文件—cbx文件)使用LATEx编写支持根据本地化排版，如: 12biber -l zh__pinyin texfile %用于指定按拼音排序biber -l zh__stroke texfile %用于按笔画排序 配置编译器，将默认的文献工具设置为Biber。 首先是在导言区调用biblatex 宏包。宏包支持以⟨key⟩=⟨value⟩ 形式指定选项，包括参考文献样式style、参考文献著录排序的规则sorting 等。 1\\usepackage[style=numeric,backend=biber]{biblatex} 接着在导言区使用\\addbibresource 命令为biblatex 引入参考文献数据库。与基于BIBTEX的传统方式不同的是，这里需要写完整的文件名。 1\\addbibresource{test.bib} 在正文中使用\\cite 命令引用参考文献。除此之外还可以使用丰富的命令达到不同的引用效果， 如\\citeauthor 和\\citeyear 分别单独引用作者和年份， \\textcite 和\\parencite分别类似natbib 宏包提供的\\citet 和\\citep 命令，以及脚注式引用\\footcite 等。 最后在需要排版参考文献的位置使用命令\\printbibliography 默认的title是英文的Reference，可以通过增加可选参数来修改。 ```latex\\printbibliography[title={参考文献}] 123456789101112 &gt; 重新编译时，要清除上一次生成的辅助文件。- **开源的样式文件**https://gitlab.com/CasperVector/biblatex-caspervector - 先将格式文件拷贝到当前工作目录中。 ![image-20220207232045374](https://gitee.com/houdezaiwu2022/image-bed/raw/master/latex/202202072320093.png) ```latex \\usepackage[style=caspervector,backend=biber,utf8]{biblatex} % 修改导言区的文件，使得引用caspervector样式 但是文章是中英文混排的，需要修改biber.exe的相关配置。 并且修改导言区的宏包配置。 12345678\\usepackage[style=caspervector,backend=biber,utf8,sorting=centy]{biblatex}% sorting=centy 就是按先英文、再中文、再姓名、再标题、再出版年份的顺序进行排序% c——chinese 中文% e——English 英文% n——name 作者姓名% t——title 文献标题% y——year 出版年份 这里需要连续编译两次，来产生正确的编号。 使用.bat进行批处理编译流程。 LaTeX自定义命令和环境\\newcommand一定义命令命令只能由字母组成,不能以 \\end开头 1\\newcommand&lt;命令&gt;[&lt;参数个数&gt;][&lt;首参数默认值&gt;]{&lt;具体定义&gt;} \\newcommand可以是简单字符串替换12%例如:使用\\PRC相当于 People 's Republic of \\emph{China}这一串内容\\newcommand \\PRC{People's Republic of \\emph{china}} \\newcommand也可以使用参数1234567891011%参数个数可以从1到9,使用时用#1,#2,..... .,#9表示\\newcommand \\ loves[2]{#1 喜欢#2}\\newcommand \\hatedby[2]{#2不受#1喜欢}\\begin{document}\\loves{猫儿}{鱼}\\hatedby{猫儿}{萝卜}\\end{document}% 猫儿喜欢鱼% 萝卜不受猫儿喜欢 \\newcommand的参数也可以有默认值123456789%指定参数个数的同时指定了首个参数的默认值，那么这个命令的%第一个参数就成为可选的参数(要使用中括号指定)\\newcommand \\love[3][喜欢]{#2#1#3}\\begin{document}\\love[最爱]{猫儿}{鱼}\\end{document}% 猫儿最爱鱼 \\renewcommand-重定义命令1234%与 \\newcommand 命令作用和用法相同,但只能用于已有命令% \\renewcommand&lt;命令&gt;[&lt;参数个数&gt;][&lt;首参数默认值&gt;]{&lt;具体定义&gt;}\\renewcommand\\abstractname{内容简介}% 将摘要的名称改为内容简介 注意，该命令只能修改已经定义的命令，不能修改还没有的命令，其他地方的使用方法相似。 定义和重定义环境12345% 基本用法和重定义命令类似\\newenvironment{&lt;环境名称&gt;}[&lt;参数个数&gt;][&lt;首参数默认值&gt;] {&lt;环境前定义&gt;}{&lt;环境后定义&gt;}\\renewenvironment{&lt;环境名称&gt;}[&lt;参数个数&gt;][&lt;首参数默认值&gt;]% {&lt;环境前定义&gt;}{&lt;环境后定义&gt;} 为book类中定义摘要( abstract)环境 12345678910111213% 导言\\newenvironment{myabstract}[1][摘要]%{\\small \\begin{center}\\bfseries #1 \\end{center}% \\begin{quotation}}%{\\end{quotation}}% 正文\\begin{document}\\begin{myabstract}[我的摘要]% 我的摘要 -&gt; #1参数 这是一段自定义格式的摘要...\\end{document} 重定义参数的嵌套使用： 123456789101112131415% 环境参数只有&lt;环境前定义&gt;中可以使用参数，% &lt;环境后定义&gt;中不能再使用环境参数。% 如果需要，可以先把前面得到的参数保存在一个命令中，在后面使用:\\newenvironment{Quotation}[1]%{\\newcommand \\quotesource{#1}% \\begin{quotation}}% {\\par \\hfill--- 《textit{\\quotesource}》% \\end{quotation}} % 正文\\begin{document} \\begin{Quotation}{易$\\cdot$乾} 初九,潜龙勿用。 \\end{Quotation}.\\end{document} 总结： 定义命令和环境是进行LaTeX格式定制、达成内容与格式分离且标的利器。 使用自定义的命令和环境把字体、字号、缩进、对齐、间距等各种琐细的内容包装起来，赋以一个有意义的名字,可以使文挡结构清晰、代码整洁、易于维护。 在使用宏定义的功能时，要综合利用各种已有的命令、环境、变量等功能，事实上，前面所介绍的长度变量与盒子、字体字号等内容，大多并丕直接出现在文档正文史，而主要都是用在实现各种结构化的宏定义里。 问题汇总 中文乱码问题。在网上下载了一个中文模板，跑起来没问题，语句测试也没有问题。但是在我的文件里却一再出现乱码。 原因：编码问题。.tex文件要以utf-8格式存储。然后导言区加入： 1\\usepackage[UTF8,noindent]{ctex} 编译器的配置。右侧的Default compiler默认值是pdflatex。这个并不支持中文的。因此将其设置为xelatex。 更多教程请见https://www.bilibili.com/video/BV1Zh411y7ps?p=2","link":"/2022/02/08/Latex/Latex%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B(%E4%BA%8C)/"},{"title":"Latex基础教程(一)","text":"hljs.initHighlightingOnLoad(); 本文主主要包含Latex文件基本结构、如何处理中文、字体字号的设置、文档篇章的设置、特殊字符的输入以及图像的插入和排版，主要参考了B站的latex中文教程-15集从入门到精通包含各种latex操作视频的前8讲。 Latex文件的基本结构导言区 在latex中，我们使用% 来进行注释。 导言区主要进行全局设置 \\title{}设置文章的标题 \\author{}设置文章的的作者 \\date{}设置文章的时间 但是设置的全局的属性是看不到的，要在正文区加\\maketitle才可以显示出来 如果使用的是book类，则输出title是单独在一页的。 1234567% 导言区\\documentclass{book}% report book letter article\\title{My First Document}\\author{Zhengtong Cao}\\date{\\today} 正文区 使用\\begin{}和\\end{}来创建一个环境。 每个文章里最多有一个document环境。 12345678% 正文区\\begin{document} \\maketitle hello, \\LaTeX Let $f(x)$ be define by the formula $f(x)=3x^2+x-1$ \\end{document} 如果两句话之间加一个空行，则相当于一个换行操作。 两段话之间可以加入多个空行，但是现实的时候就只有一个空行了。 这里的空行是实实在在的空行，不可以是注释行。之前错误的以为注释行可以作为一个空行，但是编译出来发现之前的换行消失了。 单 是行内公式，双$$$ $$$的作用是行间公式， \\begin{equation},\\end{equation}环境可以产生带编号的行间公式。 123\\begin{equation} AB^2+BC^2=AC^2\\end{equation} Latex中的中文处理texstdio配置设置编译器为XeLateX 设置默认编码为UTF-8 在导言区导入ctex宏包 未定义符号的定义1234$\\angle$A=90$\\degree$% latex内没有对应的命令\\degree的命令，编译时会显示错误，要在导言区重新定义\\newcommand{\\degree}{^\\circ} latex内没有对应的命令\\degree的命令，要在导言区重新定义，\\newcommand{\\degree}{^\\circ} 由于latex的编译器不同造成的，在typora里就定义了\\degree命令，但在latex里就没有，要重新定义。 中文的输入123456\\usepackage{ctex}\\title{\\heiti \\LaTeX 学习笔记}%\\author{Zhengtong Cao}\\author{\\kaishu 曹政通}\\date{\\today} 字体的设置：\\heiti 黑体、kaishu 楷体。 导入宏包后直接输入即可，无视下面的红色下滑线。 在cmd里输入以下命令，可以弹出ctex的宏包手册。这里也可以直接导入\\ctexart,\\ctexbook等。 1texdoc ctex 可以使用texdoc命令来查看各种文档 12texdoc lshort-zh# 一个latex的简易学习文件，112分钟入门 Latex字体字号的设置 字体族设置字体族主要分为罗马字体\\textrm{Roman Family}；无衬线字体\\textsf{San serif Family}；打字机字体\\texttt{Typewriter} 如图，\\rmfamily 代表以后的字体都是罗马体，如果之后又其他的字体的设置如\\ttfamily 则上个作用自动结束，开始新的作用。 一般使用时会加上{\\rmfamily xxx}来限定字体族的作用范围。 \\rmfamily罗马体，\\sffamily 无衬线字体，\\ttfamily打字机字体。 字体系列设置字体系列主要分为粗细、宽度设置如\\textmd{ Medium Series } 和 \\textbf{ Boldface Series }（字体设置命令） { \\mdseries xxx }和{ \\bfseries xxx } (字体设置声明) 字体形状命令字体形状直立\\textup{upright Shape}、斜体\\textit{Italic Shape}、伪斜体\\textsl{s1anted Shape}、小型大写\\textsc{Small caps Shape}（字体设置命令） { \\upshape Upright shape} {\\itshape Italic shape} {\\slshape Slanted Shape} {\\scshape Small caps Shape}(字体设置声明) 中文字体{\\songti 宋体} {\\heiti 黑体} {\\fangsong 仿宋} {\\kaishu 楷书}中文字体的\\textbf{粗体}（黑体）与\\textit{斜体}（楷体） 字体大小的设置 设置全文的默认字号大小，一般只有种10pt、11pt、12pt 对于\\ctex宏包来说又一些设置自好操作命令。 自定义字体操作 latex的思想时将内容和排版分里，因此经常在导言区设置\\newcommand{}{}来设计字体样式。 Latex文档的篇章结构提纲构建的几个命令： 1234\\chapter{⟨title⟩} %章\\section{⟨title⟩} %节\\subsection{⟨title⟩} %小节\\subsubsection{⟨title⟩} %子小节 其中\\chapter 只在book 和report 文档类有定义。这些命令生成章节标题，并能够自动编号。 article 文档类带编号的层级为\\section / \\subsection / \\subsubsection 三级； report/book 文档类带编号的层级为\\chapter / \\section / \\subsection 三级 如下，对于节操作的一些参数设计： 带可选参数的变体：\\section[⟨short title⟩]{⟨title⟩}标题使用⟨title⟩ 参数，在目录和页眉页脚中使用⟨short title⟩ 参数 带星号的变体：\\section*{⟨title⟩}标题不带编号，也不生成目录项和页眉页脚 分段操作 分段操作一般是插入一个空行，也可以使用\\par,但是多用插入空行的命令。\\\\的作用是换行而不是分段。 \\ctexart 命令 通过\\ctexart 设置布局时section是居中排版的. 使用\\ctexset命令可以自定义ctexart格式。 排版以后的格式如图所示。 目录的生成 在LATEX 中生成目录非常容易，只需在合适的地方使用命令：\\tableofcontents 这个命令会生成单独的一章（book / report）或一节（article），标题默认为“Contents“ \\tableofcontents生成的章节默认不写入目录（\\section*或\\chapter*） Latex中的特殊字符空白符号 空行分段，多个空行等同1个； 自动缩进，绝对不能使用空格代替； 英文中多个空格处理为1个空格，中文中空格将被忽略； 汉字与其它字符的间距会自动由XeLaTeX处理； 禁止使用中文全角空格。 通过命令来生成空格字符： 12345678910111213141516171819202122% 1em(当前字体中M的宽度)产生一个空格的宽度a\\quad b% 2em 产生的两个空格的宽度a\\qquad b% 约为1/6个空格的宽度a\\,b a\\thinspace b% 0.5个空格命令a\\enspace b% 产生一个空格a\\ b% 硬空格a~b% 1pc=12pt=4.218mm % 产生固定宽度值的空白 a\\kern 1pc ba\\kern -1em ba\\hskip 1em ba\\hspace{35pt}b% 占位宽度(根据大括号内的占位符的宽度产生空白)a\\hphantom{xyz}b%弹性长度(即占满整个的空间)a\\hfill b Latex的控制符由于有些符号在Latex中具有特殊的含义，因此输入的时候前面要加\\ (注意，\\\\代表换行操作，因此这里用\\textbackslash来代替) 1\\# \\$ \\% \\{ \\} \\~{} \\_{} \\&amp; \\textbackslash 输出结果如下： Latex排版符号排版中的特殊符号如下： 1\\S \\P \\dag \\ddag \\copyright \\pounds TEX标志符号123456789101112131415% 基本符号\\Tex{} \\LaTeX{} \\LaTeXe{}\\usepackage{xltxtra}% 提供了针对XeTeX的改进并且加入了XeTeX的LOGO% xltxra宏包\\XeLaTeX\\usepackage{texnames}% texnames宏包提供\\AmSTeX{} \\AmS-\\LaTeX{}\\BibTeX{} \\LuaTeX{}\\usepackage{mflogo}% mflogo宏包\\METAFONT{} \\MF{} \\MP{} 输出结果： 引号1`(左引号) '(右引号) ``(左双引号) ''(右双引号) 连字符使用几个减号的累加来代表不同长度的连字符。 1- -- --- 非英文字符12% 非英文字符\\oe \\OE \\ae \\AE \\aa \\AA \\o \\O \\l \\L \\ss \\SS !`?` 重音符号(以o为例)12%重音符号(以o为例)\\`o \\'o \\^o \\''o \\~o \\=o \\.o \\u{o} \\v{o}\\H{o} \\r{o} \\t{o} \\b{o} \\c{o} \\d{o} Latex的图像插入123456%导言区: \\usepackage{graphicx}%语法:\\includegraphics[&lt;选项&gt;]{&lt;文件名&gt;}%格式: EPS,PDF,PNG,JPEG,BMP\\usepackage{graphicx}\\graphicspath{{figures/},{pic/}} %图片在当前目录下的 figures目录 \\includegraphics[&lt;选项&gt;]{&lt;文件名&gt;} 的必选参数为文件名，可选参数为图片的缩放和旋转。 文件名，可以加类型后缀.jpg .png \\graphicspath{ {figures/},{pic/} } 指定图片的搜索路径，多个路径使用，隔开。 hexo 排版中连续两个大括号之间要加空格。 123456789101112% 指定缩放比例\\includegraphics[scale=0.3]{lion}\\includegraphics[scale=0.03]{mountain}\\includegraphics[scale=0.3]{oscilloscope}% 指定固定宽度和高度\\includegraphics[height=2cm]{lion}\\includegraphics[width=2cm]{lion.jpg}% 指定相对的高度和宽度\\includegraphics[width=0.2 \\textwidth]{lion}\\includegraphics[width=0.2 \\textheight]{mountain}% 指定多个参数\\includegraphics[angle=-45,width=0.2\\textwidth]{lion} 细节查看: 1texdoc graphic","link":"/2022/02/08/Latex/Latex%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B(%E4%B8%80)/"},{"title":"第0章绪论","text":"hljs.initHighlightingOnLoad(); 本章主要介绍控制理论的性质、发展、应用以及控制动态系统的几个基本步骤，为以后现代控制理论的学习做铺垫。 0.1 控制理论的性质控制理论的两个目标： 了解基本控制原理； 以数学表达它们，使它们最终能用以计算进入系统的输入，或用以设计自动控制系统。 两个主题：自动控制领域中有两个不同的但又相互联系的主题。 反馈的概念。 最优控制的概念。 0.2 控制理论的发展 20世纪20年代到40年代，马克斯威尔对装有调速器的蒸汽机系统动态特性的分析、马诺斯基对船舶驾驶控制的研究都是控制理论的开拓性工作。 20世纪40年代至50年代，维纳对控制理论作出了创造性的贡献。 20世纪50年代后期到60年代初期是控制理论发展的转折时期。 苏联学者在20世纪50年代对包含非线性特性、饱和作用和受到限制的控制等因素的系统的最优瞬态的研究表现出很大的兴趣。这些学者的研究讨论导致了庞特里亚金的“极大值原理”。 显示控制理论转折时期的另一个里程碑是20世纪50年代后期卡尔曼(卡尔曼——布西)滤波器的发现。 最近25年线性系统理论的研究非常活跃。 20世纪60年代后期和70年代早期，将线性二次型理论推广到无穷维系统(即以偏微分方程、泛函微分方程、积分微分方程和在巴拿赫空间的一般微分方程描述的系统)的工作得到很大进展。 目前研究的是以线性偏微分方程或相对简单的迟延方程描述的只能在空间的边界上加以观察和控制的系统。 20世纪70年代末80年代初，反馈控制的设计问题经历了一个重新修正的过程。随着人工智能的发展和引入了新的计算机结构，控制理和计算机科学的联系愈来愈密切。 0.3控制理论的应用控制系统之所以能得到如此普遍的应用，要归功于 现代仪表化(完备的传感器和执行机构) 便宜的电子硬件 控制理论有处理其模型和输出信号所具有的不确定性动态系统的能力 在控制理论中已完善的各种方法愈来愈得到普遍应用的同时，先进的理论概念的应用却仍集中在像空间工程那样的高技术方面。当然，由于计算机技术的飞速发展和世界性的激烈的工业竞争，这种情况将会改变。 控制概念得到主要应用的一个领域是石油化工生产过程。钢铁行业中热轧厂是最早成功地采用计算机控制的工厂。 0.4 控制一个动态系统的几个基本步骤简单地说，控制一个动态系统有下列四个基本步骤： 建模：为一个系统选择一个数学模型是控制工程中最重要的工作。 系统辨识： 定义为用在一个动态系统上观察到的输入与输出数据来确定它的模型的过程。 当前系统辨识方面的研究集中在下列诸基本问题上：辨识问题的可解性和问题提出的恰当性、对各类模型的参数估计方法。 信号处理：控制理论外面的独立的一门学科，但这两学科之问有许多重叠之处，而控制界曾对信号处理作出了重要贡献，特别是在滤波和平滑的领域。 控制的综合：为控制系统生成控制规律。这些过程的复杂性导致了各种控制研究课题，主要有：","link":"/2022/02/08/%E7%8E%B0%E4%BB%A3%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E7%AC%AC0%E7%AB%A0%E7%BB%AA%E8%AE%BA/"},{"title":"Auto-Encoder的补充内容","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是auto-encoder的补充内容，包括除了重构误差以外评估编码器效果的判别器模型，以及离散化表示学习的方法来进行嵌入。 More than minimizing reconstruction error 什么是好的embedding，好的embedding可以很好的表示其编码的对象的主要特征（如五等分花嫁中三玖的特点是带耳机）。 除了Reconstruction，如何评估encoder的效果？(下图为 一个编码的例子) 使用一个判别器discriminator——binary classifer 如果编码正确输出1，错误输出0。此时分类器可以很容易判断输出编码的争取与否，即编码具有很强的代表性。 如果编码不具有代表性，分类器很难判断正确和错误，此时编码器的代表很差。 于是，训练的目标函数变成最小化$\\theta$ 使得损失函数$L_D^*$最小。 \\begin{align} \\theta^*&=arg\\ \\min_\\limits{\\theta} \\ L_D^*\\\\ &=arg\\ \\min_\\limits{\\theta}\\ \\min_\\limits{\\phi}L_D^*\\\\ \\end{align}Train the encoder 𝜃and discriminator 𝜙 to minimize $L_D^*$ $\\Rightarrow$ (c.f. training encoder and decodert o minimize reconstruction error) 经典的auto-encoder形式其实是一个特例。 一般的设计如下图所示： 训练的时候给定图片和编码向量，输出的为重构的误差，这里没有了positive example和negative example的影响，只是positive example的score(重构的错误率)越小越好。 More interpretable embeddingDiscrete Representation一般方法 要得到的编码是一个一维向量，每个向量都有很好的可解释性。 non differentiable : https://arxiv.org/pdf/1611.01144.pdf 处理不能微分的论文。 Binary vector相比one hot 更好，因为其参数相对更少，而且可以产生训练参数里没有的东西（泛化性能更强）。 Vector Quantized Variational Auto encoder (VQVAE) Codebook里面是一系列的向量，Codebook里的系列也是NN学习出来的。 得出的code vector 和Codebook里面的所有向量进行相似度的计算，取相似度最高的向量，进行decoder。(通过这样来实现离散化) 离散化的作用是保留声音里文字的部分，过滤掉背景噪声的因素。","link":"/2022/02/08/Auto-encoder/Auto-encoder%E7%9A%84%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9/"},{"title":"Deep Auto-Encoder基本理论","text":"hljs.initHighlightingOnLoad(); 本文主要从PCA进行进行类比得出auto-encoder，通过增加深度来提高auto-encoder的特性，以及其在文字处理、预训练参数和图片相似度比较上的应用。 Auto-encoder 编码器：将图片的信息压缩称为一个code，代表原图的一个精简的表示。 解码器：根据code来将原来的图片进行重构。 单个encoder，只有input没有output，是没有办法学习的。 因此实际了一个解码器来得到输出。 然后将encoder和decoder连接到一起，一起来学习。 Starting from PCA $W$为变换矩阵，PCA一种Linear method。 目标是使得输出的重构图片和输入的原图片之间的差距越小越好。 可以将$W$看成是一个encoder，将$W^T$看成是一个decoder，中间的code相当于是一个hidden layer。 encoder可以是一个NN，因此可以增加中间的层数。 Deep Auto-encoder 之前，Deep Auto-encoder很难训练，需要RBM进行layer的初始化才能训练的好一点。 Symmetric is not necessary，就是可以将encoder和decoder对应起来训练，但是这样其实是不必要的，直接训练两个NN即可。 相比PCA来说，通过增加深度可以很好的提高结果的清晰度。 下图为一个效果上的对比，将数据降维成2维 PCA得出的code很难讲这个数据的特征提取出来。 Deep Auto-encoder则可以明显的将这几个特征分开。 Text Retrieval(文字处理) Vector Space Model：将文章编码成高维的向量，对比cos上的相似度。 Bag-of-word：通过0和1的形式，来完成句子的取词和构建。还有一种是用[0,1]来表示重要性。但是无法学习到单词的语义。 通过deep auto-encoder学到的文档的信息有清晰的分类。 LSA则很难将不同的文档中的特征进行区分。 Auto-encoder-similar Image Search 将图片之间直接计算相似度，得出的结果和预期相差较大。 因此使用自编码器将每张图片进行编码，直接对编码计算相似度如图所示。 Auto-encoder - Pre-training DNN使用auto-encoder进行神经网络的初始化操作。 先用auto-encoder进行预训练，这里如左图所示，先将784维转换1000维再784维，看训练的重构的情况。(将低位转换成为高维需要很强的正则化[如L1使得但部分为0只有个别几维不为0]，否则会出现一个分块的单位矩阵那样) W= \\begin{pmatrix} 1 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\end{pmatrix} 之后就将第1层进行固定，再次进行auto-encoder操作。 然后，重复上面的操作如图，再向下学习得到下一层。 最后一层直接随机初始化即可。 在大量数据进行训练的时候，上述的技术还是十分的重要的。 其他类型的auto-encoder 加入噪声后的auto-encoder，decode是跟原来没有噪声的数据进行对比的，不是最原始的结果。 Auto-encoder for CNN 目标是相同的，减小解码前和重构以后的差别。 相比于编码器的卷积和池化操作，解码器的操作为反卷积和反池化。 CNN-Unpooling Maxpooling是将一个kernel里去最大的一个。 Unpooling是保留原来卷积的位置信息，其他地方用0来补齐。但是keras是直接将值复制四份。 CNN-Deconvolution其实，deconvolution也是一种卷积。 一般的卷积是将多个值变为一个值，反卷积是将一个值通过乘以不同的权重变成多个值。 反卷积也可以等价为多个值转换为一个值，其实是等价的。 NEXT 将编码值进行解码，即可以得出如图二维code的分布，没有分布点的的地方解码以后的结果不是很好，有点分布的地方可以看到大概的数字。 通过二维的分布来判断那边是有值存在的，那边取样才可能出现结果。 在code上面加L2的正则化，保证在0的附近都是有值的。","link":"/2022/02/08/Auto-encoder/Deep-Auto-encoder%E5%9F%BA%E6%9C%AC%E7%90%86%E8%AE%BA/"},{"title":"Auto-Encoder (2021)","text":"hljs.initHighlightingOnLoad(); 本文主要从auto-encoder的Basic Idea of Auto-encoder、Feature Disentanglement、Discrete Latent Representation以及More Applications四个方面进行叙述。这里主要参考的时李宏毅2021年的新课。这里auto-encoder和 CycleGAN 和transformers机制有一定的相似之处，后续还要深入了解。 Self supervised Learning Framework 自监督学习：使用大量的资料，进行不用标注的学习任务，主要有填空题和预测下一句话。 将自监督模型做一点调整，用在下游的任务里。 BERT和GPT是主流的自监督学习的框架，但是之前的Auto-Encoder其实也可以算作是自监督学习的一种(因为其可以不要label)。 Basic Idea of Auto-encoder Auto-encoder的思想和Cycle GAN相当的类似，就是将输入的图片进行两次转换得到解码后的图片和原来的图片之间的差距越小越好。 中间编码的向量可以见Embedding、也可以叫Representation或者叫Code。 下游任务：将图片压缩后进行相关的处理。如More Dimension Reduction 技术上可以使用Auto-encoder。 为什么使用Auto-encoder这里举了神雕侠侣的例子，就是想说明3x3的图片其实受到图片自身的限制，内部是由一定规律的，可以将其进行压缩。抑或是武器是受到手臂活动范围的约束，不可能变化很多。 由于图片的自身的约束，可以简化为低维的状态。 auto-encoder is not a new ideas.assets/image-20220209095443614.png) 之前训练是使用受限玻尔兹曼机RBM来训练深度模型。 主要是利用RBM将每一层都训练好，之后拼接起来进行微调，继而完成深度模型的预训练。 但是2012年，Hinton发表文章，说明其实RBM技术没有太大的必要，于是进年来没有人再去使用RBM了。 De-noising Auto encoder 通过encoder和decoder来去掉噪声。 BERT模型其实思想上和De-noising Auto encoder相似。 Feature DisentanglementRepresentation includes information of different aspects 但是，这些信息是纠缠在一个向量里面的，我们不知道那些维里包含有上述信息的主要内容。 Feature Disentanglement：训练出来的auto-encoder,可以将信息按照维度进行分类，不同的维度代表则不同的信息。 主要的应用是 Application: Voice Conversion 之前是需要成对的声音讯号，就是让两个人读相同的句子(监督学习)。 现在A和B不需要将同样的句子和语言，就可以实现声音的转换。 提取出我自己的内容讯号和新垣结衣的声音讯号，结合起来完成Voice Conversion。 Discrete Latent RepresentationDiscrete Representation one-hot可以实现手写数据集Mnist 的分类，一共10维，每一维代表的是一维数据的。 Codebook里面是一系列的向量，Codebook里的系列也是NN从字典（大量训练集）中学到的。 得出的code vector 和Codebook里面的所有向量进行相似度的计算，取相似度最高的向量，进行decoder。 decoder的输入必须是Codebook里面的向量，这样来来实现离散化。 这里相似度的计算和attention机制相似。 在语音上面，codebook可以学习到最基本的发音。 Text as Representation将Embedding变成一段文字，这样一段文字有可能就成为文章摘要。 encoder和decoder要是transformer机制。 但是，实际train以后，encoder生成的code是人看不懂的，但是decoder就可以看懂。 因此，要增加一个判别器，来判别是不是人话。 主要的思想几就是CycleGAN的思想。 Tree as Embedding More ApplicationsGenerator With some modification, we have variational auto encoder (VAE). 这里在得到encoder的同时，也收获到一个生成器模型(decoder)。 Compression 做图片压缩，但是会有一定得失真。 Anomaly Detection(异常检测机制) 就是给了一些列的训练资料 ${x^1, x^2,x^3,\\cdots,x^N}$。 检测输入的$x$，是否和训练数据相类似。 相似的内涵是训练数据来定的。 信用卡消费检测 Fraud DetectionTraining data: credit card transactions, 𝑥: fraud or notRef: https://www.kaggle.com/ntnu-testimon/paysim1/homeRef: https://www.kaggle.com/mlg-ulb/creditcardfraud/home 网络的侵入检测 Network Intrusion DetectionTraining data: connection, 𝑥: attack or notRef: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 癌症检测 Cancer DetectionTraining data: normal cells, 𝑥: cancer or notRef: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home 异常检测的难点在于异常的资料相当少，因此不能看作二元分类器，相当于使用一元数据进行分类。 对于相似的数据，decoder可以顺利的还原图片数据。 对于异常的图片，decoder就很难还原，因为训练的时候根本没见过这样的情况，通过这种机制来实现检测。 根据重构的误差的损失来看是否异常。 More about Anomaly DetectionPart 1: https://youtu.be/gDp2LXGnVLQPart 2: https://youtu.be/cYrNjLxkoXsPart 3: https://youtu.be/ueDlm2FkCnwPart 4: https://youtu.be/XwkHOUPbc0QPart 5: https://youtu.be/Fh1xFBktRLQPart 6: https://youtu.be/LmFWzmn2rFYPart 7: https://youtu.be/6W8FqUGYyDo","link":"/2022/02/09/Auto-encoder/Auto-encoder-2021/"},{"title":"Pytorch配置和初识篇","text":"hljs.initHighlightingOnLoad(); 本文主要从Pytorch的环境配置和安装、编辑器的配置教程、python学习中法宝以及Pycharm 和 Jupyter 的对比四个方面进行介绍。主要的视频教程 。 Pytorch的环境配置和安装主要内容可以参考视频教程 这里主要记录如何查看GPU配置。 首先，可以打开任务管理器直接查看显卡的配置。 打开在cmd里输入nvidia-smi 来查看当前显卡的CUDA版本。 如果显示没有该命令，则可以参考以下教程。 总结，在选择显卡pytorch对应的显卡的配置时 如果无英伟达显卡:CUDA选择None； 如果有英伟达显卡:CUDA选择9.2 编辑器的配置教程这里主要以jupyter notebook为例，详细教程请参考以下视频。 首先，打开Anaconda Prompt，激活需要的环境。 之后，conda list来查看当前已经安装好的包。 然后，安装conda install nb_conda 最后，启动即可，cmd中输入jupyter notebook 测试安装。 12import torchtorch.cuda.is_available() 但是，这个操作我这边一直行不通，我选择直接修改kernel的.json文件。 python学习中法宝 dir()函数，能让我们知道工具箱以及工具箱中的分隔区有什么东西。 help()函数，能让我们知道每个工具是如何使用的，工具的使用方法。 实战演练： 以torch为例，我们来查找torch这个包下面包含的分区。 1234dir(torch.cuda.is_available)# 返回该函数下面的属性dir(torch.cuda.is_available())# 返回的是该函数返回值下面的属性，这里返回的是bool变量的属性 help的使用也是同理的。 返回是一个bool值，表示cuda是否可用。 如果加上括号就得出bool值得帮助。 Pycharm 和 Jupyter 的对比 python的文件是整体是一个块，如果一步出错编译以后再次运行是从头开始运行的。效率低下，每次都要重新来一次 优点：通用，传播方便，适用于大型项目 缺点：需要从头运行 python控制台，是一句是一个块，修改之后就是从出错的地方开始往下运行。代码的阅读性不好 其实可以多行输入，在输入完一句之后可以Ctrl+Enter，出现三个点，继续输入下一句。 优点：显示每个变量属性 缺点：不利于代码阅读及修改 jupyter可以自己定义代码块的大小，可以是一句，也可以是多句。功能介于二者之间 优点：利于代码的阅读和修改 缺点：环境需要配置","link":"/2022/02/10/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E9%85%8D%E7%BD%AE%E5%92%8C%E5%88%9D%E8%AF%86%E7%AF%87/"},{"title":"Pytorch数据处理和可视化篇","text":"hljs.initHighlightingOnLoad(); 本文主要从Pytorch数据加载的初识、数据集Dataset的代码实践、TensorBoard基础教程、transforms进行图片的预处理、Torchvision里数据集的使用以及DataLoader的使用五个大方面来介绍Pytorch在运行过程中数据的修饰和预处理过程。主要的视频教程 。 Pytorch数据加载的初识 数据：可以比作垃圾的海洋，从数据(垃圾)中获取由于的东西。 Dataset：取出其中的某一类有用的数据(垃圾)，并获取对应的label值。 Dateset的几种组织形式： 第一种是文件夹的名称为对应的label。 第二种是图片和文件夹分属在不同的文件夹中，label以.txt形式保存。 DataLoader：将数据进行打包和压缩，为后面的网络提供不同的数据形式。 Dataset类的使用： 1234567from torch.utils.data import Dataset# 导入dataset类help(Dataset)# 获取简易的把帮助Dataset??# 在jupyter里获取详细的帮助# 在pycharm里一般是按住ctrl再点击dataset dataset是一个抽象类，所有的子类都应该重写__getitme__方法，这个方法是用来获取数据集对应的label的。 也可以选择重写__len__用来获取数据的长度。 12345def __getitem__(self, index): raise NotImplementedErrordef __add__(self, other): return ConcatDataset([self, other]) getitem的作用：使得数据集可以支持下标的引用 len的作用：使得数据集类可以通过len()函数来返回我们的长度 数据集Dataset的代码实践图片的读取12345# 测试图片的属性from PIL import Imageimg_path=&quot;flower_photos\\\\daisy\\\\5673551_01d1ea993e_n.jpg&quot;img.size # 测试证明读取数据成功img.show() # 展示图片 路径的读取12345678910import os# 用来读取文件夹地址的包dir_path=&quot;flower_photos/roses&quot;img_path_list=os.listdir(dir_path)# 读取指定的文件夹下的文件名，以list的形式读取文件名root_dir=&quot;flower_photos&quot;label_dir=&quot;roses&quot;path=os.path.join(root_dir,label_dir)# 该函数可以按照不同的系统来分配适当的连接符# 对于windows 用\\\\连接 对于linux使用一般的 / 连接 构建数据集类1234567891011121314151617class MyData(Dataset): def __init__(self, root_dir, label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir, self.label_dir) self.img_path = os.listdir(self.path) # 得到文件下面所有图片的名称，组成一个列表 def __getitem__(self, idx): img_name = self.img_path[idx] img_item = os.path.join(self.root_dir, self.label_dir, img_name) img = Image.open(img_item) label = self.label_dir return img, label def __len__(self): return len(self.img_path) 带双下划线的是属性函数，是自动调用的函数，不需要显式的调用。如__len__()起作用是当我们调用len(MyData)时。 测试数据集的测试123456789# 图片的读取root_dir = &quot;flower_photos&quot;label_list = os.listdir(root_dir)label_dir = label_list[1]flower_set = MyData(root_dir, label_dir)# flower_set对象其实是一个图像集，靠索引来读取数据img, label = flower_set[1]img.show() __getitem__起作用时我们以索引的方式调用数据集对象时。 程序的输出如下图所示： 1234567# 数据集的拼接roses_dir = label_list[3]daisy_dir = label_list[0]rose_set = MyData(root_dir, roses_dir)daisy_set = MyData(root_dir, daisy_dir)flower_set = rose_set + daisy_setprint(len(rose_set), len(daisy_set), len(flower_set)) 通过加号可以直接实现两个数据集的合并。 以下代码创建第二类，标签和图片分里的数据集的代码(和本实验不同，大概读懂意思即可，可以作为模板来套用)： 12345678910root_dir = &quot;dataset/train&quot;target_dir = &quot;ants_image&quot;img_path = os.listdir(os.path.join(root_dir，arget_dir))label = target_dir.split('_')[0]# 相当于去除ants_image的第一个字antsout_dir = &quot;ants_label&quot;for i in img_path: file_name = i.split(&quot;.jpg&quot;)[0] with open(os.path.join(root_dir,out_dir,&quot;{}.txt&quot;.format(file_name)),'w') as f; f.write(label) TensorBoard基础教程TensorBoard的安装tensorboard起初只能tensorflow才能使用，后来pytorch1.1之后也可以使用了。 SummaryWriter类的使用TensorBoard的帮助文档如下图所示： 主要是想log_dir中写内容的事件文件，主要的参数是前两个参数，后面的参数直接使用默认值就好了。 1234567891011121314Examples::from torch.utils.tensorboard import SummaryWriter# create a summary writer with automatically generated folder name.writer = SummaryWriter()# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/# create a summary writer using the specified folder name.writer = SummaryWriter(&quot;my_experiment&quot;)# folder location: my_experiment# create a summary writer with comment appended.writer = SummaryWriter(comment=&quot;LR_0.1_BATCH_16&quot;)# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/ 这里初始化类的主要过程，可以直接初始化，此时文件保存在默认的路径下面，location: runs/May04_22-14-54_s-MacBook-Pro.local/下。 也可以设置保存的路径为my_experiment ，此时文件保存在my_experiment下。 pycharm 和 jupyter中的注释使用ctrl+/ texstdio中的注释是ctrl+t add_scalar()的使用add_scalar()经常用来绘制train/val_loss。 tag：表示的是图表的数值。 scalar_value：表示需要去保存的数值的规模。(Y轴的规模) global_step：表示需要保存多少步。(X轴的规模) 1234567from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter(&quot;logs&quot;)# 将事件文件保存在logs文件下# y = xfor i in range(100): writer.add_scalar(tag=&quot;y=x&quot;, scalar_value=i, global_step=i)writer.close() tensorboard的打开，这里的logdir=事件文件所在文件夹名,切记不可以有中文字符 12345tensorboard --logdir=logs# logdir=事件文件所在文件夹名# 不可以有中文字符# 有时实验室服务器的6006端口比较紧张，可以自行设计打开的端口tensorboard --logdir=logs --port=6007 tensorboard打不开的原因：浏览器的问题，换成原始的Edge就没问题了。 如果tag=“ ”忘记改名，就可能出现如下多个内容画到了同一图上面的情况。 add_image()的使用 其他的参数大概都是和add_scalar相同，但是图片的数据类型要注意，这里img_tensor要是tensor类型和numpy.array类型的。 如图所示，PIL读取的数据类型不符合要求，因此这里使用opencv来读取图片数据，opencv读取得到的类型是array类型的。 也可以直接将PIL的类型转换为arrry。 但是，这里要注意，输入图片的维度也是有要求的，默认是(3,H,W)类型的，如果要改成(3,H,W)需要修改dataformats='CHW'。 12345678910111213141516from torch.utils.tensorboard import SummaryWriterimport numpy as npimg = np.zeros((3, 100, 100))img[0] = np.arange(0, 10000).reshape(100, 100) / 10000img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000img_HWC = np.zeros((100, 100, 3))img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000writer = SummaryWriter()writer.add_image('my_image', img, 0)# If you have non-default dimension setting, set the dataformats argument.writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')writer.close() 123456789101112131415from torch.utils.tensorboard import SummaryWriterimport numpy as npfrom PIL import Imagewriter = SummaryWriter(&quot;logs&quot;)# 将事件文件保存在logs文件下img_path = &quot;flower_photos/roses/145862135_ab710de93c_n.jpg&quot;img_PIL = Image.open(img_path)img_array = np.array(img_PIL)print(type(img_array))# 是np.array类型，但是数据的维度排列不是很匹配print(img_array.shape)writer.add_image(&quot;roses&quot;, img_array, 1, dataformats='HWC')writer.close() 从PIL到numpy，需要在add_image()中指定shape中每一个数字/维表示的含义 如果tag不变，所有的图像都会画在同一个tag的下面，注意部署一定要修改，不然就会直接覆盖，通过左右滑动来切换。 transforms进行图片的预处理 transforms的结构和用法 整理，在pycharm中打开文档，利用structure来分析器结构。 快捷键设置：到setting-&gt;keymap-&gt;搜索目标关键字即可 小贴士：alt+7可以调出structure的栏目，主要可以用来显示我们的函数里所包含的类（功能）和变量 该包相当于一个工具的模具（模板） （ctrl+p用来显示数该函数的变量信息） tensor数据类型整理通过tensorforms.ToTensor()去理解tensors的数据类型的问题。 这里的__call__代表的是传入的参数应该的什么类型。 1234567891011from torchvision.transforms import transformsfrom PIL import Imageimg_path = &quot;flower_photos/tulips/10791227_7168491604.jpg&quot;img = Image.open(img_path)# 使用Totensortensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)print(tensor_img) 这里Totensor()是一个类，需要实例化才能使用。 以下为我们转移到交互窗口的进行查看过程的数据。tensor的数据类型一共包含以下数据的属性和类型。 pycharm 出现问题时，使用alt+enter可以弹出相应的解决方案。 常见的transforms ToTensor()的使用 1234567891011from torchvision.transforms import transformsfrom PIL import Imageimg_path = &quot;flower_photos/tulips/10791227_7168491604.jpg&quot;img = Image.open(img_path)# 使用Totensortensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)print(tensor_img) Normalized()的用法 求图片张量的每一个channel的平均值和标准差。 1234trans_norm = transforms.Normalize([.5, .5, .5], [.5, .5, .5])img_norm = trans_norm(tensor_img)writer.add_image(&quot;Normalized&quot;, img_norm) 公式主要的作用相当于是改变数据分布的范围。 $output[channel] = \\dfrac {input[channel] - mean[channel]} {std[channel]}$ PyCharm小技巧设置:忽略大小写，进行提示匹配一般情况下，你需要输入R，才能提示出Resize 我们想设置，即便你输入的是r，也能提示出Resize 也就是忽略了大小写进行匹配提示。 Resize()的作用 主要的作用是按照指定的长宽来缩放图片。 如果输入(h,w)，则将图片变为指定的 (h,w) 如果输入 int a (小于最短的边)，则将图片的最小边变为a，长边按照比例来缩放。如果大于最长的边，图片最长边为a，最短边就按比例缩放。 参数必须为一个图片PIL，不可以是一个tensor，因此在compose里排列的顺序一定要先Resize后Totensor。 代码如下： 123456789101112# Reszie# 注意先后的顺序，先Reszie，后ToTensor# img PIL -&gt; resize -&gt; img_resize PILtrans_resize = transforms.Resize((512, 512))img_resize = trans_resize(img)# img_resize PIL -&gt; totensor -&gt; img_resizetensortensor_trans = transforms.ToTensor()img_resize = tensor_trans(img_resize)writer.add_image(&quot;Reszie&quot;, img_resize)# ------------------------------------------------- Compose()的使用 这个类的主要作用是将不同的transforms类进行合并操作。 Compose()中的参数需要是一个列表Python中，列表的表示形式为[数据1，数据2，…] 在Compose中，数据需要是transforms类型所以得到，Compose([transforms参数1, transforms参数2, …]) 1234567891011# Compose# 注意先后的顺序，先Reszie，后ToTensortrans_resize = transforms.Resize(512)tensor_trans = transforms.ToTensor()trans_compose = transforms.Compose([trans_resize, tensor_trans])# 也可以不用实例化，直接生成trans_compose = transforms.Compose([ transforms.Resize(512), transforms.ToTensor()])img_resize_2 = trans_compose(img) RandomCrop的用法 主要的作用：对图像进行随机裁剪 参数：size的如果是序列(h,w)，则将图片变为指定的 (h,w)，如果输入 int a (小于最短的边)，则将图片的最小边变为a，长边按照比例来缩放。如果大于最长的边，图片最长边为a，最短边就按比例缩放。 123456# Randomcroptrans_crop = transforms.RandomCrop(100, 80)trans_compose1 = transforms.Compose([trans_crop, tensor_trans])for i in range(10): img_crop = trans_compose1(img) writer.add_image(&quot;RandomCrop100,80&quot;, img_crop, i) python__call__的用法1234567891011121314class Person: def __call__(self, name): print(&quot;__call__&quot;+&quot;HELLO &quot;+name) def hello(self, name): print(&quot;hello &quot;+name)person = Person()person(&quot;Tom&quot;)person.hello(&quot;Tom&quot;)# output:&gt;&gt;&gt; __call__HELLO Tom&gt;&gt;&gt; hello Tom call的意思其实就是调用，使用对象加()即可作为一个函数来调用，是一个内置的方法。 hello()是对象配置的一个属性，只能用.hello()来显式调用。 总结：首先，注意输入和输出；其次，关注官方文档；然后，关注这个方法需要的参数。 Torchvision里数据集的使用这里Torchvision是主要服务计算机视觉的包，主要提供一些数据集和模型来以及一些预训练号的参数来方便训练。 CIFARCLASStorchvision.datasets.CIFAR10(root: str, train: bool = True, transform:Optional[Callable] = None, target_transform: Optional[Callable] = None, download:bool = False)[SOURCE] CIFAR10 Dataset的参数： root (string) – Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. 主要是保存文件夹的位置。 train (bool, optional) – If True, creates dataset from training set, otherwise creates from test set.是否为训练集True代表是训练集。 transform (callable**, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop使用的transforms的格式。 target_transform (callable**, optional) – A function/transform that takes in the target and transforms it. download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.是否下载。 这里如果下载速度过慢，可以使用迅雷进行加速。 12345678910111213141516171819202122232425262728import torchvisionfrom torch.utils.tensorboard import SummaryWriterdataset_trans = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()])train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=dataset_trans, download=True)test_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=False, transform=dataset_trans, download=True)# 读取测试img, target = train_set[0]print(train_set[0])# 转换字典的键值idx_classes = {}idx = 0for name in train_set.class_to_idx: idx_classes[idx] = name idx = idx + 1print(idx_classes[target])# tensorboard可视化writer = SummaryWriter(&quot;p10&quot;)for i in range(10): img, target = test_set[i] writer.add_image(&quot;test&quot;, img, i)writer.close() 源代码里有相关的下载链接。 DataLoader的使用dataset相当于一组牌，dataloader相当于如何抽一组牌。 torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=2, persistent_workers=False)[SOURCE]** Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See torch.utils.data documentation page for more details. Parameters主要参数： dataset (Dataset) – dataset from which to load the data.之前我们自己创建或者导入的数据集。 batch_size (int, optional) – how many samples per batch to load (default: 1).每一批取到的数据量，每一批的数据量越大越好。 shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).是否拉乱顺序，对于训练集，最好打乱顺序 sampler (Sampler or Iterable**, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified. batch_sampler (Sampler or Iterable**, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)进程数，但是windows只能设为0 collate_fn (callable**, optional) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory (bool, optional) – If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below. drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) 每次按照批量来存取，最后不能除尽的几张是否舍去。 timeout (numeric**, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) worker_init_fn (callable**, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) generator (torch.Generator, optional) – If not None, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate base_seed for workers. (default: None) prefetch_factor (int, optional**, keyword-only arg) – Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. (default: 2) persistent_workers (bool, optional) – If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: False) dataloader是将img和target分别进行打包，在dim=0上增加一个维度。 12345678910111213141516171819202122232425262728293031323334import torchvisionfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoaderdataset_trans = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()])train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;,train=True,transform=dataset_trans,download=True)test_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;,train=False,transform=dataset_trans,download=True)## 设置DataLoader类test_loader = DataLoader(dataset=test_set, batch_size=4, num_workers=0, drop_last=False, shuffle=False)# 测试数据中的每一张图片及targetimg, target = test_set[0]print(img.shape)print(target)# 测试dataloaderfor data in test_loader: imgs, targets = data print(imgs.shape) print(targets)&gt;&gt;&gt;output:torch.Size([3, 32, 32])3torch.Size([4, 3, 32, 32])tensor([3, 8, 8, 0]) 这里我们通过tensorboard来测试其效果 12345678910writer = SummaryWriter(&quot;dataloader&quot;)# 测试dataloaderfor epoch in range(2): for idx, data in enumerate(test_loader): imgs, targets = data # print(imgs.shape) # print(targets) # 由于每次输出为多张图片，这里将其平铺起来 writer.add_images(&quot;Epoch:{}&quot;.format(epoch), imgs, idx)writer.close() 这里，我们选择留下未除尽的内容，shuffle=False时，二者一模一样。反之，两次就不相同了。","link":"/2022/02/10/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/pytorch%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AF%87/"},{"title":"DCGAN 的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要根据视频教程来搭建的一个卷积对抗网路DCGAN模型，相比之前的网络是将线性层换成卷积层和反卷积层，数据集只要需用的是Mnist手写数据集，主要从模型搭建、参数设置和模型训练三个方面进行讲解。 原论文要点 为了保证网络的稳定性，DCGAN去掉了最大池化层和平均池化层。 再判别器和生成器中使用了BatchNorm。 去掉的全连接隐藏层。 在生成器中使用ReLU激活函数，除了最后一个输出层。 在判别器中使用LeakyReLU激活函数。 导入相关包12345678import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torchvision.datasets as datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterimport torchvision.transforms as transforms 模型搭建判别器1234567891011121314151617181920212223242526272829303132333435363738class Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 nn.Sigmoid(), ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x): return self.disc(x) 主要的区别是将线性连接换成了卷积，其他不变。 生成器123456789101112131415161718192021222324252627282930313233343536class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x) 这里生成器主要使用的是反卷积操作。 初始化和测试1234567891011121314151617181920def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02)def test(): N, in_channels, H, W = 8, 3, 64, 64 z_dim = 100 x = torch.randn((N, in_channels, H, W)) disc = Discriminator(in_channels, 8) initialize_weights(disc) tmp = disc(x).shape == (N, 1, 1, 1) assert tmp gen = Generator(z_dim, in_channels, 8) z = torch.randn((N, z_dim, 1, 1)) tmp = gen(z).shape == (N, in_channels, H, W) assert tmp print(&quot;Success&quot;) 参数设置超参数设计1234567891011121314# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 2e-4# 3e-4 是对于Adam 来说最好的学习率z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 128num_epoch = 50channels_img = 1features_disc = 64features_gen = 64 features_disc、features_gen分别为判别器和生成器卷积层中的特征通道数。 channels_img = 1数据的通道数。 数据初始化123456789101112131415161718transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) 初始化模型123disc = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) 损失函数与与优化器的初始化123456789# 设置优化器opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))# 可以考虑设置betas=的参数opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数criterion = nn.BCELoss() 模型训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869writer_fake = SummaryWriter(f&quot;runs/DCGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/DCGAN_MNIST/real&quot;)step = 0# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size batch_size = real.shape[0] # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) disc_real = disc(real).reshape(-1) # view()也可以 lossD_real = criterion(disc_real, torch.ones_like(disc_real)) disc_fake = disc(fake).reshape(-1) # view()也可以 lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_fake + lossD_real) / 2 # 初始化梯度 disc.zero_grad() # 反向传播 lossD.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc(fake).view(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 lossG = criterion(output, torch.ones_like(output)) gen.zero_grad() lossG.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)}\\ &quot; f&quot;Loss D: {lossD:.4f}, Loss G: {lossG:.4f}&quot; ) with torch.no_grad(): fake = gen(fixed_noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 这里主要和Simple GAN的区别是，数据的维度上的处理，之前是将图片展成一组向量，而且batch_size也有一定的改变。","link":"/2022/02/11/Generative%20Adversarial%20Networks/DCGAN-%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Simple NN-GAN的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要根据视频教程来搭建的一个只有线性层的简单神经网络，数据集只要需用的是Mnist手写数据集，主要从模型搭建、参数设置和模型训练三个方面进行讲解。 导入相关包12345678import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torchvision.datasets as datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterimport torchvision.transforms as transforms 模型搭建判别器12345678910111213141516class Discriminator(nn.Module): def __init__(self, img_dim): super().__init__() self.disc = nn.Sequential( nn.Linear(in_features=img_dim, out_features=128), nn.LeakyReLU(0.1), # 在GAN里面LeakyReLU的效果比ReLU好 nn.Linear(128, 1), nn.Sigmoid(), # 因为输出是0和1的布尔值 # 因此这里的激活函数选用Sigmoid ) def forward(self, x): return self.disc(x) 这里使用的是最简单的现象模型来搭建的，基本的框架就是这样。 在GAN里面激活函数LeakyReLU的效果比ReLU好。 因为输出是0和1的布尔值， 因此这里的激活函数选用Sigmoid。 生成器1234567891011121314151617class Generator(nn.Module): def __init__(self, z_dim, img_dim): # z_dim 生成噪声的维度 # img_dim 图片的维度 super().__init__() self.gen = nn.Sequential( nn.Linear(z_dim, 256), nn.LeakyReLU(0.1), nn.Linear(256, img_dim), # 28x28x1 -&gt; 784 nn.Tanh() # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def forward(self, x): return self.gen(x) 参数设置超参数设计1234567891011121314151617181920# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 3e-4# lr=1e-4# 3e-4 是对于Adam 来说最好的学习率z_dim = 64# 同样的可以尝试 128, 256image_dim = 28 * 28 * 1 # 784# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 32num_epoch = 50transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数]) 超参数里最基本有敏感度的是学习率，lr=3e-4 是对于Adam 来说最好的学习率，有时候也会用lr=1e-4。 其次就是标准化参数的影响是最大的。 然后再考虑生成噪声数据的维度、每一批次的数据量batch_size(根据自己的内存来考虑，量力而行) 最后考虑总的循环数，一般是迭代次数越多越好。 实例化模型123disc = Discriminator(image_dim).to(device)gen = Generator(z_dim, image_dim).to(device)fixed_noise = torch.randn((batch_size, z_dim)).to(device) 数据预处理、损失函数和优化器123456789101112131415161718dataset = datasets.MNIST(root=&quot;/dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)# 设置优化器opt_disc = optim.Adam(disc.parameters(), lr=lr)# 可以考虑设置betas=的参数opt_gen = optim.Adam(gen.parameters(), lr=lr)# 损失函数criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/GAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/GAN_MNIST/real&quot;) GAN的优化器选择的是带有动量的Adam优化器。 损失函数选择的是二元交叉熵函数。 Parameters weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.权重参数。 size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string**, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean' 模型训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465step = 0# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.view(-1, 784).to(device) # 将图片后两维展平 第一维为batch_size batch_size = real.shape[0] # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn(batch_size, z_dim).to(device) fake = gen(noise) # G(z) disc_real = disc(real).view(-1) lossD_real = criterion(disc_real, torch.ones_like(disc_real)) disc_fake = disc(fake).view(-1) lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_fake + lossD_real) / 2 # 初始化梯度 disc.zero_grad() # 反向传播 lossD.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max log(D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc(fake).view(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 lossG = criterion(output, torch.ones_like(output)) gen.zero_grad() lossG.backward() opt_gen.step() if batch_idx == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] \\ &quot; f&quot;Loss D: {lossD:.4f} ,Loss G: {lossG:.4f}&quot; ) with torch.no_grad(): fake = gen(fixed_noise).reshape(-1, 1, 28, 28) data = real.reshape(-1, 1, 28, 28) img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( data, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 对于判别器来说，其目标函数为： \\max \\{\\log(D(x_{real})) + \\log(1-D(G(z))\\}\\\\ \\Leftrightarrow \\min \\{ -\\log(D(x_{real})) - \\log(1-D(G(z))\\} 这里我们选择的损失函数是BCLoss： L(x,y) = -[y\\log(x) + (1-y)\\log(1-x)]\\\\ L(x,y)= \\begin{cases} -\\log(x), & if\\quad y=1 \\\\ -\\log(1-x), & if\\quad y=0 \\end{cases}因此这里选择torch.ones_like()和torch.zeros_like()来作为y参数。 对于生成器来说，其目标函数为： \\min \\log(1-D(G(z)))由于训练开始时的梯度较小，可能会导致生成器训练不动，生成器过早收敛。于是将目标函数转换为： \\min -\\log(D(G(z))) 同时生成器中的D时固定的，利用的是上一轮中D的参数，因此这里要将其固定： 12345# 方法1:保持动态图不变lossD.backward(retain_grap=True)# 方法2:保持计算出来的fake&lt;-&gt;G(z)保留fake = gen(noise)disc_fake = disc(fake.detach()).view(-1) detach()返回一个新的tensor，是从当前计算图中分离下来的，但是仍指向原变量的存放位置，其grad_fn=None且requires_grad=False，得到的这个tensor永远不需要计算其梯度，不具有梯度grad，即使之后重新将它的requires_grad置为true,它也不会具有梯度grad。 注意：返回的tensor和原始的tensor共享同一内存数据。in-place函数修改会在两个tensor上同时体现(因为它们共享内存数据)，此时当要对其调用backward()时可能会导致错误。 测试是将生成器作用到一个固定的噪声数据中。 1fake = gen(fixed_noise).reshape(-1, 1, 28, 28) 这里是将数据拼接成为网格形状 1img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)","link":"/2022/02/11/Generative%20Adversarial%20Networks/Simple-NN-GAN%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"WGAN-GP的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要介绍了WGAN-GP的理论和pytorch实现，主要的相比之前WGAN修改了生成器的归一化层、新加入了一些超参数、增加了新的计算梯度惩罚项的函数、去掉了原来的损失函数并在模型训练中定义了新的损失函数。 WGAN-GP理论简介WGAN-GP是WGAN的改进版，因为截断操作一方面浪费时间，其次效果实在一般，还不如DCGAN，因此这里带有梯度惩罚项的函数来进行优化操作。 这里的主要参数如下： $\\lambda=10$ 是惩罚项的系数，其代表的是梯度惩罚项前面的权重。 $\\alpha=1e-4$ 是学习率，$\\beta_1=0 \\ ,\\beta_2=0.9$ 是Adam优化器的动量参数。 每个循环里生成器训练一次对应的判别器训练的次数$n_{critic} = 5$ $\\hat{\\boldsymbol{x}} \\leftarrow \\epsilon \\boldsymbol{x}+(1-\\epsilon) \\tilde{\\boldsymbol{x}}$ 代表的是计算真实图片和生成图片的加权值。 $\\lambda\\left(\\left|\\nabla_{\\hat{\\boldsymbol{x}}} D_{w}(\\hat{\\boldsymbol{x}})\\right|_{2}-1\\right)^{2}$ 之后将加权值求范数，如果范数值为1，则此时满足$1-Lipschitz$ 稳定性。否则，作为惩罚项加入到损失函数中。 生长器也是使用Adam优化器，其他的地方不变。 模型搭建生成器和判别器这里使用的是将生成器的BatchNorm2d换成了InstanceNorm2d，关于InstanceNorm2d的参考CSDN博文 。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import torchimport torch.nn as nnclass Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 # nn.Sigmoid(), 判别器取消了Sigmoid() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), # 整理修改为InstanceNorm2d # LayerNorm &lt;-&gt; InstanceNorm nn.InstanceNorm2d(out_channels, affine=True), nn.LeakyReLU(0.2), ) def forward(self, x): return self.disc(x)class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x)def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02) 梯度惩罚项函数为了求梯度惩罚，这里增加了求梯度惩罚项的函数如下： 1234567891011121314151617181920212223242526import torchimport torch.nndef gradient_penalty(critic, real, fake, device=&quot;cpu&quot;): batch_size, channels, H, W = real.shape epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, channels, H, W).to(device) interpolated_images = real * epsilon + fake * (1 - epsilon) # 计算判别器输出 mix_scores = critic(interpolated_images) # 求导 gradient = torch.autograd.grad( inputs=interpolated_images, outputs=mix_scores, grad_outputs=torch.ones_like(mix_scores), create_graph=True, retain_graph=True, )[0] gradient = gradient.view(gradient.shape[0], -1) gradient_norm = gradient.norm(2, dim=1) # 求2-范数 gradient_penalty_value = torch.mean((gradient_norm - 1) ** 2) return gradient_penalty_value 参数设计超参数的设计这里参数选择参考理论简介部分。以下为具体的代码： 123456789101112131415161718# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 1e-4# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 6channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5# weight_clip = 0.01LAMBEDA_GP = 10 优化器和损失函数的设置注意WGAN的损失函数都是自己写的，因此不设置损失函数了。 123456789101112# 设置优化器opt_disc = optim.RMSprop(disc_critic.parameters(), lr=lr)opt_gen = optim.RMSprop(gen.parameters(), lr=lr)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/WGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/WGAN_MNIST/real&quot;) 模型初始化和数据预处理这边也保持不变。 12345678910111213141516171819202122232425transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen) 模型训练主要的修改是给生成器去掉的参数截断功能，损失函数加入了求梯度惩罚项的部分，具体原理参考理论简介。 损失函数： L^{(i)} \\leftarrow D_{w}(\\tilde{\\boldsymbol{x}})-D_{w}(\\boldsymbol{x})+\\lambda\\left(\\left\\|\\nabla_{\\hat{\\boldsymbol{x}}} D_{w}(\\hat{\\boldsymbol{x}})\\right\\|_{2}-1\\right)^{2}主要的代码如下： 12345678910noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)fake = gen(noise) # G(z)critic_real = disc_critic(real).reshape(-1)critic_fake = disc_critic(fake).reshape(-1)gp = gradient_penalty(disc_critic, real, fake, device=device)# 这里自定义了损失函数# (f-GAN论文讲了，其实损失函数的设置对结果的影响不大)loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp) 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) critic_real = disc_critic(real).reshape(-1) critic_fake = disc_critic(fake).reshape(-1) gp = gradient_penalty(disc_critic, real, fake, device=device) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp ) # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(fixed_noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train() 相比WGAN，加入梯度惩罚项以后的效果还是比较好的。","link":"/2022/02/12/Generative%20Adversarial%20Networks/WGAN-GP%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"WGAN的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要介绍了WGAN的理论和pytorch实现，主要的相比之前DCGAN修改了模型搭建部分的激活函数、新加入了一些超参数、去掉了原来的损失函数并在模型训练中定义了损失函数。 WGAN理论简介这里有时间去复习李宏毅的WGAN相关理论，这里直接从代码部分讲起。 参考公式如图所示： 学习率$\\alpha$=5e-5, 截断系数c=0.01, 每一批的数据batch_size=64, 以及每个循环里生成器训练一次对应的判别器训练的次数$n_{critic} = 5$ 这里的 $\\theta$ 代表生成器的参数，$\\omega$ 代表判别器的参数。循环结束的条件是生成器的参数 $\\theta$ 是否收敛。 为了保证数学上的具有$1-Lipschitz$ 连续性(保证函数是光滑的)， 于是增加了截断误差，就是将判别器的参数 $\\omega$ 裁剪到一个小的范围之内，[-0.01, 0.01]。 优化器使用的是RMSProp优化器，替代了原来的Adma优化器。 模型搭建模型的结构是基本不变的，判别器的最后一层去掉了Sigmoid()函数。 其他的地方均和DCGAN相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import torchimport torch.nn as nnclass Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 # nn.Sigmoid(), 判别器取消了Sigmoid() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x): return self.disc(x)class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x)def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02) 参数设置超参数的设置这里参数选择参考理论简介部分。以下为具体的代码： 1234567891011121314151617# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 5e-5# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 6channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5weight_clip = 0.05 优化器和损失函数的设置注意WGAN的损失函数都是自己写的，因此不设置损失函数了。 123456789101112# 设置优化器opt_disc = optim.RMSprop(disc_critic.parameters(), lr=lr)opt_gen = optim.RMSprop(gen.parameters(), lr=lr)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/WGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/WGAN_MNIST/real&quot;) 模型初始化和数据预处理这边也保持不变。 12345678910111213141516171819202122232425transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen) 模型训练 这边也不变，注意这里有BN层，需要加model.train() 函数。 损失函数： 判别器： g_{w} \\leftarrow \\nabla_{w}\\left[\\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(x^{(i)}\\right)-\\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(g_{\\theta}\\left(z^{(i)}\\right)\\right)\\right]\\\\ w \\leftarrow w+\\alpha \\cdot \\mathrm{RMSProp}\\left(w, g_{w}\\right)​ 生成器： g_{\\theta} \\leftarrow \\nabla_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(g_{\\theta}\\left(z^{(i)}\\right)\\right)12345678noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)fake = gen(noise) # G(z)critic_real = disc_critic(real).reshape(-1)critic_fake = disc_critic(fake).reshape(-1)# 这里自定义了损失函数# (f-GAN论文讲了，其实损失函数的设置对结果的影响不大)loss_critic = - (torch.mean(critic_real) - torch.mean(critic_fake))# - 负号为了保证最小化 这里损失函数为判别器计算出来的数值取平均，即可。 训练的顺序是每次先训练$n_{critic}$次判别器，之后再训练1次生成器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) critic_real = disc_critic(real).reshape(-1) critic_fake = disc_critic(fake).reshape(-1) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = - (torch.mean(critic_real) - torch.mean(critic_fake)) # - 负号为了保证最小化 # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # 参数进行截断 for p in disc_critic.parameters(): p.data.clamp_(-weight_clip, weight_clip) # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train() 总结： 其实WGAN的效果在一般的Mnist数据集上的效果不是很好，因此尽管数学上的效果很好，但是试验证实的效果还是不理想。","link":"/2022/02/12/Generative%20Adversarial%20Networks/WGAN%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Condition_WGAN_GP的搭建","text":"hljs.initHighlightingOnLoad(); 这个模型和ACGAN有相似之处，都增加了标签的embedding层，用来辅助训练。这样可以实现训练的结果和原来标签实现一一对应。 本文主要是在WGAN-GP的基础上修改，主要增加了和label与embedding相关的参数，并修改了模型。但是训练的效果不是很满意，收敛的速度不如WGAN-GP。 模型搭建 在模型搭建上，该模型主要增加了label的embedding层，并将embedding层和原来的输入层进行堆叠。 判别器 增加了embedding层： 增加了与labels和embedding层相关的参数： 前向传播函数增加了embedding的内容： 完整代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Discriminator(nn.Module): def __init__(self, channel_img, features_d, num_classes, img_size): super(Discriminator, self).__init__() self.img_size = img_size self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img+1, # 由于增加了一个编码层 因此要加1 out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 nn.Sigmoid(), ) # 增加一个编码器 将一个label编码为一个图片 self.embed = nn.Embedding( num_embeddings=num_classes, embedding_dim=img_size*img_size ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x, labels): # label -&gt; [batch_size, 1] -&gt; N x 1 embedding = self.embed(labels).view([labels.shape[0], 1, self.img_size, self.img_size]) # embedding -&gt; N x 1 x img_size(64) x img_size x = torch.cat([x, embedding], dim=1) # x -&gt; N x channel_img x img_size(64) x img_size # dim=1 代表在第二维上进行堆叠 就是在channel上堆叠 return self.disc(x) 生成器 embed_size：编码器将num_classes维转换为embed_size。 新增embedding层： 修改前向传播函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.img_size = img_size self.gen = nn.Sequential( # Input: N x (z_dim + embed_size) x 1 x 1 self._block(z_dim + embed_size, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) # 编码的维度要跟噪声的维度相同 self.embed = nn.Embedding(num_embeddings=num_classes, embedding_dim=embed_size) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x, labels): embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) # unsqueeze()的作用是增加维度，这里增加了第3，4维 # embedding -&gt; N x embed_size x 1 x 1 # x -&gt; N x z_dim x 1 x 1 x = torch.cat([x, embedding], dim=1) return self.gen(x) 梯度惩罚项增加了和label相关的参数即可。 12345678910111213141516171819202122def gradient_penalty(critic, real, labels, fake, device=&quot;cpu&quot;): batch_size, channels, H, W = real.shape epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, channels, H, W).to(device) interpolated_images = real * epsilon + fake * (1 - epsilon) # 计算判别器输出 mix_scores = critic(interpolated_images, labels) # 求导 gradient = torch.autograd.grad( inputs=interpolated_images, outputs=mix_scores, grad_outputs=torch.ones_like(mix_scores), create_graph=True, retain_graph=True, )[0] gradient = gradient.view(gradient.shape[0], -1) gradient_norm = gradient.norm(2, dim=1) # 求2-范数 gradient_penalty_value = torch.mean((gradient_norm - 1) ** 2) return gradient_penalty_value 参数设置123# 新增编码参数num_classes = 10gen_embedding = 100 其他参数和原来相似，完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 5e-5# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 3channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5# weight_clip = 0.01LAMBEDA_GP = 10# 新增编码参数num_classes = 10gen_embedding = 100transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc, num_classes, image_size).to(device)gen = Generator(z_dim, channels_img, features_gen, num_classes, image_size, gen_embedding).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen)# 设置优化器opt_disc = optim.Adam(disc_critic.parameters(), lr=lr, betas=(0.0, 0.9))opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.9))fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/CWGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/CWGAN_MNIST/real&quot;) 模型训练 主要的变化是在增加了和label相关的参数，其他部分不变。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, labels) in enumerate(loader): real = real.to(device) labels = labels.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise, labels) # G(z) critic_real = disc_critic(real, labels).reshape(-1) critic_fake = disc_critic(fake, labels).reshape(-1) gp = gradient_penalty(disc_critic, real, labels, fake, device=device) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp ) # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake, labels).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(noise, labels) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train()","link":"/2022/02/12/Generative%20Adversarial%20Networks/Condition-WGAN-GP%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Kalman Filter小项目","text":"hljs.initHighlightingOnLoad(); 本文是通过一个位置判断的项目，来测试kalman filter的简单使用，了解其在opencv中的具体实现。 实现的简单的kalman filter123456789101112class KalmanFilter: kf = cv2.KalmanFilter(4, 2) kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32) kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) def predict(self, coordX, coordY): ''' This function estimates the position of the object''' measured = np.array([[np.float32(coordX)], [np.float32(coordY)]]) self.kf.correct(measured) predicted = self.kf.predict() x, y = int(predicted[0]), int(predicted[1]) return x, y 这个kalman滤波器主要是用来预测位置的。 绘制坐标点并预测 1234567891011121314img = cv2.imread(&quot;Pysource Kalman filter/blue_background.webp&quot;)ball_positions = [(50, 100), (100, 100), (150, 100), (200, 100), (250, 100), (300, 100), (350, 100)]for pt in ball_positions: cv2.circle(img, pt, 15, (0, 20, 220), -1) # 参数 绘图对象 位置 大小 颜色BGR 粗细 -1代表实心 # 绘制预测点 predicted = kf.predict(pt[0], pt[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4)cv2.imshow(&quot;Img&quot;, img)cv2.waitKey(0) 1234567891011121314# 根据上述值向后预测10代for i in range(10): predicted = kf.predict(predicted[0], predicted[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4) print(predicted)&gt;&gt;&gt;(550, 99)(600, 98)(649, 97)(698, 96)(747, 95)(796, 94)(845, 92)(894, 90)(942, 88) 123456789101112131415ball2_positions = [(4, 300), (61, 256), (116, 214), (170, 180), (225, 148), (279, 120), (332, 97), (383, 80), (434, 66), (484, 55), (535, 49), (586, 49), (634, 50),(683, 58), (731, 69), (778, 82), (824, 101), (870, 124), (917, 148),(962, 169), (1006, 212), (1051, 249), (1093, 290)]for pt in ball2_positions: cv2.circle(img, pt, 15, (0, 20, 220), -1) # 参数 绘图对象 位置 大小 颜色BGR 粗细 -1代表实心 # 绘制预测点 predicted = kf.predict(pt[0], pt[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4)# 根据上述值向后预测10代for i in range(10): predicted = kf.predict(predicted[0], predicted[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4) print(predicted) 同理，预测第二个球的轨迹。 orange轨迹预测12# 创建橘子检测器od = OrangeDetector() 1234# 实时的检测orange的轨迹orange_bbbox = od.detect(frame)x1, y1, x2, y2 = orange_bbboxcv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4) 之后我们将矩形框转换为中心点 12345678# 视频播完就退出orange_bbbox = od.detect(frame)x1, y1, x2, y2 = orange_bbbox# cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4)# 确定orange的中心cx = int((x1 + x2) / 2)cy = int((y1 + y2) / 2)cv2.circle(frame, (cx, cy), 20, (0, 0, 255), -1) 以上是根据目标检测得到的实时orange的位置，之后我们来用kalman filter预测Orange位置。 123# 滤波器预测下一时刻的位置predicted = kf.predict(cx, cy)cv2.circle(frame, predicted, 20, (220, 20, 0), 5) 完整代码： 1234567891011121314151617181920212223242526272829303132333435import cv2from orange_detector import OrangeDetectorfrom kalmanfilter import KalmanFilter# 读取视频cap = cv2.VideoCapture(&quot;Pysource Kalman filter/orange.mp4&quot;)# cap = cv2.VideoCapture(0)# 创建橘子检测器od = OrangeDetector()# 创建kfkf = KalmanFilter()while True: ret, frame = cap.read() if ret is False: break # 视频播完就退出 orange_bbbox = od.detect(frame) x1, y1, x2, y2 = orange_bbbox # cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4) # 确定orange的中心 cx = int((x1 + x2) / 2) cy = int((y1 + y2) / 2) cv2.circle(frame, (cx, cy), 20, (0, 0, 255), -1) # 滤波器预测下一时刻的位置 predicted = kf.predict(cx, cy) cv2.circle(frame, predicted, 20, (220, 20, 0), 5) # print(orange_bbbox) cv2.imshow(&quot;Frame&quot;, frame) key = cv2.waitKey(50) if key == 27: break","link":"/2022/02/13/opencv/kalman-filter%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"title":"车流统计项目","text":"hljs.initHighlightingOnLoad(); 本项目主要是来识别某路段的车流数量，主要分为目标识别和车流数量统计两个部分，对于车流数量计算部分还是需要很强的算法基础，在写的过程中不断会出现新的问题，需要一步一步的调试。但是该项目还有一定的BUG，还有待改进。 目标识别12345678910from object_detection import ObjectDetection# 初始化目标检测od = ObjectDetection()# Detect objects on frame(class_ids, scores, boxes) = od.detect(frame)for box in boxes: (x, y, w, h) = box cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2) 车流数量识别识别两帧中的相同车辆下面要判断连续两帧中的汽车是否为同一个汽车，来避免重复计数。 这里通过连续绘制车辆运行过程的中心点来进行识别是否为同一辆车： 1234567891011for box in boxes: (x, y, w, h) = box # print(&quot;FRAME N°&quot;, count, &quot; &quot;, x, y, w, h) cx = int((x + x + w) / 2) cy = int((y + y + h) / 2) center_point.append((cx, cy)) cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1) cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)for pt in center_point: cv2.circle(frame, pt, 5, (0, 0, 255), -1) 如上面的轨迹检测将会造成我们的中心点的数据越来有多，因此当我们判断好这两量车是同一辆是，可以释放掉同一量车前面时刻的路径。 12345&gt;&gt;&gt;CUR[(567, 905), (434, 750), (1724, 626), (930, 534), (857, 567), (1019, 650), (761, 655), (612, 474), (881, 473), (753, 463), (1867, 590), (641, 436), (687, 451), (943, 459), (1347, 984), (1114, 435), (1417, 465)]PRE[(571, 891), (437, 742), (1750, 635), (761, 648), (1018, 647), (928, 533), (856, 565), (612, 473), (881, 473), (1336, 980), (753, 463), (1118, 436), (942, 458), (1877, 605), (642, 435), (688, 450), (1268, 433), (1424, 466)] 以上为两次的输出结果，由此可以看出来有几对是一一对应的。 12345678910111213141516tracking_objects = {}# 检测车辆的序号track_id = 0for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 10: tracking_objects[track_id] = pt track_id += 1 # 再图中绘制出每一个图片对应的位置for object_id, pt in tracking_objects.items(): cv2.circle(frame, pt, 5, (0, 0, 255), -1) cv2.putText(frame, str(object_id), (pt[0], pt[1] - 7), 0, 1, (0, 0, 255), 2) 但是出现了以下异常的情况，一些车辆被识别但没有被统计。 原因可能是，近处的车移动量会大一点，造成违背计算在内。将distance &lt; 10 改为 distance &lt; 20. 统计车辆的id但是每张图的id都不相同，会不断的添加新的id，于是出现以下的BUG： 其原因是，第一次pre还没有数据，cur也是刚刚才有数据，于是给新的项目赋予了track_id， 下一次的比较又会给旧的目标赋予新的track_id, 于是出现以上错误。 第二次比较时，要将tracking_object的数据和新的目标点数据数据进行比较，代码如下： 123456789101112131415if count &lt;= 2: for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[track_id] = pt track_id += 1else: for pt in center_points_cur_frame: for object_id, pt2 in tracking_objects.items(): distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt 之后，如图所示没有出现新的问题，就是运行过程中的id掉了一地，没有跟着一起移动。 123456789101112131415161718for object_id, pt2 in tracking_objects.items(): object_exist = False # 判断这个对象是否存在 for pt in center_points_cur_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt object_exist = True # 已经证实器存在之后，就不用进行下面的操作了 # 就是跳出本次for循环，这样可以提高效率 if object_exist == False: # 如果这个id遍历了一圈还没有结果，那么就直接删除 tracking_objects.pop(object_id) # 但是，循环过程中时不可以随便的改变里面的元素的 # 因此要创建副本 接下来的问题就是循环过程中时不可以随便的改变里面的元素的，因此要创建副本，具体的操作如下，参与循环的是副本，进行改变的是真实参数： 下面我们为新识别出来的车辆增加标签。 完整代码以下是完整的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import cv2import numpy as npfrom object_detection import ObjectDetectionimport math# 初始化目标检测od = ObjectDetection()cap = cv2.VideoCapture(&quot;los_angeles.mp4&quot;)# 初始化计算器count = 0# center_point = []center_points_pre_frame = []tracking_objects = {}# 检测车辆的序号track_id = 0while True: ret, frame = cap.read() count += 1 if not ret: break # 当前窗口内的点 center_points_cur_frame = [] # Detect objects on frame (class_ids, scores, boxes) = od.detect(frame) for box in boxes: (x, y, w, h) = box # print(&quot;FRAME N°&quot;, count, &quot; &quot;, x, y, w, h) cx = int((x + x + w) / 2) cy = int((y + y + h) / 2) center_points_cur_frame.append((cx, cy)) cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1) cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2) if count &lt;= 2: for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[track_id] = pt track_id += 1 else: tracking_objects = tracking_objects.copy() center_points_cur_frame_copy = center_points_cur_frame.copy() for object_id, pt2 in tracking_objects.copy().items(): object_exist = False # 判断这个对象是否存在 for pt in center_points_cur_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt object_exist = True # 已经证实器存在之后，就不用进行下面的操作了 # 就是跳出本次for循环，这样可以提高效率 if pt in center_points_cur_frame: center_points_cur_frame.remove(pt) # 如果存在，就是个旧的点，将其删去后就是新的点了 # 但是多次循环会造成一个点删除了多次 if object_exist == False: # 如果这个id遍历了一圈还没有结果，那么就直接删除 tracking_objects.pop(object_id) # 但是，再循环过程中时不可以随便的改变里面的元素的 # 因此要创建副本 # 增加新的点 for pt in center_points_cur_frame: tracking_objects[track_id] = pt track_id += 1 # 再图中绘制出每一个图片对应的位置 for object_id, pt in tracking_objects.items(): cv2.circle(frame, pt, 5, (0, 0, 255), -1) cv2.putText(frame, str(object_id), (pt[0], pt[1] - 7), 0, 1, (0, 0, 255), 2) print(&quot;CUR&quot;) print(center_points_cur_frame) print(&quot;PRE&quot;) print(center_points_pre_frame) # 保留上一帧中的数据 center_points_pre_frame = center_points_cur_frame.copy() cv2.imshow('Frame', frame) key = cv2.waitKey(1) if key == 27: breakcap.release()cv2.destroyAllWindows()","link":"/2022/02/13/opencv/%E8%BD%A6%E6%B5%81%E7%BB%9F%E8%AE%A1%E9%A1%B9%E7%9B%AE/"},{"title":"Object_detection_app项目","text":"hljs.initHighlightingOnLoad(); 本项目主要是利用yolov4目标检测网络，实现对固定目标的识别功能，最后将其打包形成.exe文件。但是，最后编译的过程还有一定的bug,还未能实现封装。 初始化相机12345678910cap = cv2.VideoCapture(0)# 修改相机参数 整理修改的是相机的frame的H,Wcap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)while True: # 如果不加while循环的话，生成的程序会一闪而过 ret, frame = cap.read() cv2.imshow(&quot;Frame&quot;, frame) cv2.waitKey(1) # 单位是ms # 等待时间设为0的话，视像头会会静止，关闭一次窗口才刷新一次 导入目标检测网络1234567# opencv DNNnet = cv2.dnn.readNet(&quot;object_detection_crash_course/dnn_model/yolov4-tiny.weights&quot;, &quot;object_detection_crash_course/dnn_model/yolov4-tiny.cfg&quot;)model = cv2.dnn_DetectionModel(net)# 导入模型model.setInputParams(size=(320, 320), scale=1/255)# size越大，清晰度越大，但是可以速度比较慢# scale的功能是缩放 [0,255]-&gt;[0,1] 导入类别文件123456# 导入类别文件classes=[]with open(&quot;object_detection_crash_course/dnn_model/classes.txt&quot;,&quot;r&quot;) as file_object: for class_name in file_object: class_name = class_name.strip() classes.append(class_name) 目标检测这部分的代码放到相加的whie循环中： 1234567891011121314151617# 目标检测class_idx, scores, bboxs = model.detect(frame)# 返回值依次是类别的idx， 所得的分数， boundingboxs边界框# print(f&quot;class_idx:{class_idx}\\n scores:{scores} \\n bboxs:{bboxs}&quot;)for class_id, score, bbox in zip(class_idx, scores, bboxs): (x, y, w, h) = bbox class_name = classes[class_id] # class_name = classes[class_id[0]] # 4.4.0的版本 # 绘制目标检测三角形, 在窗口对象上绘制 cv2.rectangle(frame, (x, y), (x+w, y+h), (200, 0, 50), 3) # 参数为绘制的对象, 绘制长方形的左上点合右下点, 颜色RGB, 框的粗细 # 识别的种类合coco数据集有关 cv2.putText(frame, str(class_name), (x, y-5), cv2.FONT_HERSHEY_PLAIN, fontScale=2, color=(200, 0, 50), thickness=2) 设计鼠标点击1234567891011121314151617181920button_person = Falsedef click_button(event, x, y, flags, params): global button_person if event == cv2.EVENT_LBUTTONDOWN: # print(x, y) polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) is_inside = cv2.pointPolygonTest(polygan, (x,y), False) if is_inside: print(&quot;inside the button&quot;) if button_person is False: button_person = True else: button_person = False # 相当于一个开关，点击以下开始识别，再点一下就关闭识别# 产生一个新的窗口cv2.namedWindow(&quot;Frame&quot;)cv2.setMouseCallback(&quot;Frame&quot;, click_button) 之后就可以来设计窗口的打开的开关： 绘制类别字体1234567# 创建按钮# 这里使用多边行来绘制polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]])cv2.fillPoly(frame, polygan, (0, 0, 200))# cv2.rectangle(frame, (20, 20), (220, 70), (0, 0, 200), -1)# 厚度为-1代表布满cv2.putText(frame, &quot;Person&quot;, (30, 60), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 3) 由于绘制字体类别和设计鼠标点击的逻辑的内容有些复杂，因此我们直接导入写好的文件。 设置按钮这里定义了一个Button类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import cv2import numpy as npclass Buttons: def __init__(self): # Font self.font = cv2.FONT_HERSHEY_PLAIN self.text_scale = 3 self.text_thick = 3 self.x_margin = 20 self.y_margin = 10 # Buttons self.buttons = {} self.button_index = 0 self.buttons_area = [] np.random.seed(0) self.colors = [] self.generate_random_colors() def generate_random_colors(self): for i in range(91): random_c = np.random.randint(256, size=3) self.colors.append((int(random_c[0]), int(random_c[1]), int(random_c[2]))) def add_button(self, text, x, y): # Get text size textsize = cv2.getTextSize(text, self.font, self.text_scale, self.text_thick)[0] right_x = x + (self.x_margin * 2) + textsize[0] bottom_y = y + (self.y_margin * 2) + textsize[1] self.buttons[self.button_index] = {&quot;text&quot;: text, &quot;position&quot;: [x, y, right_x, bottom_y], &quot;active&quot;: False} self.button_index += 1 def display_buttons(self, frame): for b_index, button_value in self.buttons.items(): button_text = button_value[&quot;text&quot;] (x, y, right_x, bottom_y) = button_value[&quot;position&quot;] active = button_value[&quot;active&quot;] if active: button_color = (0, 0, 200) text_color = (255, 255, 255) thickness = -1 else: button_color = (0, 0, 200) text_color = (0, 0, 200) thickness = 3 # Get text size cv2.rectangle(frame, (x, y), (right_x, bottom_y), button_color, thickness) cv2.putText(frame, button_text, (x + self.x_margin, bottom_y - self.y_margin), self.font, self.text_scale, text_color, self.text_thick) return frame def button_click(self, mouse_x, mouse_y): for b_index, button_value in self.buttons.items(): (x, y, right_x, bottom_y) = button_value[&quot;position&quot;] active = button_value[&quot;active&quot;] area = [(x, y), (right_x, y), (right_x, bottom_y), (x, bottom_y)] inside = cv2.pointPolygonTest(np.array(area, np.int32), (int(mouse_x), int(mouse_y)), False) if inside &gt; 0: print(&quot;IS Ac&quot;, active) new_status = False if active is True else True self.buttons[b_index][&quot;active&quot;] = new_status def active_buttons_list(self): active_list = [] for b_index, button_value in self.buttons.items(): active = button_value[&quot;active&quot;] text = button_value[&quot;text&quot;] if active: active_list.append(str(text).lower()) return active_list 之后开始实例化Button: 显示button： 设计点击激活操作： 完整的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import cv2import numpy as npfrom object_detection_crash_course.gui_buttons import Buttons# 初始化Buttonbutton = Buttons()button.add_button(&quot;person&quot;, 20, 20)button.add_button(&quot;cell phone&quot;, 20, 100)button.add_button(&quot;keyboard&quot;, 20, 180)button.add_button(&quot;remote&quot;, 20, 260)button.add_button(&quot;scissors&quot;, 20, 340)colors = button.colors# opencv DNNnet = cv2.dnn.readNet(&quot;object_detection_crash_course/dnn_model/yolov4-tiny.weights&quot;, &quot;object_detection_crash_course/dnn_model/yolov4-tiny.cfg&quot;)model = cv2.dnn_DetectionModel(net)# 导入模型model.setInputParams(size=(416, 416), scale=1/255)# size越大，清晰度越大，但是可以速度比较慢# scale的功能是缩放 [0,255]-&gt;[0,1]# 导入类别文件classes=[]with open(&quot;object_detection_crash_course/dnn_model/classes.txt&quot;,&quot;r&quot;) as file_object: for class_name in file_object: class_name = class_name.strip() classes.append(class_name)# 初始化相机cap = cv2.VideoCapture(0)# 修改相机参数 整理修改的是相机的frame的H,Wcap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)# FULL HD = 1920x1080# button_person = Falsedef click_button(event, x, y, flags, params): # global button_person if event == cv2.EVENT_LBUTTONDOWN: button.button_click(x, y) # print(x, y) # polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) # # is_inside = cv2.pointPolygonTest(polygan, (x,y), False) # if is_inside: # print(&quot;inside the button&quot;) # if button_person is False: # button_person = True # else: # button_person = False # # 相当于一个开关，点击以下开始识别，再点一下就关闭识别# 产生一个新的窗口cv2.namedWindow(&quot;Frame&quot;)cv2.setMouseCallback(&quot;Frame&quot;, click_button)while True: # 如果不加while循环的话，生成的程序会一闪而过 ret, frame = cap.read() # 导出激活的button active_button = button.active_buttons_list() # 目标检测 class_idx, scores, bboxs = model.detect(frame) # 返回值依次是类别的idx， 所得的分数， boundingboxs边界框 # print(f&quot;class_idx:{class_idx}\\n scores:{scores} \\n bboxs:{bboxs}&quot;) for class_id, score, bbox in zip(class_idx, scores, bboxs): (x, y, w, h) = bbox class_name = classes[class_id] # class_name = classes[class_id[0]] # 4.4.0的版本 color = colors[class_id] # 根据id分配颜色 # if button_person and class_name == &quot;person&quot;: if class_name in active_button: # 绘制目标检测三角形, 在窗口对象上绘制 cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3) # 参数为绘制的对象, 绘制长方形的左上点合右下点, 颜色RGB, 框的粗细 # 识别的种类合coco数据集有关 cv2.putText(frame, str(class_name), (x, y-5), cv2.FONT_HERSHEY_PLAIN, fontScale=2, color=color, thickness=2) # # 创建按钮 # # 这里使用多边行来绘制 # polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) # cv2.fillPoly(frame, polygan, (0, 0, 200)) # # cv2.rectangle(frame, (20, 20), (220, 70), (0, 0, 200), -1) # # 厚度为-1代表布满 # cv2.putText(frame, &quot;Person&quot;, (30, 60), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 3) # 显示Button button.display_buttons(frame) cv2.imshow(&quot;Frame&quot;, frame) cv2.waitKey(1) # 单位是ms # 等待时间设为0的话，视像头会会静止，关闭一次窗口才刷新一次 项目部署12345678import sysfrom cx_Freeze import setup, Executablesetup(name=&quot;Simple Object Detection Software&quot;, version=&quot;0.1&quot;, description=&quot;This software detects objects in realtime&quot;, executables=[Executable(&quot;main.py&quot;)] ) 结果出现了报错应该是cx_Freeze的问题，目前没解决。","link":"/2022/02/13/opencv/object-detection-app%E9%A1%B9%E7%9B%AE/"},{"title":"Pytorch中常用的高级操作与搭建完整流程","text":"hljs.initHighlightingOnLoad(); 本文主要从模型的保存和修改、如何导入和加载现有的模型、如何使用GPU以及完整的训练和测试过程进行介绍，主要的视频教程 。 本教程以官方文档为主，这里关注最多的是图像，因此可以使用跟图像处理相关的包，打开torchvision的文档。 现有网络的使用和修改打开torchvision文档，选择model可以得出我们的一些常用的模型，如图所示。 主要的模型涉及了图像分类、目标检测、语义识别和视频处理。 这里以分类模型的VGG为例，数据集使用的时CFIAR10 如果pretained=true则使用的时预训练好的参数，为False则直接重新初始化参数来训练。 progress(bool)代表是否显示预训练模型下载的进度条。 1234567import torchvisiontrain_data = torchvision.datasets.ImageNet(&quot;./data_image_net&quot;, split='train', download=True)# ../ 表示上级目录下导入 ./ 表示当前的目录下导入&gt;&gt;&gt;RuntimeError: The dataset is no longer publicly accessible. You need to download the archives externally and place them in the root directory. 目前这个数据集不适合公开的被下载，要自己手动下载，导入到对于的目录下，但是TM 147G也太大了，所以入门还是找一些小一点的数据集吧。 现有模型的使用下载的时候默认会下载到我们C盘中C:\\Users\\dell/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth 123456789101112131415161718192021# 导入网络的模型vgg16_pretrain = vgg16(pretrained=True)# 有预训练权重的,会下载一会，528vgg16_normal = vgg16(pretrained=False)# 没有权重，仅模型本身print(vgg16_pretrain)&gt;&gt;&gt;VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ...... (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) ...... (6): Linear(in_features=4096, out_features=1000, bias=True) )) 现有模型的修改模型层数的增加add_module() 1234567891011121314151617181920212223# 模型层的增加print(vgg16_normal.classifier[6])# 直接在模型最后增加vgg16_normal.add_module('linear1', nn.Linear(1000, 10))# 加入到最后一个模型块中vgg16_normal.classifier.add_module('linear0', nn.Linear(1000, 10))print(vgg16_normal)&gt;&gt;&gt; ( ......(classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) (linear0): Linear(in_features=1000, out_features=10, bias=True) ) (linear1): Linear(in_features=1000, out_features=10, bias=True)) 模型的修改 12345# -----------------------------------------------# 模型的修改vgg16_normal.classifier[7] = nn.Linear(1000, 256)vgg16_normal.linear0 = nn.Linear(256, 10)print(vgg16_normal) 注意： 在 Sequential里的模型，索引都是按照0、1、2、… 的索引来选取的，因此金国最后一层的名字为linear0，但是要求改时还是用数值7来索引vgg16_normal.classifier[7] 其次如果vgg16_normal.linear0不存在时，就会在模型的最后重新建立一个vgg16_normal.linear0的层。 至于Sequential()的一些操作，后期可以参考李沐的课程。 模型的保存和模型的加载这里将展示两种模型的保存和加载的方式，一种将网络的参数和结构全部保存下来，另一种只是保存模型的参数（我感觉模型的参数占用的内存应该达到90%以上，只保存参数感觉不怎么节省空间其实，不知道为什么官方推荐。） 保存和加载方式112345678910111213141516171819202122# 保存方式1，模型结构+模型参数vgg16_1 = vgg16(pretrained=False)torch.save(vgg16_1, &quot;vgg16_method1.pth&quot;)# 对应方式1的加载model = torch.load(&quot;vgg16_method1.pth&quot;)print(model)&gt;&gt;&gt;VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ...... (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) ...... (6): Linear(in_features=4096, out_features=1000, bias=True) )) 因为模型和权重的数据都保存了，打印的时候只是打印模型的结构，而不打印权重数据。 注意： 对于方式1，如果模型是自己定义，那么torch.load()时，如果模型定义的文件不是同一个文件，那么会报错。 解决的办法就是将我们我们自定义的模型和torch.load()放在用一个文件(这样有点蠢)，或者就是将定义的模型文件from model import *进来即可。 保存和加载方式2123456789# 保存方式2，模型参数(官方推荐)vgg16_2 = vgg16(pretrained=False)torch.save(vgg16_2.state_dict(), &quot;vgg16_method2.pth&quot;)# 对应方式1的加载model = torch.load(&quot;vgg16-397923af.pth&quot;)print(model)model = torch.load(&quot;vgg16_method2.pth&quot;)print(model) 官方带有预训练参数的模型和这种方法一样，都是采用的是保存参数而不是保存模型结构的方式。 直接torch.load()的话结果，打印出来都是参数字典的形式。 12345678# 对应方式2的加载# 先初始化模型vgg16_3 = vgg16(pretrained=False)# 获取参数存储dictdict_para = torch.load(&quot;vgg16-397923af.pth&quot;)# 将字典参数导入模型vgg16_3.load_state_dict(dict_para)print(vgg16_3) 完整的训练套路导入数据集1234567891011121314151617181920212223from torchvision.transforms import transformsfrom torchvision.datasets import CIFAR10# 准备数据集train_data = CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=False)test_data = CIFAR10(root=&quot;./root&quot;, train=False, transform=transforms.ToTensor(), download=False)# length长度train_data_size = len(train_data)test_data_size = len(test_data)# 打印信息print(&quot;训练数据集长度为：{}&quot;.format(train_data_size))print(&quot;测试数据集长度为：{}&quot;.format(test_data_size))# 利用DataLoader来加载数据库集train_loader = DataLoader(train_data, batch_size=64)test_loader = DataLoader(test_data, batch_size=64) 搭建网络1234567891011121314151617181920212223242526272829from torch import nnimport torch# 搭建神经网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(64*4*4, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)# 验证正确性if __name__ == '__main__': net = Net() inputs = torch.ones((64, 3, 32, 32)) outputs = net(inputs) print(outputs.shape) 在train.py中导入网络，创建网络模型： 123from model import Net# 创建网络模型net = Net() 优化器和损失函数123456# 损失函数loss_fn = nn.CrossEntropyLoss()# 优化器learning_rate = 1e-2optimizer = optim.SGD(net.parameters(), learning_rate) 基本的训练过程12345678910111213141516171819202122232425262728293031# 设置训练网络和一些参数# 记录训练的次数total_train_step = 0# 记录测试次数total_test_step = 0# 训练的轮数epoch = 10# 添加tensorboardwriter = SummaryWriter(&quot;train_logs&quot;)for i in range(epoch): print(&quot;----------第{}轮训练开始----------&quot;.format(i+1)) # 训练步骤开始 for data in train_loader: imgs, targets = data outputs = net(imgs) loss = loss_fn(outputs, targets) # 优化器优化模型 optimizer.zero_grad() loss.backward() optimizer.step() total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(&quot;训练次数：{}，Loss:{}&quot;.format(total_train_step, loss.item())) writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step) # loss -&gt; tensor([xxx]) # loss.item() -&gt; xxx 直接返回真实的数据 这里的loss.item()就是将tensor类型变量里具体的数字输出出来。 以上的代码是对模型进行训练，那么如何评价训练的好坏情况?——那就是直接在测试集合进行测试。 12345678910111213141516# 测试步骤开始total_test_loss = 0.0with torch.no_grad(): for data in test_loader: imgs, targets = data outputs = net(imgs) loss = loss_fn(outputs, targets) total_test_loss = total_test_loss + loss.item() print(&quot;整体测试集上的Loss:{}&quot;.format(total_test_loss)) writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step) total_test_loss = total_test_step + 1 # 模型的保存 这里是将每轮的模型都进行保存 torch.save(net, &quot;net_{}.pth&quot;.format(i)) print(&quot;模型已经保存&quot;) 模型的优化过程对于目标检测和语义分割来说，可以直接使用测试误差来作为训练过程的评价指标。但是分类问题还有其他的方法，比如我们的outputs=[0.1, 0.2, 0.3, 0.4]其实是该图片属于各种情况的概率，而真实的输出可以是outputs=[3] 即为最后的判别类别的编号。 12345678910111213total_accuracy = 0for xxx ... ... accuracy = (outputs.argmax(1) == targets).sum() # 也可以使用max函数 pred = torch.max(outputs, dim=1)[1] accuracy0 = (pred == targets).sum() total_accuracy = total_accuracy + accuracy # total_accuracy = total_accuracy + accuracy0 print(&quot;整体测试数据的正确率为:{}&quot;.format(total_accuracy/len(test_loader))) 通过上述方法来实现输出准确率。 模型搭建的细节12net.train()net.eval() 由于我们的net是Net的实例化的对象，同时Net是继承nn.Module的一个子类。要去nn.Containers-&gt;nn.Module帮助文档里查阅。 其实，这两个层就是在有dropout和BatchNorm是才主要起作用。 使用GPU训练模型的方法方法一只要把对于的网络模型、数据(输入imgs、标注labels)和损失函数进行.cuda()，然后覆盖原来的即可。 12345678if torch.cuda.is_available(): # 修改模型 net = net.cuda() # 修改数据 imgs = imgs.cuda() targets = targets.cuda() # 修改损失函数 loss_fn = loss_fn.cuda() 土堆的提醒： colab的GPU免费版每周可以使用免费30小时。 对于colab,如果想要运行在terminal上的语言，需要加在命令前加！。 up测试结果，colab的速度是一般他自己电脑的4倍。运行一轮只要0.7s，而up的电脑要2.5s 但是上述方法不是很常用。 方法二这是比较常用的方法。 1234567891011121314151617# Device是torch.device类型的数据Device = torch.device(&quot;cpu&quot;)Device0 = torch.device(&quot;cuda&quot;)# 可以选择不同的显卡Device1 = torch.device(&quot;cuda:0&quot;)Device2 = torch.device(&quot;cuda:1&quot;)device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# 导入到网络中net = net.to(Device)# 修改数据imgs = imgs.to(Device)targets = targets.to(Device)# 修改损失函数loss_fn = loss_fn.to(Device)# 对于未来和损失函数来说直接to(device)即可 其他与方法一相同，也是对网络、数据以及损失函数进行设置。 以下为我在字节电脑上训练的速度结果。 123456789101112131415161718-------------CPU------------------------第2轮训练开始----------消耗的时间为:0.21944713592529297训练次数：800，Loss:1.8308976888656616消耗的时间为:1.417208194732666训练次数：900，Loss:1.782684564590454消耗的时间为:2.627971887588501训练次数：1000，Loss:1.8661954402923584消耗的时间为:3.8746509552001953训练次数：1100，Loss:1.9508962631225586消耗的时间为:5.333733797073364训练次数：1200，Loss:1.6595017910003662消耗的时间为:6.635249376296997训练次数：1300，Loss:1.6205933094024658消耗的时间为:7.973670244216919训练次数：1400，Loss:1.7197237014770508消耗的时间为:9.35597276687622训练次数：1500，Loss:1.8009873628616333 以下是使用cpu训练的效果： 123456789101112131415161718192021-------------GPU------------------------第2轮训练开始----------消耗的时间为:1.0422132015228271训练次数：800，Loss:1.8765838146209717消耗的时间为:6.844691276550293训练次数：900，Loss:1.856195330619812消耗的时间为:12.637196063995361训练次数：1000，Loss:1.9804853200912476消耗的时间为:18.599247694015503训练次数：1100，Loss:1.9813154935836792消耗的时间为:24.597208261489868训练次数：1200，Loss:1.7226444482803345消耗的时间为:30.361780405044556训练次数：1300，Loss:1.6570385694503784消耗的时间为:36.181212425231934训练次数：1400，Loss:1.7347835302352905消耗的时间为:41.99465990066528训练次数：1500，Loss:1.8110555410385132整体测试集上的Loss:304.21112048625946整体测试数据的正确率为:19.764331817626953模型已经保存 以下是colab的cpu与GPU的效果： 完整的验证/测试(demo)步骤主要的核心就是使用已经训练的好的模型。 主要提醒： Resize()的特点：输入的图格式和输出的格式是一一对应的。 12345678910111213141516171819202122232425262728293031323334353637import torchfrom PIL import Imagefrom torchvision.transforms import transformsfrom model import Net# 导入图片image_path = &quot;&quot;image = Image.open(image_path)print(image)# 数据增强处理transform = transforms.Compose([ transforms.Resize((32, 32)), transforms.ToTensor(),])image = transform(image)image = torch.reshape(image, (1, 3, 32, 32))# 导入模型# 注意使用第一种模型保存时要将模型导入# from model import Net 即可torch.load(&quot;net_0.pth&quot;)# 第二种导入方法net = Net()net.load_state_dict(torch.load(&quot;net_0.pth&quot;))# 养成练好得到习惯net.eval()with torch.no_grad(): # 节约内存和使用性能 outputs = net(image) pred_label = outputs.argmax(1) # 输出预测的类别print(pred_label) 比较标准的步骤请见教程。 注意： 如果是使用GPU训练的模型，但是测试时的设备如果为CPU，此时需要修改torch.load(&quot;xxx.pth&quot;,map_location=torch.device(&quot;cpu&quot;)) 实际项目中的参数问题 可以将require的参数改成defalut的，先让代码跑起来。","link":"/2022/02/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%90%AD%E5%BB%BA%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/"},{"title":"Pytorch基本网络的搭建过程","text":"hljs.initHighlightingOnLoad(); 本文主要从网络的搭建、损失函数和优化器三个方面进行介绍，并且以下教程主要还是出自于官方的文档，因此还是有时间多看看官方文档，主要的视频教程 。 神经网络基本骨架 本节主要讲的是nn.Containers内的工具。 nn.Module这是一个基类，所有的神经网络的搭建都需要去集成这个类，将整个类的__init__()和forward()进行重写即可。这里我们以官网上的一个教程为例。 123456789101112import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 通过调试可以发现，进入类中第一步就会进入到我们的forward()函数中，进行运行，再通过return返回到类的外部。 卷积操作Conv2d()torch提供了两个一个是torch.nn一个是torch.nn.functional前者可以看作是后者的一个封装，调研起来会更方便，但是后者实现的更加基础，相当于前者的一些具体的实现。 这里是nn.Conv2d的帮助文档，相比之下比functional.conv2d的教程要详细很多。 123456789101112131415161718192021222324252627282930313233343536373839404142import torchimport torch.nn.functional as Finput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])# 养成良好的习惯，将矩阵写成以上的格式kernel = torch.tensor([[1, 2, 1], [0, 1, 0], [2, 1, 0]])input = torch.reshape(input, (1, 1, 5, 5))kernel = torch.reshape(kernel, (1, 1, 3, 3))print(input.shape)print(kernel.shape)output = F.conv2d(input=input, weight=kernel, stride=1)print(output)output2 = F.conv2d(input=input, weight=kernel, stride=2)print(output2)output3 = F.conv2d(input=input, weight=kernel, stride=1, padding=1)print(output3)&gt;&gt;&gt;torch.Size([1, 1, 5, 5])torch.Size([1, 1, 3, 3])tensor([[[[10, 12, 12], [18, 16, 16], [13, 9, 3]]]])tensor([[[[10, 12], [13, 3]]]])tensor([[[[ 1, 3, 4, 10, 8], [ 5, 10, 12, 12, 6], [ 7, 18, 16, 16, 8], [11, 13, 9, 3, 4], [14, 13, 9, 7, 4]]]]) 以下主要来讲解的是nn.Conv2d()的操作。 Parameters参数（这里面最常用的是上面的是前五个） in_channels (int) – Number of channels in the input image 输入的通道数 out_channels (int) – Number of channels produced by the convolution 输出的通道数 kernel_size (int or tuple) – Size of the convolving kernel 可以自定义卷积核的大小，可以不是方形的结构。 stride (int or tuple, optional) – Stride of the convolution. 指的是卷积的步长 Default: 1 padding (int, tuple or str, optional) – Padding added to all four sides of the input. 是否再边缘进行填充Default: 0 padding_mode (string**, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1 groups (int, optional) – Number of blocked connections from input channels to output channels. 分组卷积，在ResNeXt 网络中多见到。 Default: 1 bias (bool, optional) – If True, adds a learnable bias to the output. Default: True 详细的理解可以参考Link kernel_size 是我们实际不断去学习的参数。 以下为输入和输出的形状。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import torchimport torchvisionimport torch.nn as nnfrom torch.nn import Conv2dfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformstrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)# 设置网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0) def forward(self, x): x = self.conv1(x) return xnet = Net()print(net)# 打印网络的相关的参数step = 0writer = SummaryWriter(&quot;conv&quot;)for data in dataloader: img, target = data output = net(img) # print(img.shape) # print(output.shape) # torch.Size([64, 3, 32, 32]) writer.add_images(&quot;input&quot;, img, step) # torch.Size([64, 6, 30, 30]) # tensorboard 是没有办法显示6 channel的图片的 output = torch.reshape(output, (-1, 3, 30, 30)) writer.add_images(&quot;output&quot;, output, step) step += 1 如图为卷积处理前后图片的对比情况。 池化层 Parameters kernel_size – the size of the window to take a max over stride – the stride of the window. Default value is kernel_size，默认是和kernel_size一样的。 padding – implicit zero padding to be added on both sides dilation – a parameter that controls the stride of elements in the window 空洞卷积。 return_indices – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later ceil_mode – when True, will use ceil instead of floor to compute the output shape 取整方式，上取证还是下取整。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torchvisionimport torch.nn as nnfrom torch.nn import MaxPool2dfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformsinput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]], dtype=torch.float32)# 养成良好的习惯，将矩阵写成以上的格式# 设置网络class Net(nn.Module): def __init__(self, ceil_mode): super(Net, self).__init__() self.ceil_mode = ceil_mode self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=self.ceil_mode) def forward(self, x): output = self.maxpool1(x) return outputnet1 = Net(ceil_mode=True)net2 = Net(ceil_mode=False)print(net1)print(net2)# 打印网络的相关的参数input = torch.reshape(input, (-1, 1, 5, 5))output1 = net1(input)output2 = net2(input)print(&quot;ceil_mode=true\\n&quot;, output1)print(&quot;ceil_mode=False\\n&quot;, output2)&gt;&gt;&gt;ceil_mode=true tensor([[[[2., 3.], [5., 1.]]]])ceil_mode=False tensor([[[[2.]]]]) 最大池化是保留主要的图片特征，会变模糊，eg.1080P-&gt;720P 12345678910111213141516171819train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)step = 0writer = SummaryWriter(&quot;maxpooling&quot;)for data in dataloader: img, target = data output = net1(img) writer.add_images(&quot;input&quot;, img, step) writer.add_images(&quot;output&quot;, output, step) step += 1 这里padding层的作用跟padding是意义的，可以用0或者任意的一个常数去进行，用的比较少，此处不在详细介绍。 非线性激活函数比较常用的nn.ReLU(inplace=Flase)，nn.Sigmoid()。 inplace的作用其实就是是否在原来的位置进行替换。 默认inplace=False , 这样可以保留原始的数据不丢失。 以下我们使用Sigmoid激活函数为例，来观察处理前后的情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import torchimport torchvisionimport torch.nn as nnfrom torch.nn import ReLUfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformsinput = torch.tensor([[1, -0.5], [-1, 3]])input = torch.reshape(input, (-1, 1, 2, 2))class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.relu = ReLU() def forward(self, input): output = self.relu(input) return outputnet = Net()print(net)output = net(input)print(output)# 图片操作情况# ----------------------------------------------------train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)class Net2(nn.Module): def __init__(self): super(Net2, self).__init__() self.sigmoid = nn.Sigmoid() def forward(self, input): output = self.sigmoid(input) return outputnet2 = Net2()print(net2)step = 0writer = SummaryWriter(&quot;Sigmoid&quot;)for data in dataloader: img, target = data output = net2(img) writer.add_images(&quot;input&quot;, img, step) writer.add_images(&quot;output&quot;, output, step) step += 1 线性层nn.Linear() 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchimport torchvisionimport torch.nn as nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformstrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)# 设置网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.linear1 = nn.Linear(in_features=196608, out_features=10) def forward(self, x): output = self.linear1(x) return outputnet = Net()step = 0writer = SummaryWriter(&quot;maxpooling&quot;)for data in dataloader: img, target = data print(img.shape) # output = torch.reshape(img, (1, 1, 1, -1)) output = torch.flatten(img) print(output.shape) output = net(output) print(output.shape) 使用torch.flatten()得到的结果为 123torch.Size([64, 3, 32, 32])torch.Size([196608])torch.Size([10]) 就是最后直接展平成为一维。 nn.Sequential()实战这里提供一个实际的例子，来展示nn.Sequential()的使用过程。 1234567891011121314151617181920212223242526272829303132333435import torchimport torch.nn as nnfrom torch.utils.tensorboard import SummaryWriterclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), # 默认式和kernel_size相等的 nn.Linear(1024, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)net = Net()print(net)input = torch.ones((64, 3, 32, 32))print(input.shape)output = net(input)writer = SummaryWriter(&quot;Sequential&quot;)writer.add_graph(net, input)writer.close() 如图所示，为使用tensorboard后的基本结果，其可以显示基本的网络连接和维度上的变化。 损失函数 损失函数根据定义来讲一定是越小越好。 Loss function 衡量的是实际计算和目标之间的差距。 为我们更新参数提供依据，即反向传播。 L1Loss以下以L1Loss来举例： 要重点注意的时输入与输出的Shape: Input: (), where means any number of dimensions. Target: (*), same shape as the input. Output: scalar. If reduction is 'none', then (*), same shape as the input. 12345678910111213141516import torchimport torch.nn as nnfrom torch.nn import L1Loss# 损失函数一般在torch.nn里inputs = torch.tensor([1, 2, 3], dtype=torch.float32)targets = torch.tensor([1, 2, 5], dtype=torch.float32)inputs = torch.reshape(inputs, (1, 1, 1, 3))targets = torch.reshape(targets, (1, 1, 1, 3))loss = L1Loss(reduction='mean')# reduction='mean' 时我们的inputs和targets要都是float类型才对result = loss(inputs, targets)print(result) MSELoss之后，我们以最小二乘法常用的MSELoss为例： 其主要的使用方法和上面的基本相同，这里不在赘述。 CrossEntropyLoss然后，我们以分类任务中比较常用的交叉熵损失函数为例： 以上公式和视频里的公式不同，因为这里将交叉熵softmax进行了内置处理。 Shape: Input: (N, C) where C = number of classes, or$ (N, C, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. 这里输入需要连两个参数，第二个参数为分类数(就是其所示类别的编号数) ； 如上面的例子，狗的分类数为1，C=[0, 1, 0] 这里要输入的必须为one-hot编码 Target:（N） If containing class indices, shape (N)where each value is $ 0 \\leq \\text{targets}[i] \\leq C-10$, or $(N, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. If containing class probabilities, same shape as the input. Output: If reduction is 'none', shape (N) or $(N, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. Otherwise, scalar. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torch.nn as nnfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoaderfrom torchvision.transforms import transformsimport torchvisiontrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=1, num_workers=0, drop_last=False, shuffle=True)class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), # 默认式和kernel_size相等的 nn.Linear(1024, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)net = Net()loss = nn.CrossEntropyLoss()for i, data in enumerate(dataloader): imgs, targets = data outputs = net(imgs) # print(outputs) # print(targets) result = loss(outputs, targets) print(result) result.backward() if i == 10: break 提供反向传播使其拥有grad参数。 优化器优化器的构件过程12optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr=0.0001) 优化器的使用过程123456for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() 这里有很多优化器，不同的优化器所使用的参数是不同的，但是共有的参数是para=和 lr= (即要优化的参数和学习率)","link":"/2022/02/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E5%9F%BA%E6%9C%AC%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/"},{"title":"Pix2pix GAN 论文及模型搭建","text":"hljs.initHighlightingOnLoad(); 论文：Image-to-Image Translation with Conditional Adversarial Networks论文链接：https://arxiv.org/abs/1611.07004代码链接：https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix 本文主要介绍的pix2pix，并且自己定义损失函数，生成器主要使用的是U-Net(主要做语义分割的网络) 。 原论文简介教程1 对于CGAN，是将条件信息进行如label进行embedding之后，一起加入到网络中进行识别生成的。整体的条件信息还可以直接是图片。 判别器要做两件事情： 生成的图片的真假问题，如果是从我们的数据集中得到的，就为真；如果是生成器生成的，就为假。 其次还要判断生成的图片和真实的条件信息是否对应。 这里的我们也可以不加噪声，直接输出条件信息(图片)。 这里是在编码的过程中加入了随机噪声，加强了我模型的稳定性。 基于auto-encoder的想法，我们定义了U-Net网络。 U-Net将encoder的过程加到了decoder的上面，实现了跳跃连接，这样的可训练性更好。 pix2pix最后要经过一个patchGAN，使得最后得输出为一个矩阵来判断真假。再与真实的网络进行一个L1loss函数，实现条件信息和真实信息的吻合。 教程2文章总述： pix2pix本质上是一个CGAN，其输入里有一张图片 (相比传统的CGAN的输入为一个label+embedding) ，而不是一般的噪声量。 这个网络里面没有损失函数，因为判别器网络其实学习到的就是一个损失函数。 由于一般的基于欧式距离的损失函数会造成平均值效应，使得我们生成的图片比较的模糊。于是我们用判别器来学习出一种损失函数来到达生成图片的高清的效果。 目标函数： CGAN的目标函数为： 这里判别函数的输入x——原图片，y——目标图片。 将该损失函数和没有非CGAN进行对比，结果为：CGAN的效果更好。 为了生成器的精度，要加入正则化项，由于L2正则化会造成平均值损失，于是我们使用L1正则化项。 L1 loss function 可以去除模糊，提高清晰度： 最后的损失函数为： 网络结构生成器结构 总体上，还是使用的是卷积网络，Convolution-BatchNorm-ReLU/LeakyReLU的形式。 生成器的网络的使用的是U-Net 这里可以看作是及是将我们Encoder通过跳跃连接实现encoder和decoder的堆叠。 判别器结构 这里的判别器使用的是PatchGAN。 如图patchGAN输入是一个NXN的块，输出是整个块的true or fake，最后取所以的patch块的输出的平均值即可。 这种方法的优点是：参数少，运行速度快；并且使用的图片可以是任意大的图片。 优化和推断 使用标准的优化方法，一次判别器，一次生成器。 对于生成器来说，我们不使(用一般的$\\min \\{\\log(1-D(G(x,z)))\\}$作为损失函数，我们使用$\\max \\{\\log(D(G(x,z))\\}$ 作为损失函数。 这里的优化器使用的是Adam优化器，参数$\\beta_1=0.5, \\beta_2=0.999$，学习率$\\alpha=0.0002=2e-4$. 评估指标这里使用了we run “real vs. fake” perceptual studies on Amazon Mechanical Turk(AMT) 给1秒的时间来看图片，来判断图片的真假。 选择patchGAN的尺寸这里我们选择1x1-&gt;pixelGAN，NxN(整张图片)-&gt;ImageGAN，介于之间的是patchGAN，论文中比较了不同的持尺寸对生成结果的影响。 16x16 有较高的FCN-scores 并且 可以生成 artifacts。 70x70的结果也很不错，也达到了上述的要求。 附录生成器的网络结构： 这里的Ck-&gt;Convolution-BatchNorm-ReLU（channels=k）， CDK-&gt;Convolution-BatchNorm-Dropout-ReLU（channels=k）。 这里的卷积核都是4x4，stride=2. 以下为U-Net的网络结构：(这里是没有跳跃连接的) 有跳跃连接的U-Net解码器： 生成器的最后使用的是Tanh激活函数。 判别器的网络结构 使用的是LeakyReLU(0.2)，最后一层使用的是Sigmoid函数。 第一个卷积上没有使用BatchNorm。 Keras实现简介 Pytorch实现模型搭建由于模型的工作量较大，因此我们这里采用的将模型，训练过程以及相关参数的配置分开写。 判别器的搭建1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import torchimport torch.nn as nnclass CNNBlock(nn.Module): def __init__(self, in_channels, out_channels, stride=2): super(CNNBlock, self).__init__() self.conv = nn.Sequential( nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding_mode=&quot;reflect&quot;), # padding_mode # 参考https://blog.csdn.net/weixin_42211626/article/details/122542323?utm_medium=distribute. # pc_aggpage_search_result.none-task-blog-2~aggregatepage~first_rank_ecpm_v1~rank_v31_ecp # m-4-122542323.pc_agg_new_rank&amp;utm_term=padding_mode&amp;spm=1000.2123.3001.4430 nn.BatchNorm2d(out_channels), # 由于BatchNorm2d的影响，造成我们的bias要设成False nn.LeakyReLU(0.2), ) def forward(self, x): return self.conv(x)# x, y &lt;- 将输入和输出图片堆叠在一起class Discriminator(nn.Module): def __init__(self, in_channels=3, features=[64, 128, 256, 512]): # 256-&gt; 26x26 286-&gt;30x30 最后输出的形状是和输入由有关的 super(Discriminator, self).__init__() self.initial = nn.Sequential( nn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=&quot;reflect&quot;), nn.LeakyReLU(0.2) ) layer = [] in_channels = features[0] for feature in features[1:]: layer.append( CNNBlock(in_channels, feature, # stride在最后一层为1，其他为2 stride=1 if feature == features[-1] else 2), ) in_channels = feature layer.append( nn.Conv2d( in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=&quot;reflect&quot; ), ) self.model = nn.Sequential(*layer) def forward(self, x, y): x = torch.cat([x, y], dim=1) x = self.initial(x) return self.model(x) 生成器的搭建123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import torchimport torch.nn as nnclass Block(nn.Module): def __init__(self, in_channels, out_channels, down=True, act=&quot;relu&quot;, use_dropout=False): super(Block, self).__init__() self.conv = nn.Sequential( # 分情况，看是上取样还是下取样 nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=&quot;reflect&quot;) if down else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU() if act==&quot;relu&quot; else nn.LeakyReLU(0.2), ) self.use_dropout = use_dropout self.dropout = nn.Dropout(0.5) def forward(self, x): x = self.conv(x) # dropout 主要在网络的上三层来使用 return self.dropout(x) if self.use_dropout else xclass Generator(nn.Module): def __init__(self, in_channels=3, features=64): super(Generator, self).__init__() # 256 self.initial_down = nn.Sequential( nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=&quot;reflect&quot;), nn.LeakyReLU(0.2), ) # 128 x 128 self.down1 = Block(features, features * 2, down=True, act=&quot;leaky&quot;, use_dropout=False) # 64 x 64 self.down2 = Block( features * 2, features * 4, down=True, act=&quot;leaky&quot;, use_dropout=False ) # 32 x 32 self.down3 = Block( features * 4, features * 8, down=True, act=&quot;leaky&quot;, use_dropout=False ) # 16 x 16 self.down4 = Block( features * 8, features * 8, down=True, act=&quot;leaky&quot;, use_dropout=False ) # 8 x 8 self.down5 = Block( features * 8, features * 8, down=True, act=&quot;leaky&quot;, use_dropout=False ) # 4 x 4 self.down6 = Block( features * 8, features * 8, down=True, act=&quot;leaky&quot;, use_dropout=False ) # 2 x 2 self.bottleneck = nn.Sequential( nn.Conv2d(features*8, features*8, 4, 2, 1, padding_mode=&quot;reflect&quot;), nn.ReLU(), ) # 1 x 1 self.up1 = Block( features * 8, features * 8, down=False, act=&quot;relu&quot;, use_dropout=True ) self.up2 = Block( features * 8 * 2, features * 8, down=False, act=&quot;relu&quot;, use_dropout=True ) self.up3 = Block( features * 8 * 2, features * 8, down=False, act=&quot;relu&quot;, use_dropout=True ) self.up4 = Block( features * 8 * 2, features * 8, down=False, act=&quot;relu&quot;, use_dropout=False ) self.up5 = Block( features * 8 * 2, features * 4, down=False, act=&quot;relu&quot;, use_dropout=False ) self.up6 = Block( features * 4 * 2, features * 2, down=False, act=&quot;relu&quot;, use_dropout=False ) self.up7 = Block(features * 2 * 2, features, down=False, act=&quot;relu&quot;, use_dropout=False) self.final_up = nn.Sequential( nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1), nn.Tanh(), # [-1, 1] ) def forward(self, x): d1 = self.initial_down(x) d2 = self.down1(d1) d3 = self.down2(d2) d4 = self.down3(d3) d5 = self.down4(d4) d6 = self.down5(d5) d7 = self.down6(d6) bottleneck = self.bottleneck(d7) up1 = self.up1(bottleneck) up2 = self.up2(torch.cat([up1, d7], 1)) up3 = self.up3(torch.cat([up2, d6], 1)) up4 = self.up4(torch.cat([up3, d5], 1)) up5 = self.up5(torch.cat([up4, d4], 1)) up6 = self.up6(torch.cat([up5, d3], 1)) up7 = self.up7(torch.cat([up6, d2], 1)) return self.final_up(torch.cat([up7, d1], 1))def test(): x = torch.randn((1, 3, 256, 256)) model = Generator(in_channels=3, features=64) print(model) preds = model(x) print(preds.shape)if __name__ == &quot;__main__&quot;: test() print(torch.__version__) 总结 Pix2pix是用作图像翻译的CGAN Pix2pix用比较深的网络，进行精密的调参产生了不错的结果 Pix2pix需要配对的标签图像和实际图像 pix2pix获得了比较良好的图像翻译的效果,但训练pix2pix需要一番工夫。","link":"/2022/02/24/Generative%20Adversarial%20Networks/pix2pix-GAN-%E8%AE%BA%E6%96%87%E5%8F%8A%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/"},{"title":"第二章单自由度系统自由振动-第二部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍有阻尼自由振动和等效阻尼振动，包括库伦力但摩擦系统自由振动的精确分析。 阻尼自由振动理论知识 实际系统的机械能不可能守恒，存在各种各样的阻力 振动中将阻力称为阻尼：摩擦阻尼，电磁阻尼，介质阻尼和结构阻尼 尽管已经提出了许多数学上描述阻尼的方法，但是实际系统中阻尼的物理本质仍然极难确定 最常用的一种阻尼力学模型是粘性阻尼 例如：在流体中低速运动或沿润滑表面滑动的物体，通常就认为受到粘性阻尼 对于高速运动的物体，其受到的是高速阻尼力的影响，比如足球在空中高速的飞过其受到的阻尼力跟速度的二次方成正相关。 由于振动的存在，因此阻尼造成的延迟是广泛存在的。 粘性阻尼力与相对速度称正比，即： P_d = cv 其中，c:为粘性阻尼系数，或阻尼系数单位：$ N.s /m $ 建立平衡位置，并受力分析： 动力学方程： m\\ddot x(t) + c\\dot x(t) + kx(t) = 0 或者写成： \\ddot x(t) + 2\\zeta \\omega_0 \\dot x(t) + \\omega_0^2x(t) = 0\\\\ \\frac c m = 2 \\zeta\\omega_0 这里我们的固有频率：$\\omega_0 = \\sqrt {\\dfrac k m}$ ; 相对阻尼系数为 $\\zeta = \\dfrac c {2\\sqrt {km}}$ 使用特征根法来求解得： 这里只有欠阻尼才会引起振动，一般情况下，我们结构阻尼都在0.05以下。 过阻尼和临界阻尼会造成系统逐渐停止。 情况一：欠阻尼 $\\zeta &lt; 1$ 特征根：$\\lambda_{1,2} = -\\zeta\\omega_0 \\pm i\\omega_d$ 此时有两个复数根； 此时得振动方程解为：$ x(t)=e^{-\\zeta\\omega_0t}(c_1\\cos\\omega_dt + c_2\\sin\\omega_dt )$ 其中 $c_1,c_2:$初始条件确定； 有阻尼的自由振动频率 ： $\\omega_d = \\omega_0\\sqrt{1-\\zeta^2}$，从中得出，有阻尼情况下的振动周期更长，频率更慢。 此时的周期为$T_d = \\dfrac{2\\pi}{\\omega_0\\sqrt{1-\\zeta^2}} = \\dfrac{T_0}{\\sqrt{1-\\zeta^2}}$ 是初始条件：$x(0) = x_0 \\qquad \\dot x(0)=x_0$ 此时的振动界为：$x(t)=e^{-\\zeta\\omega_0t}(x_0\\cos\\omega_dt + \\dfrac {\\dot x_0 + \\zeta\\omega_0x_0} {\\omega_d}\\sin\\omega_dt ) $ 或简写成：$x(t)=e^{-\\zeta\\omega_0t}Asin(\\omega_d t + \\theta) $ 其中$A = \\sqrt{x_0^2+\\left (\\dfrac {\\dot x_0 + \\zeta\\omega_0x_0} {\\omega_d}\\right )^2}$ , $\\theta = tg^{-1} \\dfrac{x_0\\omega_d}{\\dot x_0 + \\zeta\\omega_0x_0}$ 如图为振动的图形： 欠阻尼为振幅逐渐衰减，周期不变的振动。 对比阻尼下的振动可知：在高频的振动下可以通过阻尼材料进行控制，但是对于低频材料来说阻尼材料的作用不大； 阻尼材料属于被动控制的一种，主要是振动的能量 减幅系数 $\\eta$ 评价阻尼对振幅衰减快慢的影响，定义为相邻两个振幅的比值： \\eta = \\dfrac {\\Delta_i} {\\Delta_{i+1}} = \\dfrac {Ae^{-\\zeta\\omega_0t}} {Ae^{-\\zeta\\omega_0{(t+T_d)}}}=e^{-\\zeta\\omega_0T_d} $\\eta$ 和 t 无关，任意两个相邻振幅之比均为$\\eta$ 衰减振动的频率为$\\omega_d$，振幅衰减的快慢取决于$\\zeta\\omega_0$，这两个重要的特征反映在特征方程的特征根的实部和虚部. 但是由于其含有指数项，不利于工程上的应用，因此实际中采用对数衰减率，$\\Lambda=\\ln \\eta = \\zeta\\omega_0T_d$ ,可以使用仪器测得。 实验测量： 利用相隔j 个周期的两个峰值进行求解： \\dfrac {\\Delta_i} {\\Delta_{i+j}} = \\dfrac {\\Delta_i} {\\Delta_{i+1}}\\dfrac {\\Delta_{i+1}} {\\Delta_{i+2}}\\dfrac {\\Delta_{i+2}} {\\Delta_{i+3}}\\dots\\dfrac {\\Delta_{i+j-1}} {\\Delta_{i+j}} 得：$\\Lambda=\\dfrac1 j\\ln \\eta = \\zeta\\omega_0 \\dfrac{2\\pi}{\\omega_0\\sqrt{1-\\zeta^2}} = \\dfrac{2\\pi\\zeta}{\\sqrt{1-\\zeta^2}} \\approx 2\\pi\\zeta$ 其中$\\zeta = \\dfrac{\\Lambda}{2\\pi}$ 情况二：过阻尼 $\\zeta &gt; 1$ 特征根：$\\lambda_{1,2} = -\\zeta\\omega_0 \\pm i\\omega_0\\sqrt{\\zeta^2-1}$ 此时有两个负实根； $\\omega^* = \\omega_0\\sqrt{\\zeta^2-1}$ 此时得振动方程解为：$ x(t)=e^{-\\zeta\\omega_0t}(c_1ch\\omega^*t + c_2sh\\omega^*t )$ 其中 $c_1,c_2:$初始条件确定； 是初始条件：$x(0) = x_0 \\qquad \\dot x(0)=x_0$ 此时的振动界为：$x(t)=e^{-\\zeta\\omega_0t}(x_0ch\\omega^*t + \\dfrac {\\dot x_0 + \\zeta\\omega_0x_0} {\\omega_d}sh\\omega^*t ) $ 响应的图形为： 一种按指数规律衰减的非周期蠕动，没有振动发生 情况三：临界阻尼 $\\zeta = 1$ 特征根：$\\lambda_{1,2} = -\\omega_0$ 此时有两个重根； 此时得振动方程解为：$ x(t)=e^{\\omega_0t}(c_1 + c_2t )$ 其中 $c_1,c_2:$初始条件确定； 是初始条件：$x(0) = x_0 \\qquad \\dot x(0)=x_0$ 此时的振动界为：$x(t)=e^{-\\zeta\\omega_0t}(x_0+ ({\\dot x_0 + \\omega_0x_0})t)$ 这里也是呈现指数衰减的，只是衰减的速度没有过阻尼快； 临界阻尼系数$c_{cr}$ ： $\\zeta = \\dfrac c {2\\sqrt {km}}$ 但是要注意的是，这里的c是等效阻尼，可能有系数，要整体来乘，最后得出$c_{cr} = 2\\sqrt{km}$ 欠阻尼是一种振幅逐渐衰减的振动 过阻尼是一种按指数规律衰减的非周期蠕动，没有振动发生 临界也是按指数规律衰减的非周期运动，但比过阻尼衰减快些 应用案例 由题知：$x(0) = x_0 \\qquad \\dot x(0)=x_0$ 此时的振动界为：$x(t)=e^{-\\zeta\\omega_0t}(x_0\\cos\\omega_dt + \\dfrac {\\dot x_0 + \\zeta\\omega_0x_0} {\\omega_d}\\sin\\omega_dt ) $ 求导可得：$\\dot{x}(t)=-\\frac{\\omega_{0}^{2} x_{0}}{\\omega_{d}} e^{-\\zeta \\omega_{0} t} \\sin \\omega_{d} t$ 设在时刻t1 质量越过平衡位置到达最大位移，此时速度： $\\dot{x}(t)=-\\frac{\\omega_{0}^{2} x_{0}}{\\omega_{d}} e^{-\\zeta \\omega_{0} t} \\sin \\omega_{d} t=0 \\Rightarrow t_1 = \\dfrac{\\pi}{\\omega_d}$ 即经过半个周期后出现第一个振幅x1 $x_{1}=x\\left(t_{1}\\right)=-x_{0} e^{-\\zeta \\omega_{0} t_{1}}=-x_{0} e^{-\\frac{\\pi \\zeta}{\\sqrt{1-\\zeta^{2}}}}$ 由题可知：$\\left|\\dfrac{x_{1}}{x_{0}}\\right|=e^{-\\frac{\\pi \\zeta}{\\sqrt{1-\\zeta^{2}}}}=10 \\%$ 解得：$\\zeta=0.59$ 这里建立广义坐标 $\\theta$，这里我们假设初始位置已经达到了静平衡位置了，因此不用分析重力和弹性力(因为已经抵消了)。 阻尼力的方向和速度的方向相反，弹性力和位移的方向相反。 由力矩平衡可得： m \\ddot{\\theta}(t) l \\cdot l+c \\dot{\\theta}(t) a \\cdot a+k \\theta(t) b \\cdot b=0\\\\ m l^{2} \\ddot{\\theta}(t)+c a^{2} \\dot{\\theta}(t)+k b^{2} \\theta(t)=0 则此时根据公式可求出无阻尼时的固有频率为：$\\omega_{0}=\\sqrt{\\dfrac{k b^{2}}{m l^{2}}}=\\dfrac{b}{l} \\sqrt{\\dfrac{k}{m}}$ $\\dfrac{c a^{2}}{m l^{2}}=2 \\xi \\omega_{0} \\Rightarrow \\xi=\\dfrac{c a^{2}}{2 m l^{2} \\omega_{0}}=\\dfrac{c a^{2}}{2 m l b} \\sqrt{\\dfrac{m}{k}}$ 于是，阻尼振动的固有频率为 $\\omega_{d}=\\omega_{0} \\sqrt{1-\\xi^{2}}=\\dfrac{1}{2 m l^{2}} \\sqrt{4 k m b^{2} l^{2}-c^{2} a^{4}}$ 进而得到临界阻尼系数为： $\\xi=1 \\quad \\Rightarrow \\quad c_{c r}=\\dfrac{2 b l}{a^{2}} \\sqrt{m k}$ 等效粘性阻尼 阻尼在所有振动系统中是客观存在的； 大多数阻尼是非粘性阻尼，其性质各不相同 非粘性阻尼的数学描述比较复杂，处理方法之一： 采用能量方法将非粘性阻尼简化为等效粘性阻尼 等效原则：等效粘性阻尼在一个周期内消耗的能量等于要简化的非粘性阻尼在同一周期内消耗的能量。 通常假设在简谐激振力作用$F=F_0\\sin\\omega t$\\ 下，非粘性阻尼系统的稳态响应仍然为简谐振动。 该假设只有在非粘性阻尼比较小时才是合理的，否则会造成大量的相位上的延迟。 粘性阻尼在一个周期内消耗的能量 $\\Delta E$ 可近似地利用无阻尼振动规律计算出： \\Delta E=-\\oint c \\dot{x} d x=-\\int_{0}^{T} c \\dot{x} d t=-c \\omega_{0}^{2} A^{2} \\int_{0}^{T} \\cos ^{2}\\left(\\omega_{0} t+\\theta\\right) d t=-\\pi c A^{2}目的是为了采用该式计算等效粘性阻尼系数 讨论以下几种非粘性阻尼情况： 干摩擦阻尼、平方阻尼(高速运动的物体)、结构阻尼 干摩擦阻尼 干摩擦阻尼又称库伦阻尼 其摩擦力：$F_{d}=-\\mu F_{N} \\operatorname{sgn} \\dot{x}(t)$ ， $\\operatorname{sgn} \\dot{x}(t)=\\left\\{\\begin{array}{ll}1, &amp; \\dot{x}(t)&gt;0 \\\\ 0, &amp; \\dot{x}(t)=0 \\\\ -1, &amp; \\dot{x}(t)&lt;0\\end{array}\\right.$ 其中$\\mu:$摩擦系数 $F_N:$正压力 $sgn\\dot x$：为符号函数 平方阻尼 工程背景：低粘度流体中以较大速度运动的物体时，阻尼力与相对速度的平方成正比，方向相反。 阻尼力：$F_{d}=-c_{d} \\dot{x}^{2}(t) \\operatorname{sgn} \\dot{x}(t)$ $c_d$：阻力系数 在运动方向不变的半个周期内计算耗散能量，再乘2： $\\Delta E=-\\oint c_{d} \\dot{x}^{2}(t) \\operatorname{sgn} \\dot{x}(t) d x=-2 \\int_{-T / 4}^{T / 4} c_{d} \\dot{x}^{3}(t) d t=-\\dfrac{8}{3} c_{d} \\omega_{0}^{2} A^{2}$ 等效粘性阻尼系数：$c_{e}=-\\dfrac{8}{3 \\pi} c_{d} \\omega_{0} A$ 结构阻尼 由于材料为非完全弹性，在变形过程中材料的内摩擦所引起的阻尼称为结构阻尼。 特征：应力－应变曲线存在滞回曲线加载和卸载沿不同曲线 内摩擦所耗散的能量等于滞回环所围的面积：$\\Delta E=-vA^2$ 等效粘性阻尼系数：$c_e=-\\dfrac {v} {\\pi\\omega_0}$ 库伦摩擦单自由度系统自由振动的精确分析有库伦力阻尼的一般振动系统在许多机械系统中，为了简单与方便经常采用库伦摩擦 $F=\\mu N$, 其中$\\mu$ : 动力学系数，N：法向力 摩擦自由振动问题： 动力学方程：$m\\ddot x+ kx = -\\mu N$ 是个二阶齐次的微分方程 通解：$x(t)=A_{1} \\cos \\omega_{0} t+A_{2} \\sin \\omega_{0} t-\\dfrac{\\mu N}{k}$ A1和A2为常数，由这半个周期的初始条件确定，$\\omega_0=\\sqrt{k/m}$ 为振动的固有频率。 动力学方程：$m\\ddot x+ kx = -\\mu N$ 是个二阶齐次的微分方程 通解：$x(t)=A_{1} \\cos \\omega_{0} t+A_{2} \\sin \\omega_{0} t-\\dfrac{\\mu N}{k}$ 动力学方程：$m\\ddot x+ kx = \\mu N$ 是个二阶齐次的微分方程 通解：$x(t)=A_{1} \\cos \\omega_{0} t+A_{2} \\sin \\omega_{0} t+\\dfrac{\\mu N}{k}$ A3和A4为常数，由这半个周期的初始条件确定 $\\mu$N / k 可以看做常力以静载荷方式作用在质量块上时弹簧产生的虚位移。 这两式同时表明，在每一个半周期中运动都是简谐的，只是对应的平衡位置从$\\mu$N / k变为$-\\mu$N / k。 合并可得：$m\\ddot x+ \\mu N sgn(\\dot x)kx = 0$ 是非线性方程，无解析解，可以分段求解。 由于开始的情况下，质量块是被拉到了最左端，然后释放，则此时的是从右向左运动的，满足情况2； 假定初始条件：$x(0)=x_0, \\dot x(0)=0$ $A_3=x_0-\\dfrac {\\mu N}{k} A_4=0$ $x(t)=\\left(x_{0}-\\dfrac{\\mu N}{k}\\right) \\cos \\omega_{0} t+\\dfrac{\\mu N}{k}$ , 在半周期内成立$t \\in [0, \\dfrac \\pi {\\omega_0}]$ 当$t=\\dfrac \\pi {\\omega_0}$时，质量快达到最左端位置： $x_{1}=x\\left(t=\\dfrac{\\pi}{\\omega_{0}}\\right)=\\left(x_{0}-\\dfrac{\\mu N}{k}\\right) \\cos \\omega_{0} t+\\dfrac{\\mu N}{k}=-\\left(x_{0}-\\dfrac{2 \\mu N}{k}\\right)$ 第二个半周期内，前一个半周期终止时刻的运动情况是这半个周期的初始条件： $x(0)=-(x_0-\\dfrac{2\\mu N}{k}), \\dot x(0)=0$ $\\Rightarrow$ $A_1=-x_0+\\dfrac {3\\mu N}{k} A_2=0$ $x(t)=\\left(x_{0}-\\dfrac{3\\mu N}{k}\\right) \\cos \\omega_{0} t-\\dfrac{\\mu N}{k}$ , 在半周期内成立$t \\in [\\dfrac \\pi {\\omega_0}, \\dfrac {2\\pi} {\\omega_0}]$ 在这半个周期的最后时刻的位移和速度：$x_2=x_0-\\dfrac{4\\mu N}{k}, \\dot x=0$ 在运动终止前，发生的半个周期的个数r 为：$x_{0}-r \\dfrac{2 \\mu N}{k} \\leq \\dfrac{\\mu N}{k} \\Rightarrow r \\geq \\dfrac{x_{0}-\\dfrac{\\mu N}{k}}{\\dfrac{2 \\mu N}{k}}$ 有库伦力阻尼的扭转振动系统 第2个半周期结束时振幅：$\\theta_{r}=\\theta_{0}-r \\dfrac{2 T}{k_{\\theta}}$ ,其中$\\theta_0$为t=0时的角位移(t=0时的$\\dot \\theta_0=0$) 当$r \\geq\\left(\\theta_{0}-\\dfrac{T}{k_{\\theta}}\\right) /\\left(\\dfrac{2 T}{k_{\\theta}}\\right)$时，会停止运动。 固有频率：$\\omega_{0}=\\sqrt{\\dfrac{k}{m}}=\\dfrac{2 \\pi}{T}=\\dfrac{2 \\pi}{0.4}(\\mathrm{rad} / s)=15.708(\\mathrm{rad} / s)$ 在一个周期内振幅减少量为：$\\dfrac{4 \\mu N}{k}=\\dfrac{4 \\mu m g}{k}$ 由初始条件得：$5\\times\\dfrac{4 \\mu m g}{k} = 0.10-0.01(m)=0.09(m)$ 摩擦系数：$\\mu=\\dfrac{0.09 k}{20 m g}=\\dfrac{0.09 \\omega_{0}^{2}}{20 g}=\\dfrac{0.09 \\times 15.708^{2}}{20 \\times 9.81}=0.1132$ $r \\geq \\dfrac{0.10472-\\dfrac{400}{49087.5}}{\\dfrac{800}{49087.5}}=5.926$ 因此其停止要经过的周期数为6个半周期的时间，其初始的位移 $\\theta_0=0.10472 rad$ 第r根半周期结束后的振幅为：$\\theta_{r}=\\theta_{0}-r \\dfrac{2 T}{k_{\\theta}}$ 经过6个半周期后滑轮的角位移为：$\\theta=0.10472-6 \\times 2 \\times \\dfrac{400}{49087.5}(\\mathrm{rad})=0.006935(\\mathrm{rad})=0.39724^{\\circ}$ 滑轮停止的位置与平衡位置的夹角0.397240°，且与初始角位移在平衡位置的同侧。","link":"/2022/03/05/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%87%AA%E7%94%B1%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"},{"title":"第二章单自由度系统自由振动_第一部分","text":"单自由度系统振动的第一部分，主要讲的是无阻尼自由振动、能量法、瑞利法以及等效刚度与质量。 无阻尼的自由振动令$x$为位移，以质量块静平衡位置为原点，$\\lambda$为静变形。受到初始扰动时，由牛顿第二定律，得： m\\ddot x(t) = mg - k(\\lambda+x(t))在静平衡时的位置：$mg = k\\lambda$ 则物体的振动或自由振动的微分方程为： m\\ddot x(t)+kx(t)=0令：$\\omega_0=\\sqrt \\frac{k}{m}$ 固有频率，单位：弧度/秒（rad/s) 则有微分方程为：$\\ddot x(t) + \\omega_0^2x(t)=0 \\Rightarrow$ 对应得特征方程为：$ms^2+k=0 \\Rightarrow$ 特征根：$s_{1,2}=\\pm i\\omega_0$ $s_1、s_2$都满足特征方程，因此通解可以写为 \\begin{align} x(t) &= A_1e^{s_1t}+A_2 e^{s_2t}\\\\ &=A_1e^{i\\omega t}+A_2 e^{-i\\omega t}\\\\ &= c_1 \\cos (\\omega_0t) +c_2 \\sin (\\omega_0t)\\\\ &= A \\sin (\\omega_0t+\\varphi) \\end{align}$c_1和c_2$为新常数，主要由初始条件来决定 振幅：$A=\\sqrt {(c_1^2+c_2^2)}$ 初相位：$\\varphi=tg^{-1} \\left (\\dfrac {c1}{c2} \\right )$ $\\omega_0$ ：系统固有的数值特征，与系统是否振动以及如何振动无关。 $A,\\varphi$：非系统固有数字特征，与系统所受激励和初始状态有关。 考虑系统在初始扰动下的自由振动，设$t=\\tau$的初始位移和初始速度为：$x(\\tau)=x_\\tau \\qquad \\dot x(\\tau)=\\dot x_\\tau$ 则令： c_1=b_1 \\cos(\\omega_0 \\tau)-b_2\\sin (\\omega_0\\tau)\\\\ c_2=b_1 \\sin(\\omega_0 \\tau)-b_2\\cos (\\omega_0\\tau)带入得：$b_1=x_\\tau \\quad b_2=\\dfrac{\\dot x_\\tau}{\\omega_0}$ 并且$\\tau$时刻后得自由振动的解为： x(t) = x_\\tau \\cos \\omega_0(t-\\tau) +\\frac{\\dot x_\\tau}{\\omega_0} \\sin \\omega_0(t-\\tau)\\\\带入零时刻的初始条件$x(0)=x_0 \\qquad \\dot x(0)=\\dot x_0$可得： x(t) = x_0 \\cos \\omega_0(t) +\\frac{\\dot x_0}{\\omega_0} \\sin \\omega_0(t)=A\\sin(\\omega_0t+\\varphi)此时的振幅为$A=\\sqrt{x_0^2+\\left(\\frac{\\dot x_0}{\\omega_0}\\right)^2}$，相位为$\\varphi=tg^{-1} \\dfrac{x_0\\omega_0}{\\dot x_0}$ 无阻尼的质量弹簧系统受到初始扰动后，其自由振动是以$\\omega_0$为振动顿率的简谐振动,并且永无休止。 初始条件的说明：初始条件是外界能量转入的一种方式， 有初始位移即转入了弹性势能。 有初始速度即转入了动能，用锤子敲击的脉冲就是输入了动能。 如图所示：可以得出， 弹簧的刚度为蓝色最大、黑色最小。 刚度越大、圆频率$\\omega_0=\\sqrt{\\dfrac{k}{m}}$ 越大，周期（$T=\\dfrac{2\\pi}{\\omega_0}$）越小。 由此得出圆频率的另一种计算方法： 由于在静止情况下$mg=k\\lambda$ 可得：$\\omega_0=\\sqrt{\\dfrac{k}{m}}=\\sqrt{\\dfrac{g}{\\lambda}}$ 对于不易得到m和k 的系统，若能测出静变形$\\lambda$，则用该式计算较为方便。 无阻尼振动的计算实例 首先，我们规定的原点一般为静平衡位置，这样可以时建模得到了微分方程为齐次的，在原长处建模时，微分方程为非齐次的。 m\\ddot x+kx=0\\\\ 如果以原长的位置进行建模：\\\\ m\\ddot x+k x=mg 由题可得振动的频率为：$\\omega_0=\\sqrt{\\dfrac{gk}{W}}=19.6\\,rad/s$ 重物匀速下降时处于静平衡位置，若将坐标原点取在绳被卡住瞬时重物所在位置，则t=0时有，$x_0=0\\quad \\dot x_0=v$ 于是带入振动方程： \\begin{align} x(t) &= x_0 \\cos \\omega_0(t) +\\frac{\\dot x_0}{\\omega_0} \\sin \\omega_0(t)\\\\ &=\\dfrac{v}{\\omega_0} \\sin(\\omega_0t)=12.8\\sin(19.6t) (cm) \\end{align} 求绳子的最大张力，一定时是绳子在最底部时的。绳中最大张力等于静张力与因为振动引起动的张力之和： T_{max}=T_s+kA=W+kA\\\\ =1.47\\times10^5+0.74\\times10^5=2.21\\times10^5 动张力几乎是静张力的一半 由于$kA=k\\dfrac{v}{\\omega_0}=v\\sqrt{km}$ 为了减少振动引起的动张力，应当降低升降系统的刚度。尽管这样可能会造成A过大舒适性下降，但是由于加速度减小，安全性得到保证。 首先，取平衡位置，以梁承受重物静平衡位置为坐标原点（就像相当于将重物平稳的放在上面产生的静变形为$\\lambda$） 由材料力学得：$\\lambda=\\dfrac{mgl^3}{48EJ}$ 自由振动频率：$\\omega_0=\\sqrt{\\dfrac{g}{\\lambda}}=\\sqrt {\\dfrac{48EJ}{ml^3}}$ 撞击时刻为零时刻，则t=0 时，有：$x_0=-\\lambda\\quad\\dot x_0=\\sqrt{2gh}$ 自由振动振幅：$A=\\sqrt{x_0^2+\\left(\\frac{\\dot x_0}{\\omega_0}\\right)^2}=\\sqrt{\\lambda^2+2h\\lambda}$ 梁的最大扰度：$\\lambda_{max}=A+\\lambda$ 由牛顿第二定律： I\\ddot \\theta(t)+k_\\theta \\theta(t)=0\\\\ \\ddot \\theta(t)+\\omega_0^2\\theta(t)=0 则扭振的固有频率为$\\omega_0=\\sqrt{\\dfrac{k_\\theta}{I}}$ 总结：由上例可看出，除坐标不同外，角振动与直线振动的数学描述完全相同。如果在弹簧质量系统中将m、k 称为广义质量及广义刚度，则弹簧质量系统的有关结论完全适用于角振动。以后不加特别声明时，弹簧质量系统是广义的。 从上两例还可看出，单自由度无阻尼系统总包含着惯性元件和弹性元件两种基本元件。 惯性元件是感受加速度的元件，它表现为系统的质量或转动惯量； 弹性元件是产生使系统恢复原来状态的恢复力的元件，它表现为具有刚度或扭转刚度的弹性体。 同一个系统中，若惯性增加，则使固有频率降低，而若刚度增加，则固有频率增大。 求复摆在平衡位置附近做微振动时的微分方程和固有频率。这里我们考虑的振动都是微振动（$\\sin \\theta \\approx \\theta$ )。 首先，由牛顿定律：$I_0\\ddot \\theta(t)+mga\\sin \\theta(t)=0$ 得出固有频率$\\omega_0=\\sqrt{\\dfrac{mga}{I_0}}$ 若已测出物体的固有频率，则可求出$\\omega_0$，再由移轴定理，可得物质绕质心的转动惯量：$I_c=I_0-ma^2$ 实验确定复杂形状物体的转动惯量的一个方法。 以静平衡位置为坐标原点建立坐标系，振动固有频率：$\\omega_0=\\sqrt{\\dfrac{K}{m}}=70rad/s$ 振动初始条件：$kx_0=mg\\times\\sin30° \\ \\Rightarrow x_0=-0.1cm$ 由题初始速度为$\\dot x_0=0$，则运动方程为$x(t)=-0.1\\cos(70t) cm$ 如果系统竖直放置，振动频率是否改变？不改变，因为质量是不变的，只有原长会改变。 能量法 对于不计阻尼即认为没有能量损失的单自由度系统，可利用能量守恒原理建立自由振动微分方程，或直接求出固有频率。 无阻尼系统为保守系统，其机械能守恒，即动能T 和势能V 之和保持不变，即：$T+V=const$ \\dfrac{d}{dt} (T+V)=0 当原点在静平衡位置的情况： 则总的势能为： V=-mgx+\\int_0^xk(\\lambda+x)\\,dx\\\\=-mgx+k\\lambda x+\\frac 1 2 k x^2 =\\frac 1 2 kx^2 则总的能量的为：$T+V=\\dfrac 1 2 k x^2+\\dfrac 1 2m \\dot x^2$ $\\dfrac{d}{dt} (T+V)=(m\\ddot x +kx)\\dot x=0 \\,\\Rightarrow \\, m\\ddot x(t)+kx(t)=0$ 能量法的步骤：(1)列方程，(2)求固有频率 如果将坐标原点不是取在系统的静平衡位置，而是取在弹簧为自由长时的位置，此时由能量之和的导数为零可得： $m\\ddot x \\dot x-mg\\dot x +kx \\dot x=0 \\, \\Rightarrow \\, m\\ddot x(t)+kx(t)=mg$ 设新的坐标$y=x-\\dfrac {mg} k =x- \\lambda \\ \\Rightarrow \\ m\\ddot y(t)+ky(t)=0$ 如果重力的影响仅是改变了惯性元件的静平衡位置，那么将坐标原点取在静平衡位置上，方程中就不会出现重力项。 由于$T+V=const$，则有$T_{max}= V_{max}\\Rightarrow\\frac 1 2m \\dot x_{max}^2=\\frac 1 2 k x_{max}^2$ 即有$\\dot x_{max}=\\omega_0x_{max}$，带入$x(t)= A \\sin (\\omega_0t+\\varphi) $，可得$\\omega_0=\\sqrt{\\dfrac{k}{m}}$ 对于转动：$\\dot \\theta_{max}=\\omega_0\\theta_{max}$ (1) 倒摆作微幅振动时的固有频率(2) 摆球$m=0.9kg$时，测得频率f为1.5HZ，$m=1.8kg$时，测得频率f为0.75HZ，问摆球质量为多少千克时恰使系统处于不稳定平衡状态？ 解法一：静平衡位置如左图，广义坐标为$\\theta$ 势能：由于是微振动，正弦遵循小角近似，但是余弦不遵循 \\begin{align} V&=2 \\times \\frac{1}{2}\\left(\\frac{1}{2} k\\right)(\\theta a)^{2}-m g l(1-\\cos \\theta)\\\\ &=\\frac{1}{2} k a^{2} \\theta^{2}-m g l\\left(1-\\left(1-2 \\sin ^{2} \\frac{\\theta}{2}\\right)\\right)\\\\ &=\\frac{1}{2}\\left(k a^{2} \\theta^{2}-m g l \\theta^{2}\\right)=\\frac{1}{2}\\left(k a^{2}-m g l\\right) \\theta^{2} \\end{align} 动能：$T=\\dfrac{1}{2} I \\dot{\\theta}^{2}=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2}$ 微分方程为：$T_{max}=U_{max} \\Rightarrow \\frac{1}{2} m l^{2} \\dot{\\theta}_{\\max }^{2}=\\dfrac{1}{2}\\left(k a^{2}-m g l\\right) \\theta_{\\max }^{2}$ 求固有频率：$\\dot \\theta_{max}=\\omega_0\\theta_{max} \\Rightarrow \\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 这里我们将$ m l^{2}$视为等效质量，$\\left(k a^{2}-m g l\\right) $视为等效刚度。 解法二：静平衡位置如右图，广义坐标为$\\theta$ 势能：由于是微振动，这里用小角近似 \\begin{align} V&=2 \\times \\frac{1}{2}\\left(\\frac{1}{2} k\\right)(\\theta a)^{2}+m g l\\cos \\theta\\\\ &=\\frac{1}{2} k a^{2} \\theta^{2}+m g l\\left(1-2 \\sin ^{2} \\frac{\\theta}{2}\\right)\\\\ &=\\frac{1}{2}\\left(k a^{2} -m g l \\right)\\theta^{2}+mgl \\end{align} 动能：$T=\\dfrac{1}{2} I \\dot{\\theta}^{2}=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2}$ 由于$\\frac{d}{d t}(T+U)=0 \\Rightarrow\\ 2 m l^{2} \\dot{\\theta} \\ddot{\\theta}+2 \\theta\\left(k a^{2}-m g l\\right) \\dot{\\theta}=0$ 则有：$2 m l^{2} \\ddot{\\theta}+2\\left(k a^{2}-m g l\\right) \\theta=0 \\Rightarrow \\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 确定系统微振动的固有频率。 广义坐标：圆柱微转角$\\theta$，圆柱做一般运动， 由柯希尼定理，动能：$T=\\frac{1}{2}\\left(\\frac{3}{2} m R^{2}\\right) \\dot{\\theta}^{2}$ C点为瞬心 A点速度：$v_{A}=(R+a) \\dot{\\theta} \\ \\Rightarrow \\ x_{A}=(R+a) \\theta$ B点的速度：$v_{B}=(R-b) \\dot{\\theta}\\ \\Rightarrow \\ x_{B}=(R-b) \\theta$ 则势能为：$U=\\dfrac{1}{2}\\left(2 k_{1}\\right)(R+a)^{2} \\theta^{2}+\\dfrac{1}{2}\\left(2 k_{2}\\right)(R-b)^{2} \\theta^{2}$ 由于$T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{\\theta}_{\\max }=\\omega_{0} \\theta_{\\max }$ \\begin{align} \\omega_{0}^{2}&=\\dfrac{2\\left[k_{1}(R+a)^{2}+k_{2}(R-b)^{2}\\right]}{3 m R^{2} / 2}\\\\ &=\\dfrac{4}{3 m}\\left[k_{1}\\left(1+\\dfrac{a}{R}\\right)^{2}+k_{2}\\left(1-\\dfrac{b}{R}\\right)^{2}\\right] \\end{align}解得固有频率为：$\\omega_{0}=\\sqrt{\\dfrac{4}{3 m}\\left[k_{1}\\left(1+\\dfrac{a}{R}\\right)^{2}+k_{2}\\left(1-\\dfrac{b}{R}\\right)^{2}\\right]}$ 坐标原点设在静平衡位置，因为计算势能的时候预先压缩造成的重力势能和弹性势能一定会被约掉，得出其次动力学方程。 因此，广义的坐标：质量块的垂直位移x 动能： \\begin{align} T&=\\frac{1}{2} m \\dot{x}^{2}+\\dfrac{1}{2} M\\left(\\dfrac{1}{2} \\dot{x}\\right)^{2}+\\frac{1}{2}\\left(\\dfrac{1}{2} M R^{2}\\right)\\left(\\dfrac{\\dot{x}}{2 R}\\right)^{2}\\\\ &=\\frac{1}{2}\\left(m+\\frac{1}{4} M+\\frac{1}{8} M\\right) \\dot{x}^{2}\\\\ &=\\frac{1}{2}\\left(m+\\frac{3}{8} M\\right) \\dot{x}^{2} \\end{align} 势能：$U=\\dfrac{1}{2} k_{2} x^{2}+\\dfrac{1}{2} k_{1}\\left(\\dfrac{1}{2} x\\right)^{2}=\\dfrac{1}{2}\\left(k_{2}+\\dfrac{1}{4} k_{1}\\right) x^{2}$ 由能量守恒可知： $T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{x}_{\\max }=\\omega_{0} x_{\\max }$ 解得：$\\omega_{0}=\\sqrt{\\dfrac{2 k_{1}+8 k_{2}}{3 M+8 m}}$ 瑞利法利用能量法求解固有频率时，对于系统的动能的计算只考虑了惯性元件的动能，而忽略不计弹性元件的质量所具有的动能，因此算出的固有频率是实际值的上限。 这种简化方法在许多场合中都能满足要求，但有些工程问题中，弹性元件本身的质量因占系统总质量相当大的比例而不能忽略，否则算出的固有频率明显偏高。 当弹簧的质量的质量不是远小于小车的质量时，就无法忽略弹簧的质量，因此这里用等效的质量来分析。 系统最大的势能：$V_{\\max }=\\frac{1}{2} k x_{\\max }^{2}$ $T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{x}_{\\max }=\\omega_{0} x_{\\max } \\quad \\Rightarrow \\quad \\omega_{0}=\\sqrt{\\dfrac{k}{m+m_{t}}}$ 若忽略 $m_t$ ，则$\\omega_0$增大，因此忽略弹簧动能所算出的固有频率是实际值的上限。 瑞利法求解等效的质量是通过积分来求解的。 等效质量与等效阻尼方法一：能量法求解 选定广义位移坐标后，将系统得动能、势能写成如下形式：$T=\\dfrac{1}{2} M_{e} \\dot{x}^{2} ,\\, V=\\dfrac{1}{2} K_{e} x^{2}$ 当 $\\dot x,x$分别取最大值时：$T \\rightarrow T_{\\max }， \\ V \\rightarrow V_{\\max }$ ，可以得出：$\\omega_{0}=\\sqrt{\\dfrac{K_{e}} {M_e}}$ $K_e$：简化系统的等效刚度；$M_e$：简化系统的等效质量。 等效的含义是指简化前后的系统的动能和势能分别相等。 主要步骤： 选定广义坐标 将动能和势能写成标准形式 根据系数写出等效刚度和等效阻尼 动能：$T=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2} \\Rightarrow M_{e}=m l^{2}$ 势能：$V=\\frac{1}{2}\\left(k a^{2}-m g l\\right) \\theta^{2} \\Rightarrow K_{e}=k a^{2}-m g l$ 固有频率：$\\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 势能：$U=\\frac{1}{2}\\left[\\left(2 k_{1}\\right)(R+a)^{2}+\\left(2 k_{2}\\right)(R-b)^{2}\\right] \\theta^{2}$ 等效刚度：$K_{e}=\\left(2 k_{1}\\right)(R+a)^{2}+\\left(2 k_{2}\\right)(R-b)^{2}$ 则固有频率为：$\\omega_{0}^{2}=\\dfrac{2\\left[k_{1}(R+a)^{2}+k_{2}(R-b)^{2}\\right]}{3 m R^{2} / 2}$ 方法二：定义法 等效刚度：使系统在选定的坐标上产生单位位移而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效刚度。 等效质量：使系统在选定的坐标上产生单位加速度而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效质量。 使系统在选定的坐标上产生单位位移而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效刚度。 串联弹簧的刚度的倒数是原来各个弹簧刚度倒数的总和 并联弹簧的刚度是原来各个弹簧刚度的总和。 求：系统对于坐标x 的等效质量和等效刚度。 方法1：能量法 动能：$T=\\dfrac{1}{2} m_{1} \\dot{x}^{2}+\\dfrac{1}{2} m_{2}\\left(\\dfrac{l_{2}}{l_{1}} \\dot{x}\\right)^{2}=\\dfrac{1}{2}\\left(m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}\\right) \\dot{x}^{2}$ 等效质量：$M_{e}=m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}$ 势能：$V=\\dfrac{1}{2} k_{1} x^{2}+\\dfrac{1}{2} k_{2}\\left(\\dfrac{l_{3}}{l_{1}} x\\right)^{2}=\\dfrac{1}{2}\\left(k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}\\right) x^{2}$ 等效刚度：$K_{e}=k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}$ 方法二：定义法 设使系统在x方向产生单位加速度需要施加力P，则在m1、m2上产生惯性力，对支座取矩： $P l_{1}=\\left(m_{1} \\cdot 1\\right) l_{1}+\\left(m_{2} \\cdot \\dfrac{l_{2}}{l_{1}}\\right) l_{2}$ 等效质量：$M_{e}=m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}$ 画虚线是夸张化的m，其实其位移非常的为微小，在零时刻产生了单位加速度$\\ddot x=1$，$m_1,m_2$在平衡位置静平衡位置：重力和弹簧预变形相互抵消，因此弹力可以忽略。 设使系统在x坐标上产生单位位移需要施加力P，则在k1、k2处将产生弹性恢复力，对支点取矩： $P l_{1}=\\left(k_{1} \\cdot 1\\right) l_{1}+\\left(k_{2} \\cdot \\dfrac{l_{3}}{l_{1}}\\right) l_{3}$ 等效刚度：$K_{e}=k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}$ 等效质量(惯性力)：动态求解(达朗贝尔原理)； 等效刚度(静力平衡：静态求解(列静力学方程)。","link":"/2022/01/27/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E8%87%AA%E7%94%B1%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第三章 单自由度系统的受迫振动-第一部分","text":"hljs.initHighlightingOnLoad(); 本文从线性系统的受迫振动、稳态响应的特性、受迫振动的过渡阶段、单自由度库仑摩擦力系统的受迫振动的近似分析四个方面进行讲解。 线性系统的受迫振动简谐力激励下的受迫振动 其实部和虚部分别和 $F_0cos\\omega t$ 、$F_0sin\\omega t$ 相对应。 受力分析： 振动的微分方程：$m \\ddot{x}(t)+c \\dot{x}(t)+k x(t)=F_{0} e^{i \\omega t}$ ，是显含时间的，非齐次微分方程。 这里x为复数变量，分别对应 $F_0cos\\omega t$ 、$F_0sin\\omega t$ 这里设$x=\\bar{x}e^{i\\omega t}$ , 其中的$\\bar{x}$ ： 为稳态响应的复振幅 代入刀振动方程中可得：$\\left(-m \\omega^{2}+i c \\omega+k\\right) \\bar{x} e^{i \\omega t}=F_{0} e^{i \\omega t}$ $\\Rightarrow \\bar{x} = H(\\omega ) F_0$ 复频响应函数：$H(\\omega) = \\dfrac 1 {k-m\\omega^2+ic\\omega}$ 振动方程化为：$\\ddot{x}(t)+2 \\xi \\omega_{0} \\dot{x}(t)+\\omega_{0}^{2} x(t)=B \\omega_{0}^{2} e^{i \\omega t}$ 其中$\\omega_0=\\sqrt{\\dfrac k m}$, $B=\\dfrac {F_0} k$ (静变形) ，$\\xi=\\dfrac{c}{2 \\sqrt{k m}}$ 引入外部激励频率与系统固有频率之比：$s = \\dfrac \\omega {\\omega_0}$ 则有： $H(\\omega)$同时反映了系统响应的幅频特性和相频特性 其中有$\\dfrac{m}{k} \\omega^{2}=\\dfrac{\\omega^{2}}{\\omega_{0}^{2}}=s^{2} \\quad \\dfrac{c \\omega}{k}=\\dfrac{c \\omega}{m \\omega_{0}^{2}}=2 \\xi \\omega_{0} \\dfrac{\\omega}{\\omega_{0}^{2}}=2 \\xi \\dfrac{\\omega}{\\omega_{0}}=2 \\xi s$ $x(t)=\\bar{x} e^{i \\omega t}\\quad \\bar{x}=H(\\omega) F_{0}\\quad H(\\omega)=\\frac{1}{k} \\beta e^{-i \\theta}$ 带入得：$\\Rightarrow x(t)=\\dfrac{F_{0}}{k} \\beta e^{i(\\omega t-\\theta)}=A e^{i(\\omega t-\\theta)}$ 有上式可得，阻尼是产生相位差得根源，而自由振动中简谐振动的相位差造成的原因是初始条件造成的。 没有阻尼c时，$\\zeta=0$ 没有相位差。 主要的公式总结如下： 结论如下： 线性系统对简谐激励的稳态响应是频率等同于激振频率、而相位滞后激振力的简谐振动。位移相位要滞后于输入力的相位。 稳态响应的振幅及相位只取决于系统本身的物理性质（m, k, c）和激振力的频率及力幅，而与系统进入运动的方式（即初始条件）无关。 对于有阻尼振动，初始条件的影响为暂态的，最后就衰减没了。 对于无阻尼的振动，其振动的解应该是稳态和暂态的叠加，是与初始条件有关的。 稳态响应特性复频特性 当$s\\ll 1$时，$(\\omega \\ll \\omega_0)$: 激振频率相对于系统固有频率很低，$\\beta \\approx 1$ 结论：响应的振幅A 与静位移B 相当 就是说外部的激励非常的慢的时候，基本上产生的振幅和静位移相当了。 当$s\\gg 1$时，$(\\omega \\gg \\omega_0)$: 激振频率相对于系统固有频率很高，$\\beta \\approx 0$ 结论：响应的振幅很小 外部的激励非常的快的时候，质量m反应不过来了，振幅几乎没啥响应。 对于$s\\ll 1$和$s\\gg 1$情况而言： 对应于不同 $\\zeta$值，曲线较为密集，说明阻尼的影响不显著 结论：系统即使按无阻尼情况考虑也是可以的 当$s\\approx 1$时，$(\\omega \\approx \\omega_0)$: 对应于较小 $\\zeta$ 值，$\\beta(s)$ 迅速增大，当 $\\zeta$ = 0 $\\Rightarrow$ $\\beta(s) \\rightarrow \\infty$ 结论：共振振幅无穷大； 应用： 对于成像卫星而言，共振会造成所成的像有抖动，模糊，此时的共振需要防止。 对于矿山用的振动筛以及手机的信号都是利用共振现象来放大振幅。 但共振对于来自阻尼的影响很敏感，在s=1 附近的区域内，增加阻尼使振幅明显下降 注意$\\beta_{max}$的位置 对于有阻尼系统，$\\beta_{max}$ 并不出现在s=1处，而且稍偏左 $\\dfrac {d\\beta} {ds}=0 \\Rightarrow s=\\sqrt{1-2\\zeta^2} $ 对应的峰值：$\\beta_{max}=\\dfrac 1 {2\\zeta \\sqrt{1-\\zeta^2}}$ 当$\\zeta &gt; 1/\\sqrt 2$时，$\\beta&lt;1$: 振幅没有峰值 品质因子和半功率带宽之间的关系： 阻尼越弱，对应的$\\zeta$ 会非常的小，会造成 Q 越大，带宽越窄，共振峰越陡峭 相频特性 当$s\\ll 1$时，$(\\omega \\ll \\omega_0)$: 相位差 $\\theta \\approx 0$ 位移与激振力在相位上几乎相同 当$s\\gg 1$时，$(\\omega \\gg \\omega_0)$: $\\theta \\approx \\pi$ 位移与激振力反相 (其实这就解释了为什么此时的响应振幅几乎为0) 当$s\\approx 1$时，$(\\omega \\approx \\omega_0)$: 共振时的相位差为$\\dfrac \\pi 2$ ，与阻尼无关 高层结构地基上有阻尼结构，其恢复力和位移之间存在滞回特性滞回环，时间上的之后。 而激励和位移之间存在相位是的滞后特性。 受迫振动的过渡阶段无阻尼受迫振动的过渡阶段受迫振动的过渡阶段： 在系统受到激励开始振动的初始阶段，其自由振动伴随受迫振动同时发生。系统的响应是暂态响应与稳态响应的叠加。 考虑无阻尼正弦激励的情况： 解得通解是： 带入初始的条件：$x(0)=x_{0} \\quad \\Rightarrow \\quad c_{1}=x_{0}$ $\\dot{x}(0)=\\dot{x}_{0} \\ \\Rightarrow \\ \\dot{x}(0)=c_{2} \\omega_{0}+\\dfrac{B}{1-s^{2}} \\ \\Rightarrow \\ c_{2}=\\dfrac{\\dot{x}_{0}}{\\omega_{0}}-\\dfrac{B s}{1-s^{2}}$ 得出最后的解为： 自由伴随的特点：以系统固有频率为振动频率。 零初始条件： 如果为零初始条件，则自由伴随依然存在： 其解的表达式：$x(t)=-\\dfrac{B s}{1-s^{2}} \\sin \\omega_{0} t+\\dfrac{B}{1-s^{2}} \\sin \\omega t$ $s","link":"/2022/03/06/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%97%E8%BF%AB%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第三章 单自由度系统的受迫振动-第二部分","text":"hljs.initHighlightingOnLoad(); 本文从简谐惯性力的受迫振动，机械阻抗与导纳，以及一些工程上的受迫振动三个方面进行讲解，在单自由度振动上面花了很多的时间，后期的对于多自由度的问题可以是呀模态叠加法是来将其转化为单自由度的问题。 简谐惯性力的受迫振动背景：地基振动，转子偏心引起的受迫振动 特点：激振惯性力的幅值与频率的平方成正比例 以地基的位移为坐标 通过上面的推理可得： m_1\\ddot{x_1}(t)+c\\dot x_1(t)+kx_1(t)=mD\\omega^2e^{i\\omega t}这里我们设：$mD\\omega^2=F_0$ 同时由于$\\omega_0^2=\\dfrac k m,\\ s=\\dfrac \\omega {\\omega_0}$ 振动的解析表达式如下： x_{1}=\\beta B e^{i\\left(\\omega t-\\theta_{1}\\right)}=\\beta \\frac{F_{0}}{k} e^{i\\left(\\omega t-\\theta_{1}\\right)}=\\beta \\frac{m D \\omega^{2}}{k} e^{i\\left(\\omega t-\\theta_{1}\\right)}\\\\ =\\frac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}} D e^{i\\left(\\omega t-\\theta_{1}\\right)}=\\beta_{1} D e^{i\\left(\\omega t-\\theta_{1}\\right)}其中，振幅放大因子为：$\\beta_1=\\dfrac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}}$, 相位差为：\\theta_{1}(s)=\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}} 这里我们对比简谐激励的振动解析表达式： 二者的相位差还是一样的，就是振幅放大因子多了个$s^2$ 这里，我们的可以看出最左边式s&gt;1，质量和地基是同步的； 中间的是s=1，此时相当于发生了共振，其振幅相当大； 最右边是s&lt;1，此时相对的振动的位移是小于地基振动的幅值的。 以绝对位移为坐标 如果以绝对位移为坐标：$x(t)=x_{1}(t)+x_{f}(t)$ 其中：$x_{1}(t)=\\beta_{1} D e^{i\\left(\\omega t-\\theta_{1}\\right)}$ ，$x_{f}(t)=D e^{i \\omega t}$； 于是：$x(t)=\\beta_{1} D e^{i\\left(\\omega t-\\theta_{1}\\right)}+D e^{i \\omega t}=(\\beta_{1}+e^{i\\theta_{1}})D e^{i\\left(\\omega t-\\theta_{1}\\right)}$ 其中，振幅放大因子为：$\\beta_1=\\dfrac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}}$, 相位差为：$\\theta_{1}(s)=\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}$ 于是有： \\begin{align} \\beta_{1}+e^{i \\theta_{1}}&=\\frac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}+\\left(\\cos \\theta_{1}+i \\sin \\theta_{1}\\right)\\\\ &=\\frac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}+\\frac{1-s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}+i \\frac{2 \\zeta s}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}\\\\ &=\\frac{1+2 i \\xi_{S}}{\\sqrt{\\left(1-S^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}=\\frac{1}{\\sqrt{\\left(1-S^{2}\\right)^{2}+\\left(2 \\zeta_{S}\\right)^{2}}} \\sqrt{1+\\left(2 \\xi_{S}\\right)^{2}} e^{i \\theta_{2}}\\\\ &=\\sqrt{\\frac{1+\\left(2 \\xi_{S}\\right)^{2}}{\\left(1-S^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}} e^{i \\theta_{2}}\\\\ &=\\beta_{2} e^{i \\theta_{2}} \\end{align} 于是有： 表达式 振幅 相位 地基位移 $\\beta_{1} D e^{i\\left(\\omega t-\\theta_{1}\\right)}$ $\\dfrac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}}$ $\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}$ 绝对位移 $\\beta_{2} e^{i \\theta_{2}}$ $\\sqrt{\\dfrac{1+\\left(2 \\xi_{S}\\right)^{2}}{\\left(1-S^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}$ $\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}-\\operatorname{tg}^{-1}\\left(2 \\xi_{S}\\right)$ 最后带入到表达式中，得到振动的解析表达式为： $x(t)=\\beta_{2} D e^{i\\left[\\omega t-\\left(\\theta_{1}-\\theta_{2}\\right)\\right]}=\\beta_{2} D e^{i(\\omega t-\\theta)}$，其中$\\theta=\\theta_{1}-\\theta_{2}$ 无阻尼时的表达式为：$x(t)=D \\dfrac{1}{1-s^{2}} e^{i \\omega t}$ 使用其他的方法进行分析角度位移为坐标的情况 x(t)=\\frac{k D}{k} \\frac{1}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}} \\sin \\left(\\omega t-\\theta_{1}\\right)+\\frac{c D \\omega}{k} \\frac{1}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}} \\cos \\left(\\omega t-\\theta_{1}\\right)\\\\ =\\frac{D}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}}\\left[\\sin \\left(\\omega t-\\theta_{1}\\right)+2 \\xi s \\cos \\left(\\omega t-\\theta_{1}\\right)\\right]其中有， $\\dfrac{c \\omega}{k}=\\dfrac{c \\omega}{\\omega_{0}^{2} m}=\\dfrac{c}{m} \\cdot \\dfrac{s}{\\omega_{0}}=2 \\xi \\omega_{0} \\dfrac{s}{\\omega_{0}}=2 \\xi_{S}$， $\\theta_1=\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}$ 之后，将$\\sin \\theta_{2}=\\dfrac{2 \\xi_{S}}{\\sqrt{1+\\left(2 \\xi_{S}\\right)^{2}}}$和$\\cos \\theta_{2}=\\dfrac1 {\\sqrt{1+\\left(2 \\xi_{S}\\right)^{2}}}$ 此时有$\\theta_2=\\operatorname{tg}^{-1}\\left(2 \\xi_{S}\\right)$ 于是令：$\\theta = \\theta_1-\\theta_2$ , $\\beta=\\sqrt{\\dfrac{1+\\left(2 \\xi_{S}\\right)^{2}}{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}}$ 带入到上述式子中。 和前面第一种方法的结果是一样的。 实际案例 首先，确定系统输入的激励： 之后，求解再空载和满载的时候，我们的阻尼c是不变的，改变的只有质量； 因此根据公式$c_{cr}=2\\sqrt{km},\\ \\dfrac c m=2\\zeta\\omega_0$ 可得： $\\Rightarrow \\ c=\\zeta c_{cr}=2\\zeta \\sqrt{km}$, 由于c和k是常数，因此可以得出：$\\zeta$ 和 $\\sqrt m$ 成反比 进而得到空载时的阻尼比为：$\\xi_{2}=\\xi_{1} \\sqrt{\\dfrac{m_{1}}{m_{2}}}=1.0$ ; 满载和空载时的频率比： 满载：$s_{1}=\\dfrac{\\omega}{\\omega_{01}}=\\omega \\sqrt{\\dfrac{m_{1}}{k}}=1.87$ 空载：$s_{2}=\\dfrac{\\omega}{\\omega_{02}}=\\omega \\sqrt{\\dfrac{m_{2}}{k}}=1.87$ 然后，计算振幅，满载时振幅 B1，空载时振幅 B2： 满载：$\\dfrac{B_{1}}{a}=\\sqrt{\\dfrac{1+\\left(2 \\xi_{1} s_{1}\\right)^{2}}{\\left(1-s_{1}^{2}\\right)^{2}+\\left(2 \\xi_{1} s_{1}\\right)^{2}}}=0.68$ 空载：$\\dfrac{B_{2}}{a}=\\sqrt{\\dfrac{1+\\left(2 \\xi_{2} s_{2}\\right)^{2}}{\\left(1-s_{2}^{2}\\right)^{2}+\\left(2 \\xi_{2} s_{2}\\right)^{2}}}=1.13$ 因此可得振幅比为：$B_1/B_2=0.60$ 首先，将模型进行简化，并求解系统的固有频率$\\omega=\\sqrt{k/m}=\\sqrt{g/\\delta}$： 等效的时候可以将我们的杆子看作刚体，将A处的运动等效过去；然后再将杆子看作是柔性体，计算刚度k。 之后，列出动力学方程： $m \\ddot{x}+k\\left(x-x_{f}\\right)=0 \\Rightarrow m \\ddot{x}+k x=\\dfrac {k b d} a \\sin \\omega t$ 根据上面的求绝对位移为坐标，且无阻尼情况下的方程， 计算可得：$\\bar{x}=\\dfrac{k b d / a}{k} \\cdot \\dfrac{1}{1-s^{2}}=\\dfrac{b d}{a} \\cdot \\dfrac{1}{1-s^{2}} \\quad s=\\dfrac{\\omega}{\\omega_{0}}$ 支撑运动的小结 这里的输入不是力，是基础的位移。 这里我们有两种坐标形式来考虑动力学方程的建立，一个是使用相对位移坐标，另一个是研究绝对位移坐标。 振幅放大因子，两种位移坐标下是不一样的。 如果要想隔振有效果，那么$s&gt;\\sqrt{2}$ 转子偏心问题高速旋转机械中，偏心质量产生的离心惯性力是主要的激励来源。旋转机械总质量为M，转子偏心质量为m，偏心距为e，转子转动角速度为 $\\omega$ 这里$me$：不平衡量，$me\\omega^2$：不平衡量引起的离心惯性力。 于是我们设：$F_0=me\\omega^2$, $x(t)=\\beta B \\sin (\\omega t-\\theta)$ 其中$\\beta=\\dfrac 1 {\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2} } }$, $B=\\dfrac {F_0} k=\\dfrac {me\\omega^2} k$ , $s=\\dfrac{\\omega}{\\omega_{0}}$, $\\theta=\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}$, 注意$\\omega = \\sqrt{\\dfrac K M}$ 这里B可以写作为：$B=\\dfrac {F_0} k=\\dfrac {me\\omega^2} k = \\dfrac {me\\omega^2} {\\omega_0^2M}=\\dfrac {me} {M} s^2$ 此时振动的表达式为： $x(t)=\\dfrac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}} \\dfrac{m e}{M} \\sin (\\omega t-\\theta)=\\beta_{1} B_{1} \\sin (\\omega t-\\theta)$ 其中，$\\beta=\\dfrac 1 {\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2} } }$, $B_1=\\dfrac {me} M$ 公式：$x(t)=\\dfrac{s^{2}}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}} \\dfrac{m e}{M} \\sin (\\omega t-\\theta)$ 首先，共振的时候，我们的s=1; 代入得共振时的最大振幅是：$\\dfrac 1 {2\\zeta} \\dfrac {me}{M}=0.1(m) \\ \\Rightarrow \\ e=0.1(m)$ 其次，这里假设增加质量后阻尼是不变的，其实是要变的： $\\dfrac 1 {2\\zeta} \\dfrac {me}{M+\\Delta M}=0.01(m) \\ \\Rightarrow \\ \\dfrac{\\Delta M} {M}=9$ 得出：$\\Delta M=9M$ 机械阻抗与导纳 工程中常用机械阻抗来分析结构的动力特性； 机械阻抗定义为：简谐激振时复数形式的输入与输出之比； 位移阻抗 Z_{x}(\\omega)=\\dfrac{F_{0} e^{i \\omega t}}{\\bar{x} e^{i \\omega t}}=\\dfrac{F_{0}}{\\bar{x}}=\\dfrac{1}{H(\\omega)}=k-m \\omega^{2}+i c \\omega\\\\ H(\\omega)=\\frac{1}{k-m \\omega^{2}+i c \\omega}位移阻抗与复频响应函数互为倒数， $H(\\omega)$ 也称为导纳。 输出也可以定义为速度或加速度，相应的机械阻抗称为速度阻抗和加速度阻抗。 速度阻抗 Z_{\\dot x}(\\omega)=\\frac{F_{0} e^{i \\omega t}}{\\dot{x}}=\\frac{F_{0} e^{i \\omega t}}{i \\omega \\bar{x} e^{i \\omega t}}=\\frac{1}{i \\omega} Z_{x}(\\omega) 加速度阻抗 Z_{\\ddot{x}}(\\omega)=-\\frac{1}{\\omega^{2}} Z_{x}(\\omega) 机械阻抗的倒数称为机械导纳，相应 $Z_{x}(\\omega)$、$Z_{\\dot x}(\\omega)$ 、$Z_{\\ddot x}(\\omega)$ 分别有位移导纳、速度导纳和加速度导纳。 机械阻抗和机械导纳都仅仅取决于系统本身的动力特性（m，k，c），它们都是复数。 现已有多种专门测试机械阻抗的分析仪器，根据系统的机械阻抗可以确定和分析系统的固有频率、相对阻尼系数等参数及其它动力特征 复频响应函数又可写为： H(\\omega)=\\frac{1}{k-m \\omega^{2}+i c \\omega}=\\frac{1}{k} \\cdot \\frac{1}{\\left(1-s^{2}\\right)+i\\left(2 \\xi_{S}\\right)}其中的模与幅角为： |H(\\omega)|=\\frac{1}{k} \\cdot \\frac{1}{\\sqrt{\\left(1-s^{2}\\right)^{2}+(2 \\xi s)^{2}}}=\\frac{\\beta}{k}=\\frac{\\beta}{F_{0} / B}=\\frac{\\beta B}{F_{0}}\\\\ \\arg H(\\omega)=\\operatorname{tg}^{-1} \\frac{2 \\xi_{S}}{\\left(1-S^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2}}=-\\theta$H(\\omega)$同时反映了系统响应的幅频特性和相频特性。 $\\operatorname{Re}(H)=\\dfrac{1}{k} \\cdot \\dfrac{1-s^{2}}{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi{s}\\right)^{2}} \\quad \\operatorname{Im}(H)=-\\dfrac{1}{k} \\cdot \\dfrac{2 \\xi_{s}}{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}$ 还可以用频率比 s 或相对阻尼系数 $\\xi$ 作参变量，把 $H(\\omega)$ 画在复平面上，这样得到的曲线称为乃奎斯特图(Nyquict plot) 工程中的受迫振动问题惯性式测振仪 此时，由$s=\\dfrac \\omega {\\omega_0}$ 此时 $s \\rightarrow \\infty \\Leftrightarrow \\omega&gt;&gt;\\omega_{0}$ 有$\\lim \\limits_{s \\rightarrow \\infty} A_{1} \\approx D$ 低固有频率的测量仪用于测量振动的位移幅值，称为位移计。 当仪器固有频率远小于外壳振动频率时，仪器读数幅值 A1 接近外壳振动的振幅 D。 A1还可以写成：$A_{1}=\\dfrac{1}{\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi{s}\\right)^{2}}}\\left(\\dfrac{D \\omega^{2}}{\\omega_{0}^{2}}\\right)$ 此时，由$s=\\dfrac \\omega {\\omega_0}$ 此时 $s \\rightarrow 0 \\Leftrightarrow \\omega \\ll \\omega_{0}$ 于是有$\\lim \\limits_{s \\rightarrow 0} A_{1} \\approx \\dfrac 1 {\\omega_0^2}(D\\omega^2)$ ；其中$D\\omega^2$：被测物体的加速度幅值。 当仪器的固有频率远大于外壳振动频率时，仪器读数的幅值 A1与外壳加速度的幅值成正比。 高固有频率测量仪用于测量振动的加速度幅值，称为加速计。 振动的隔离主动隔振将作为振源的机器设备与地基隔离，以减少对环境的影响称为主动隔振。 具体的计算如下所示： 其中，$\\beta=\\dfrac 1 {\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2} } }$,$s=\\dfrac{\\omega}{\\omega_{0}}$, $\\theta=\\operatorname{tg}^{-1} \\dfrac{2 \\xi s}{1-s^{2}}$ 其中，$\\theta_{2}=\\operatorname{tg}^{-1} 2 \\xi s ,\\quad \\dfrac{c \\omega}{k}=\\dfrac{c \\omega}{\\omega_{0}{ }^{2} m}=\\dfrac{c}{m} \\cdot \\dfrac{s}{\\omega_{0}}=2 \\xi \\omega_{0} \\dfrac{s}{\\omega_{0}}=2 \\xi_{S}$ 因此可得隔振系数为：$\\eta = \\dfrac{F_{1\\max} } {F_0} = \\sqrt{\\dfrac{1+\\left(2 \\xi_{S}\\right)^{2}}{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi_{S}\\right)^{2} } }$ 画出主动隔振系数的图像，可得到以下的结论： 典型例题： 小结：本节课是如何计算隔振后的力，主动隔振系数的计算、已经频率比的选择问题。 被动隔振将地基的振动与机器设备隔离，以避免将振动传至设备，称为被动隔振。 这里主动隔振系数和被动隔振系数的表达式时相同的，但是其中的内涵是不一样的。 主动隔振系数是力之比，而被动隔振系数是位移之比。 典型例题： 为计算梁自身质量对其横向振动固有频率的影响，需确定梁质量等效到自由端时的等效质量，然后可按单自由度计算悬臂梁横向振动的固有频率。 这里我们首先计算是悬臂梁的等效质量： 等效之后系统称为一个单自由度系统： 根据刚度和等效的质量可以得出： 电路板的固有频率：$\\omega_0 = \\sqrt{\\dfrac {k_b}{m_{eq}}}=\\sqrt{\\dfrac{1.296\\times10^6}{0.5893}}rad/s=1482.99\\ rad/s$ 计算机底座的频率：$\\omega = \\dfrac{2\\pi \\times 3000}{60} = 312.66\\ rad/s$ 频率之比：$s=\\dfrac{\\omega}{\\omega_0}=\\dfrac{312.66}{1482.99}=0.2108$ 位移传递率： \\eta = \\sqrt{\\dfrac{1+\\left(2 \\xi_{S}\\right)^{2}}{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi{s}\\right)^{2}}}=\\sqrt{\\dfrac{1+\\left(2 \\times 0.01\\times0.2108\\right)^{2}}{\\left(1-0.2108^{2}\\right)^{2}+\\left(2 \\times0.01\\times{0.2108}\\right)^{2}}}=1.0465 位移传递率（104.65%）已超过最大许可值（10%）; 因此这里假装了隔振器，这里加入以后可能会成为一个双自由度的系统，因此为了简化模型，将梁简化为刚体。 于是计算隔振器的频率和刚度： $k=\\dfrac {M\\omega^2} {s^2} = \\dfrac {2.75\\times 312.66^2}{11.0218}N/m = 24390.7309\\ N/m$ 隔振器的阻尼常数为： $c=2\\xi\\sqrt{Mk}=2\\times 0.01 \\sqrt {2.75\\times24390.7309}=5.1797\\ N\\cdot s/m$ 转子的临界转速气轮机、发电机等高速旋转机械在开机或停机过程中经过某一转速附近时，支撑系统经常会发生剧烈振动。 轴以角速度$\\omega$恒速旋转，轴沿着x和y方向的横向刚度：$k=\\dfrac {48EI} {l^3}$ 由于离心惯性力，轴产生的静扰度$OO_1=f$ 粘性阻尼力正比于圆盘形心 $O_1$ 的速度 设形心 $O_1$ 的坐标$(x,y)$，质心C的坐标为$(x+e\\cos\\omega t,\\ y+\\sin \\omega t)$ 由质心的运动定理： m\\frac{d^2}{dt^2} (x+e\\cos\\omega t)=-kx-c\\dot x\\\\ m\\frac{d^2}{dt^2} (y+e\\sin\\omega t)=-ky-c\\dot y\\\\ 设：$\\omega_0 =\\sqrt{k/m}=\\sqrt{g/l_0}$ ，其中$l_0$是静变形，$\\omega_0 $ 转子不转动而作横向自由振动时的固有频率 此时动扰度为：$f=\\sqrt{x^2+y^2}=e\\beta_1=\\dfrac {es^2} {\\sqrt{\\left(1-s^{2}\\right)^{2}+\\left(2 \\xi{s}\\right)^{2} } }$ 当s=1时：$f=\\dfrac e {2\\xi}$ ,此时的$\\omega=\\omega_0$,即转动频率与转子的固有频率相等； 可见，当阻尼比$\\xi$ 较小时，即使转子平衡得很好(e 很小)，动挠度f 也会相当大，容易使轴破坏，这样的转速称为临界转速： 每分钟的转速表示：$n_f=\\dfrac {60\\omega_f}{2 \\pi} (r/min)$ 其中，$\\omega_f=\\omega_0=\\sqrt{k/m}$ 当s&gt;&gt;1时，即$\\omega \\gg \\omega_0$, 有：$\\beta_1 \\approx1$ , $\\theta_1 \\approx \\pi$ , 转动频率远大于转子固有频率, 出现自动定心现象。 这里面主要学会如何求临界转速，以及了解自动定心现象。 计算实例 这里，在单自由度振动上面花了很多的时间，后期的对于多自由度的问题可以是呀模态叠加法是来将其转化为单自由度的问题。","link":"/2022/03/08/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%97%E8%BF%AB%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"},{"title":"论文写作指导","text":"hljs.initHighlightingOnLoad(); 本文为伍老师给博士的写作指导课，主要简述的是科研论文写作的注意事项、IEEE的相关规范以及Latex的使用技巧等。 论文撰写基本语法 标题和关键字 摘要 简介 相关工作 问题和方法 实验和分析 总结 参考文献 这里不一定要按照上述的顺序，但是该有的结构一定要有。这样便于信息的定位和内容的完备。 语义 要让读者读的舒服为目的。 建议： 必要让评委和审稿人去猜 临近相同的区域避免同一个词的重复出现 避免句子以“And”开头 避免过长的句子 避免使用过于生晦的词汇 具体内容部分 标题 标题要精炼、突出 让编委和评委快速了解论文的方向，吸引读者 摘要 主编会看的部分，评委和读者的入口 就是主编通过看摘要来判断论文的方向，进而选择合适的副编，之对于最后的能否录用很关键。 必须描述论文主要的创新和贡献，内容要有足够的吸引性 不要有公式，不要冗长，就是不要写多段。 简介 陈述问题的背景和意义 简述过去在这个方向都做了什么工作 指出之前工作的不足之处(要委婉一点，万一审稿人就是之前工作的作者)，也就是提交的Motivation和拟解决的主要问题 简介工作的主要创新点 简介要针对前述问题的创新方法，让人感觉可以解决前述问题 不要过长，更具体地信息要留在后面来写，简介主要突出关键点 方便编委快速确认论文的方向，针对性地挑选编委 吸引读者阅读论文 相关工作 注重对调研工作的质量 从评委和主编的角度来看，调研工作的质量决定了你的品味，进而影射了你工作的质量和水准。 其实就是文献综述的时候不要引用太差劲的期刊上的水文，要跟顶刊和顶会看齐。 相关工作也是对过去领域内相关工作的调研和整理 概述过去工作的主要贡献 突出过去工作的不足之处 凸显提交工作的”先进性“ 确保你的工作有效地针对了当前工作的不足之处，并由卓有成效的推进 确保你工作的能够打动评委，能够吸引期刊的读者 很像一个读书笔记，不像是一个相关工作 问题和方法 问题陈述 比摘要和简介中进一步的陈述 问题要让人容易理解，在可能的情况下，举例说明会更清晰 一般的计算机顶会会有一张图，来介绍本文的思路，现在的审稿人会直接先看有没有那张图。 创新方法 ”为什么why“比“怎么how”还重要 why-&gt;how-&gt;what 聚焦于“怎么”做的容易陷入技术报告，缺乏逻辑上的串联 聚焦于“为什么”，这样会吸引评委和读者思考（从而认同），并且能够自然的带出“怎么做的”。 对这个方法多讨论，避免现如技术报告 实验与分析 在论文实验部分，牢记“实验是对所提方案的验证”，有三个要素不可缺少。一是可重复性，要有公认的实验场景和配置；二是可解释性，图片清晰，避免出现图片“误用”；三是要可分析性，即佐证结论。 公认的实验场景和配置 可参考相关领域的定会顶刊所发表的实验设置 实验要针对所研究的问题和目标 相关工作的对比，要state-of-the-art (呼应:相关工作) 对于自己的数据集最好要公开数据和代码 实验图表现要专业、清晰 尽可能使用专业的绘图工具python、Matlab 清晰度、字体的大小适当 可学习相关领域顶会顶刊的图标 实验结果的分析 分析不是简单地陈述实验结果，重点是内 解释为什么提升了 消融性的分析，三个条件缺少了某个会造成什么样的后果 灵敏度分析，对超参数进行敏感性分析 多读论文 不仅仅凸显所提出方案的有优势，更能凸显为什么有优势（呼应：为什么比怎么做还重要） 左侧的图片可解释性不是很强，因为齐亨佐那个坐标的标题栏以及图片的比例都不是很好，有点空。 其次是没有图例，legend的标注。 对比清晰，可以不是和state of the art进行比较，至少要让读者感觉你对自己的方法有信心；用了专业的作图工具，且考虑了黑白打印的效果。 总结 像摘要，但是又不是摘要 总结向另一个更形象的词：takeaway 是读者在阅读完论文的一个回应 在进一步理解的基础上再次总结论文的主要贡献(拔高) 展望(可选)，在现有的工作的基础上，指出下一步工作的方向和重点 这里展望要有理有据，不是瞎说 其次不要过渡的否决自己的工作，防止被拒稿或者打回来修改 实事求是 展望，不是猜想，也要有理有据 避免变成对提交工作的否决 排版 尽量使用LaTeX进行排版 专业、漂亮、对公式友好 换投时，修改格式方便快捷 使用指定的论文模板 IEEE for IEEE Springer for Springer 否则，主编可以直接拒稿。 定理最后的方框代表是否证明结束，这里面证明结束的方框是没有对齐的。 审稿回复 逐条阅读并且理解审稿让人的意见 逐条撰写回应，一方面解释，另一方面直接陈述论文相应怎么改的 遇到审稿人的意见方向不对的要怎么办？ 先从自身找原因： 逻辑不清，审稿人没有找到？ 没有表述清楚，引发歧义？ 忽略了，引起误解？ 多方证实，确实是审稿人的意见不对，就礼貌地回应。 写回复地时候也是要委婉的说是自己写的不好引起了您的误解。 之前的部分为过敏意老师的内容，之后的为伍老师自己总结的内容。 其他注意事项单复数、时态 不可数名词：equirment、research、faculty、noise。 可数名词：(这里就是要注意以上名词的单复数) Datum $\\rightarrow$ data Statistic $\\rightarrow$ Statistics Criterion $\\rightarrow$ Criteria Analysis $\\rightarrow$ Analyses Spectrum $\\rightarrow$ Spectra 在欧洲一般表达都用被动语态，但是在美国可以接受被动语态，因此这方面大可放心。 就算只有一个作者，也要用We 而不是 I 实验部分一般用过去时态 介绍别人的方法，一般用现在时，而不是过去式 related work 上面表述别人的方法，要使用一般现在时态，而不是过去时态 其他的注意事项 首先还是不要过分夸大自己而贬低别人 图、表、伪代码等应出现在正文以后(要先图，然后再文中出现figx.)，居中对齐，不要跨页，标题不要分离。 图的清晰度要高，字体不要太大或太小 不要出现错别字(then &amp; than) 每一段话也比要太长，多用短句，每段话也不宜过长。 每条参考文献都要在正文中得到引用 每个符号都要定义，(让别人帮忙检查) 尽量不要缩写，伍老师的经验来看少于5次就不要用缩写了 论断不要太绝对：may、could 表示转折用whereas 而非 while 表示其他用 Additionally / In addition 而非 Besides Principal component analysis , 而不是Principle 一般用approach , 而非method(比较口语化和低端) If 后面的从句最好加 then(英语为母语的人比较习惯) ; although 后面一定不加but Compared with … ,而非 Comparing with; 区分compared with(实验方法最比较) 和 compared to(做时会用). It ‘s worth doing $Latex$ 中的左引号”是``(数字1旁边的那个点) 不是正常的” 冠词的使用，主要是根据实际的发音来定的 An hour , an 8-hour delay , an SVM(这里是因为S的发音) an FS , &lt;-&gt; a fuzzy set a Euclidean space :zap:Defend 的意思 Defend : 保护、保佑， e.g. ,God Defend New Zealand Defend against : 抵御、防御，e.g. ,defend againist adversarial attacks 句子不要以And 开头 句子要写完整，When the number of labeled instances is in few-shot TL setting ,i.e.,each category with instances less than 10 3个或以上的作者才用et al. A proposed … ; A and B proposed … ; A et al. proposed … 公式后的标点符号 公式之后的逗号是因为后面的是前面的补充，这一句还没说完。 其次公式之间的等号要对齐。 这里的公式有逗号是由于，(13)(14)是下一句的一部分，因此没有标点符号。 学术规范IEEE规定：投稿有时候我们感觉审稿的希望不大时，我们可以选择撤回稿件，在去投别的期刊，但是一定要得到确认文章被撤回的信息才能下一轮投稿。 有些期刊比较反感axiv，因此在投稿的时候要提前声明，防止其追究一稿多投的责任。 IEEE规定：参考和引用图片的规定，如果文章已经发表，那么文章的版权归出版社所有，就算要引用自己论文里的图片，也要和出版社打招呼，征求同意。 因此我们对于以前自己的图还是重新画一下吧。 IEEE规定：论文抄袭&gt;50%的判定及处罚决定 这里对抄袭的规定有两种： 其一，全文抄袭，这里包括直接使用了他人的文章里的句子没有引用。(比较严重) 其二，50% 可以是一篇全问抄袭了50%，或者是同一个作者连投两篇论文，一共50%. IEEE规定：论文抄袭&gt;50%的处罚决定 包括在IEEE公开的数据库里写道歉信； 情节严重者会5年内不许投稿IEEE，就是直接拒稿。 连续两次会被终身拒稿。 IEEE规定：论文抄袭20-50%的判定及处罚决定 基本的判定和上面一样。 处罚不同的是，3年之内禁止发论文； 也要写处罚决定； 其次就是再投的文章或者已接受但未发表的文章直接拒稿。 IEEE规定：论文抄袭&lt;20%的判定及处罚决定 基本的判定规则还是一样的，一篇&lt;20%或者是同作者的同时投的文章&lt;20%。 不用被禁投 但是要写处罚决定 IEEE规定：不正确引用情况1：整段整句的引用 直接抄要将文字变为斜体，并加上参考文献。 要是用自己的话来描述别人的思路，那直接给出参考文献即可。 如果被认为是抄袭，要要在数据库道歉，并给对方的作者和期刊的主编道歉 情况2：大篇幅的引用 相当于是剽窃思路这类的； 道歉并且在本期刊发表撤稿声明 IEEE规定：不引用已有的文献 这种问题主要是，有一个和自己很相关的工作，但是效果不如人家的好，又怕别嘲讽没有意义，所以故意不去引用。 初犯通常主编直接警告处理，多次出现要写检讨。 IEEE规定：一稿多投 给主编写道歉信，并公开。 经验技巧理工科写论文的技巧建议使用Latex ，美观、高效、格式切换方便 MikTex,https://miktex.org，后台引擎 WinEdt，https://www.winedt.com，前端编辑 JabRef，https://www.jabref.org，参考文献管理 Latex里的公式 这里伍老师推荐使用align环境来排版 不带编号的公式的排版可以使用align*或者是使用\\nonumber 这里的\\[ *** \\]的作用和align环境相同。 Latex里的图片 对于插图一般是使用的是\\usepackeage{graphic,subfigure} 尽量是使用最简单的命令，防止无法运行成功。 图片的格式最好使用.eps矢量图，因为矢量图放大后不会变糊，而位图会变糊。 [htbp] -option参数，代表有限放到当前的位置h，否则地方不不够，会放到下一列的最顶端t，其次放到整体页面的两列的下方b。 clip: 代表直接裁掉边缘的空白。 产生EPS图像 这里伍老师建议还是再ppt中画图比较好，进本可以满足我们的基本要求，下载Visio比较麻烦，别人想让编辑还要再下载一个Visio。 PPT图像转EPS: PPT中选中图像，右键另存为jpg或png，然后用wxbmpp https://lsourceforge.net/p/dktools/wiki/wxbmpp 转EPS Matlab中定制图像大小和缺省字体: 12figure('Position',[100 100 400 300]);set(gcf，'DefaulttextFontName'，'times new roman'，'DefaultaxesFontName','times new roman' , 'defaultaxesfontsize', 11); Matlab 中定制 legend列数和位置: 1H = legend(CSP-LDA’,'CSP-CLDA’, 'location', 'eastoutside' , 'numcolumns'，1);set(h, 'fontsize', 9, 'position', get(h,'position')+[0.13 0.1 0 0]); Matlab 中存彩色EPS图像: 12saveas(gcf, 'name.eps' , 'epsc2');# epsc2 中的c代表color，如果不写c会存成黑白图片 Latex中的表格 使用的是\\usepackage{multirow}包。 Latex中的伪代码 使用的是\\usepackage[linked, ruled]{algorithm2e}包 这个例子还是全的，包括了代码的注释、if-end，while-end，if-then. 很有代表性，仔细参考。 参考文献参考文献管理 LaTex的参考文献建议用JabRef来创建和管理bib文件 假定 bib文件名为drwubib.bib，需要的时候只要在文章结尾\\end {document}之前加入:\\bibliographystyle{IEEEtran} \\bibliography {drwubib}即可以自动处理参考文献 \\Bibliographystyle {IEEEtranS}是把文献按第一作者 last name(姓)音序排列，IEEEtran是按照参考文献在正文中出现的顺序排列。 要使所有 .tex 文件都能找到公用的 bib 文件，而不是手动在每个 .tex 文件目录里面copy一份 bib 文9件，可以把公用的bib 文件 copy到MikTex 的对应目录下，比如C:Program Files\\LaTex\\MiKTeX2.9\\bibtexlbibldrwubib 对于老版本的 WinEdt，在菜单选择Tex→MikTex → Options → General → Refresh FNDB, MikTex以后就可以自动找到bib文件 对于新版本的WinEdt，可能菜单里面没有上述选项，或者不可用。解决办法是在开始菜单里面搜索MikTex Console，Tasks 菜单里面有Refresh file name database 特殊的对于中文的参考文献 要在JabRef中使用中文参考文献，需要设置:Options →Preference → General → Default Encoding → UTF8Options → Preference → Appearance → Set table font → MicrosoftYaHei 否则中文可能是乱码Preference设置好之后可export保存，以后装新版本时直接import进去Preference文件是不同版本通用的 一般要求 一般参考文献之间使用的是 and 连接, 因为逗号有其特殊的含义。 { } 的作用是防止编译器将其自动变成小写。 参考文献格式期刊论文Article Volume(卷)、issue(期)、pages都不能少 IEEE Transactions on XXX一般简写为{IEEE}Trans. on XXX 注意Trans后面有个点，期刊名首字母大写 花括号里面的文字在输出时大小写不变，这样可防止编译后IEEE变成Ieee 文章标题中有特殊字符需要大写的话，也用花括号包起来 会议论文 inproceedings 会议月份、地点都不能少 有时候没有页码，就不输入 Proceedings的简写是Proc. International的简写是Int’l Conference的简写是 Conf. 注意后面的点，全议名首字母大写 在美国开的会议，要写出主办城市和州的缩写，NV内华达州 Latex文档的Track Change 安装latexdiff： https://ctan.org/pkg/latexdiff?lang=enlatexdiff —math-markup=0 old.tex new.tex &gt; diff.tex 如果检查数学公式有无变化出问题，可设置—math-markup=0,不检查公式更新 Latexdiff不是完美的，产生的tex文件里面可能有错误,无法编译，对数学命令和表格命令会出错 最常见是在表中，可用新文档中的表格源码直接覆盖diff.tex中对应的部分，即表格就不track change了 因为期刊审稿的过程中会要求提交新老版本之间的区别。这样操作比较的方便。 蓝色为新添加的内容，红色为修改删去的内容。 PDF字体嵌入 IEEE会议文章为什么要求字体embedded? 有些文章用了特殊的字体(德语韩语如果没有装可能无法显示)，读者计算机上面可能没有安装，导致文章无法正常显示。把字体embed 到文章中去，就相当于编译程序时把所有依赖的包都打包进去，这样没有安装该字体的读者也能正常阅读 IEEE会议文章为什么要求字体subset? 一个字体包括很多字母和符号，体积可能很大，而文章中用到的可能只有几个字符。Subset就是只打包这几个用到的字符，从而避免文章体积过大 （期刊文章后期会有对于的编辑帮我们处理，而会议论文没有人帮我们后期处理） IEEE期刊文章为什么不要求字体embedded subset? IEEE的期刊文章都会经过编辑部重新排版整理，他们会处理这些东西不需要作者考虑。会议文章是作者上传最终版，所以责任在作者 字体嵌入的方法 PPT内使用Latex: lguanaTexhttp://www.jonathanleroux.org/software/iguanatex 但是，这个仅仅是个插件，不可以将一整个命令都copy过来。 在生成的公式是以一个图的形式插入在ppt中，不管怎样放大都很清晰。 而且这个公式的很方便编辑，而且就算别人的电脑里没有这个插件也是可以正常显示的。 Matlab中使用LaTex 如上图所示，'interpreter','latex'表示解析latex的命令。 使用latex的字符会更加的美观。","link":"/2022/03/03/%E7%A7%91%E7%A0%94%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C%E6%8C%87%E5%AF%BC/"},{"title":"第三章 单自由度系统的受迫振动-第三部分","text":"hljs.initHighlightingOnLoad(); 本文从任意周期的激励响应，非周期的激励响应以及任意激励的响应（杜哈梅积分）三个方面进行讲解，并复习了关于傅里叶级数得相关知识。 任意周期激励的响应理论基础前面讨论的强迫振动，都假设了系统受到激励为简谐激励，但实际工程问题中遇到的大多是周期激励而很少为简谐激励。 假定粘性阻尼系统受到的周期激振力：$F(t)=F(t+T)$, T为周期。 则傅里叶级数展开为： F(t)=\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left(a_{n} \\cos n \\omega_{1} t+b_{n} \\sin n \\omega_{1} t\\right)=\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty} c_{n} \\sin \\left(n \\omega_{1} t+\\varphi_{n}\\right)其中：$c_n=\\sqrt{a_n^2+b_n^2},\\quad \\varphi_n=tg^{-1}\\dfrac {a_n} {b_n}$ , $\\tau$为任一时刻; \\begin{align} a_{0}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) d t\\\\ a_{n}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) \\cos n \\omega_{1} t d t \\quad n的偶函数\\\\ b_{n}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) \\sin n \\omega_{1} t d t \\quad n的奇函数\\\\ \\end{align}之后建立运动的微分方程： 其中： $s=\\dfrac {\\omega_{1}}{\\omega_{0}} \\quad \\beta=\\dfrac{1}{\\sqrt{\\left(1-n^2s^{2}\\right)^{2}+(2 \\xi ns)^{2}}} \\quad \\theta_n=\\operatorname{tg}^{-1} \\dfrac{2 \\xi n{s}}{1-n^2s^{2}} $ 其中，$\\dfrac {a_0}{2k}$ 代表着平衡的位置，当$\\dfrac {a_0}{2}$ 作用于系统上所产生的静变形 周期激励通过傅氏变换被表示成了一系列频率为基频整数倍的简谐激励的叠加，这种对系统响应的分析被成为谐波分析法 由欧拉公式，我们可以将其转化成为复数表示 $ \\sin n \\omega_{1} t=\\dfrac{i}{2}\\left(e^{-i n \\omega_{1} t}-e^{i n \\omega_{1} t}\\right) $, $ \\cos n \\omega_{1} t=\\dfrac{i}{2}\\left(e^{-i n \\omega_{1} t}+e^{i n \\omega_{1} t}\\right) $ 此时的可以将原来的傅里叶技术式转化为： \\begin{align} F(t)&=\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left(a_{n} \\cos n \\omega_{1} t+b_{n} \\sin n \\omega_{1} t\\right)\\\\ &=\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty} \\frac {a_n-ib_n} 2 e^{i n \\omega_{1} t} + \\sum_{n=1}^{\\infty} \\frac {a_n+ib_n} 2 e^{-i n \\omega_{1} t}\\\\ &=\\sum_{n=1}^{\\infty} F_ne^{i n \\omega_{1} t} \\end{align}其中的对于每一项的系数：(取$\\tau=-\\frac T 2$) \\begin{align} F_n&=\\dfrac {a_n-ib_n} 2= \\frac{1}{T} \\int_{\\tau}^{\\tau+T} F(t) [\\cos n \\omega_{1} t - i\\sin n \\omega_{1}t]\\ d t\\\\ &= \\frac{1}{T} \\int_{\\tau}^{\\tau+T} F(t) e^{- i n \\omega_{1} t} dt = \\frac{1}{T} \\int_{-\\frac T 2}^{\\frac T 2} F(t) e^{- i n \\omega_{1} t} dt \\end{align}此时，在复数表示下的系统动力学方程为： m \\ddot{x}(t)+c \\dot{x}(t)+k x(t)=\\sum_{n=1}^{\\infty} F_ne^{i n \\omega_{1} t}有叠加原理可得，系统的稳态响应为： x(t)=\\sum_\\limits{n=-\\infty}^{\\infty} A_ne^{i(n \\omega_{1} t - \\theta_n)} \\quad \\theta_n=\\operatorname{tg}^{-1} \\dfrac{2 \\xi n{s}}{1-n^2s^{2}}将上式进行求导可得： \\dot x(t)=\\sum_\\limits{n=-\\infty}^{\\infty} iA_n n\\omega_{1} e^{i(n \\omega_{1} t - \\theta_n)} \\quad \\ddot x(t)=-\\sum_\\limits{n=-\\infty}^{\\infty} A_n n^2 \\omega_{1}^2 e^{i(n \\omega_{1} t - \\theta_n)}带入到原运动方程中可得： \\sum_{n=-\\infty}^{\\infty}\\left[-m A_{n} n^{2} \\omega_{1}^{2}+i c A_{n} n \\omega_{1}+k A_{n}\\right] e^{i\\left(n \\omega_{1} t-\\theta_{n}\\right)}=\\sum_{n=-\\infty}^{\\infty} F_{n} e^{i n \\omega_{1} t} 因此有：$\\beta=\\dfrac{1}{\\sqrt{\\left(1-n^2s^{2}\\right)^{2}+(2 \\xi ns)^{2}}} \\quad A_n=\\dfrac 1 k \\beta_n F_n$ 解得系统的响应为： x(t)=\\sum_{n=-\\infty}^{\\infty} \\frac{1}{k \\sqrt{\\left(1-n^{2} s^{2}\\right)^{2}+(2 \\xi n s)^{2}}} F_{n} e^{i\\left(n \\omega_{1} t-\\theta_{n}\\right)}其中$\\beta_n$、$\\theta_n$ 分别为第n次谐波激励所对应的振幅放大因子和相位差。 相关例题 理想方波只有“高”和“低”这两个值。电流或电压的波形为矩形的信号即为矩形波信号。方波在数码开关电路中广泛应用。因为方波可以快速从一个值转至另一个(即0→1或1→0)，所以方波就用作时钟讯号来准确地触发同步电路。如果用频率定义域来表示方波，就会出现一连串的谐波。这可能会产生电磁波和电流脉波，影响周围的电路，产生噪声。 首先是确定傅里叶级数的展开式： 激励的周期：$T=\\dfrac {2\\pi} \\omega=\\dfrac {12\\pi} {\\omega_0}$ , 其中弹簧—质量的固有频率； 于是有：$\\omega_0=\\dfrac {12\\pi} T ,\\quad \\omega_1=\\dfrac {\\omega_0} 6 = \\dfrac {2\\pi} T$ F(t)=\\frac{a_{0}}{2}+\\sum_{n=1}^{\\infty}\\left(a_{n} \\cos n \\omega_{1} t+b_{n} \\sin n \\omega_{1} t\\right)\\\\ \\begin{align} a_{0}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) d t\\\\ a_{n}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) \\cos n \\omega_{1} t d t \\quad n的偶函数\\\\ b_{n}&=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) \\sin n \\omega_{1} t d t \\quad n的奇函数\\\\ \\end{align} 其中，$a_0$为一个周期内信号围成的总面积，得$a_0=0$; 区间 $[0, T]$ 内，$F(t)$ 关于$\\dfrac T 2$为反对称， 而$\\cos n\\omega_1 t$关于$\\dfrac T 2$对称的，因此$a_n=0$ ； 可以那a1来进行举例，很容易可以看出 区间 $\\left[0, \\dfrac T 2 \\right]$ 内，$F(t)$ 关于$\\dfrac T 4$为对称， 而$\\sin n\\omega_1 t$关于 $\\dfrac T 4$ 反对称的，同时区间 $\\left[\\dfrac T 2,T \\right]$ 内，$F(t)$ 关于$\\dfrac {3T} 4$为对称， 而$\\sin n\\omega_1 t$关于 $\\dfrac T 4$ 反对称的，因此$b_n=0,\\ n=2,4,6,\\cdots$ ； 经过上面的推理，原表达式可以化简成： F(t)=\\sum_{n=1}^{\\infty} b_{n} \\sin n \\omega_{1} t, \\\\ b_{n}=\\frac{2}{T} \\int_{\\tau}^{\\tau+T} F(t) \\sin n \\omega_{1} t\\ d t ,\\quad n= 1,3,5,\\cdots于是在n取奇数时， b_{n}=\\frac{2}{T} \\int_{0}^{T} F(t) \\sin n \\omega_{1} t\\ d t = \\frac{8}{T} \\int_{0}^{T/4} F_0 \\sin n \\omega_{1} t\\ d t=\\frac {4 F_0} {n\\pi}\\\\ n= 1,3,5,\\cdots最终求得周期性激励$F(t)$的展开式为： \\begin{aligned} F(t) &=\\sum_{n=1}^{\\infty} b_{n} \\sin n \\omega_{1} t=\\frac{4 F_{0}}{\\pi} \\sum_{n=1,3,5 \\ldots }^{\\infty} \\frac{1}{n} \\sin n \\omega_{1} t \\\\ &=\\frac{4 F_{0}}{\\pi}\\left(\\sin \\omega_{1} t+\\frac{1}{3} \\sin 3 \\omega_{1} t+\\frac{1}{5} \\sin 5 \\omega_{1} t+\\cdots\\right) \\end{aligned} 之后将激励带入到系统的运动方程中去： m \\ddot{x}(t)+c \\dot{x}(t)+k x(t)=F(t)\\\\ \\Rightarrow x=\\frac{4 F_{0}}{\\pi} \\sum_{n=1,3,5 \\ldots }^{\\infty} \\beta_n \\sin (n \\omega_{1} t-\\theta_n)其中，$s=\\dfrac {\\omega_{1}}{\\omega_{0}} \\quad \\beta=\\dfrac{1}{\\sqrt{\\left(1-n^2s^{2}\\right)^{2}+(2 \\xi ns)^{2}}} \\quad \\theta_n=\\operatorname{tg}^{-1} \\dfrac{2 \\xi n{s}}{1-n^2s^{2}} $ 但不计阻尼时： x(t)=\\frac{4 F_{0}}{\\pi k} \\sum_{n=1,3,5 \\ldots}^{\\infty} \\frac{1}{n\\left(1-n^{2} s^{2}\\right)} \\sin n \\omega_{1} t 工程上的周期激励较多，比简谐激励少。 首先，求解傅里叶级数展开的表达式，这里跟上题的方法类似，还可以根据奇偶性进行化简。 为了简化，这里主要取前三项,带入到系统的动力学方程： F(t) \\approx 25000 A-\\frac{2 \\times 10^{5} A}{\\pi^{2}} \\cos \\omega t-\\frac{2 \\times 10^{5} A}{9 \\pi^{2}} \\cos 3 \\omega t \\\\ m \\ddot{x}(t)+c \\dot{x}(t)+k x(t)=F(t)得出阀稳态响应： \\begin{aligned} x_{p}(t)=& \\frac{25000 A}{k}-\\frac{2 \\times 10^{5} A /\\left(k \\pi^{2}\\right)}{\\sqrt{\\left(1-s^{2}\\right)^{2}+(2 \\xi s)^{2}}} \\cos \\left(\\omega t-\\phi_{1}\\right) \\\\ &-\\frac{2 \\times 10^{5} A /\\left(9 k \\pi^{2}\\right)}{\\sqrt{\\left(1-9 s^{2}\\right)^{2}+(6 \\xi s)^{2}}} \\cos \\left(3 \\omega t-\\phi_{3}\\right) \\end{aligned} 非周期激励的响应理论基础 对于脉冲激励情形，系统只有暂态响应而不存在稳态响应； 单位脉冲力可利用狄拉克（Dirac）分布函数δ(t) 表示； δ函数也称为单位脉冲函数，定义为： $\\delta(t-\\tau)$的图象用位于时刻τ、长度为 1 的有向线段表示。 以下引入信号与系统中讲过的 $\\delta$ 函数的定义和性质： 定义 \\delta(t-\\tau)=\\left\\{\\begin{array}{ll}\\infty & (t=\\tau) \\\\ 0 & (t \\neq \\tau)\\end{array} \\qquad \\int_{-\\infty}^{+\\infty} \\delta(t-\\tau) d t=1\\right. \\delta(t-\\tau)=\\lim_\\limits{\\varepsilon \\rightarrow 0} \\delta_{\\varepsilon}(t-\\tau)其中，$\\delta_{\\varepsilon}(t-\\tau)$ 也可以定义为其它形状的面积为 1 的脉冲，其量纲为1/秒，表达式为： \\delta_{\\varepsilon}(t-\\tau)=\\left\\{\\begin{array}{ll}\\dfrac 1 \\varepsilon & (\\taut_{1} \\ (2)\\end{array}\\right.（1）当 $0 \\le t &lt; t_1$ 时，应用上面例题的结果为： \\begin{aligned} x(t) & =\\frac{Q_{0}}{k}\\left(1-\\cos \\omega_{0} t\\right) \\end{aligned}（2）当 $t_1 \\le t &lt; t_2$ 时， 叠加可得： \\begin{align} x(t)&=x_{1}(t)+x_{2}(t)\\\\&=\\frac{Q_{1}}{k}\\left[\\cos \\omega_{0}\\left(t-t_{1}\\right)-\\cos \\omega_{0} t\\right]-\\frac{Q_{2}}{k}\\left[1-\\cos \\omega_{0}\\left(t-t_{1}\\right)\\right] \\end{align}（3）当$t_2&lt;t$ 时， 对于Q1，应用(2)式：$x_1(t)=\\dfrac{Q_{1}}{k}\\left[\\cos \\omega_{0}\\left(t-t_{1}\\right)-\\cos \\omega_{0} t\\right]$ 对于Q2，应用(2)式，注意力的作用时间滞后t1： \\begin{aligned} x_{2}(t) &=-\\frac{Q_{2}}{k}\\left\\{\\cos \\omega_{0}\\left[\\left(t-t_{1}\\right)-\\left(t_{2}-t_{1}\\right)\\right]-\\cos \\omega_{0}\\left(t-t_{1}\\right)\\right\\} \\\\ &=-\\frac{Q_{2}}{k}\\left[\\cos \\omega_{0}\\left(t-t_{2}\\right)-\\cos \\omega_{0}\\left(t-t_{1}\\right)\\right] \\end{aligned}将两式相加： \\begin{align} x(t)&=x_{1}(t)+x_{2}(t)\\\\&=\\frac{Q_{1}}{k}\\left[\\cos \\omega_{0}\\left(t-t_{1}\\right)-\\cos \\omega_{0} t\\right]-\\frac{Q_{2}}{k}\\left[\\cos \\omega_{0}\\left(t-t_{2}\\right)-\\cos \\omega_{0}\\left(t-t_{1}\\right)\\right]\\\\ \\end{align}","link":"/2022/03/09/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%97%E8%BF%AB%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度振动-多自由度系统的动力学方程 第一部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是多自由振动系统中动力学方程的第一部分，主要是介绍作用力方程、刚度矩阵与质量矩阵以及相关的系统，由于多自由度系统较为复杂，因此后续的分节不比较的细致。 建模方法1：将车、人等全部作为一个质量考虑，并考虑弹性和阻尼 优点：模型简单 缺点：模型粗糙，没有考虑人与车、车与车轮、车轮与地面之间的相互影响 建模方法2：车、人的质量分别考虑，并考虑各自的弹性和阻尼 优点：模型较为精确，考虑了人与车之间的耦合 缺点：没有考虑车与车轮、车轮与地面之间的相互影响 建模方法3：车、人、车轮的质量分别考虑，并考虑各自的弹性和阻尼 优点：分别考虑了人与车、车与车轮、车轮与地面之间的相互耦合，模型较为精确 问题：如何描述各个质量之间的相互耦合效应？ 对于航天领域的火箭，近似可以可以看作是刚体，因此频率方程有零根，对于飞船这样比较柔性的可以看作频率方程有重根。 作用力方程平动的案例例1：双质量弹簧系统，两质量分别受到激振力不计摩擦和其他形式的阻尼，试建立系统的运动微分方程 解：坐标原点：取 $m_1、m_2$ 的静平衡位置设某一瞬时：$m_1、m_2$上分别有位移 $x_1 、x_2$ , 加速度$\\ddot x_1、 、\\ddot x_2$ 受力分析： 建立动力学方程：(其主要的量纲为力量纲) \\left\\{\\begin{array}{l}m_{1} \\ddot{x}_{1}(t)+k_{1} x_{1}(t)+k_{2}\\left(x_{1}(t)-x_{2}(t)\\right)=P_{1}(t) \\\\ m_{2} \\ddot{x}_{2}(t)-k_{2}\\left(x_{1}(t)-x_{2}(t)\\right)+k_{3} x_{3}(t)=P_{2}(t)\\end{array}\\right. 将其准化为矩阵形式可得： \\left[\\begin{array}{cc}m_{1} & 0 \\\\ 0 & m_{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}k_{1}+k_{2} & -k_{2} \\\\ -k_{2} & k_{2}+k_{3}\\end{array}\\right]\\left[\\begin{array}{l}x_{1}(t) \\\\ x_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{c}P_{1}(t) \\\\ P_{2}(t)\\end{array}\\right]其中的$-k_2$ 为坐标间的耦合项，如果没有此项就变成了一个单自由度的系统了。 转动的案例例2：转动运动，外力矩为$M_1(t)\\ M_2(t)$ ，其转动惯量为$I_1、I_2$，轴的三段的扭转刚度为 $k_{\\theta_1}、k_{\\theta_2}、k_{\\theta_3}$ ，建立系统的动力学微分方程。 首先，建立坐标，设在某个瞬时的角位移为$\\theta_1\\ \\theta_2$, 对应的角加速度为 $\\ddot\\theta_1\\ \\ddot\\theta_2$ ，之后受力分析可得： 于是可得： \\left\\{\\begin{array}{l}I_{1} \\ddot{\\theta}_{1}(t)+k_{\\theta 1} \\theta_{1}(t)+k_{\\theta 2}\\left[\\theta_{1}(t)-\\theta_{2}(t)\\right]=M_{1}(t) \\\\ I_{2} \\ddot{\\theta}_{2}(t)+k_{\\theta 2}\\left[\\theta_{2}(t)-\\theta_{1}(t)\\right]+k_{3} \\theta_{3}(t)=M_{2}(t)\\end{array}\\right.改写成为矩阵形式可得： \\left[\\begin{array}{cc}I_{1} & 0 \\\\ 0 & I_{2}\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{\\theta}_{1}(t) \\\\ \\ddot{\\theta}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}k_{\\theta 1}+k_{\\theta 2} & -k_{\\theta 2} \\\\ -k_{\\theta 2} & k_{\\theta 2}+k_{\\theta 3}\\end{array}\\right]\\left[\\begin{array}{l}\\theta_{1}(t) \\\\ \\theta_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{c}M_{1}(t) \\\\ M_{2}(t)\\end{array}\\right] 多自由度系统的角振动与直线振动在数学描述上相同 如同在单自由度系统中所定义的，在多自由度系统中也将质量、刚度、位移、加速度及力都理解为广义的 对于倒立摆等结构，其M矩阵为质量和力矩耦合。 基于K矩阵来讲，反对角上的元素是相等的。 于是可以得出作用力的方程： 若系统有n个自由度，则各项的皆为n维矩阵或向量。 一般情况下M、K矩阵是二阶常微分的，如果再复杂一点可能是实变的。 对于连续体来说，其方程式偏微的，可以离散化的方法变为常微的 偏微分很难求解，因此要离散化近似。 刚度矩阵和质量矩阵当我们得到作用力方程时，只要M、K确定后，系统的动力可以完全确定，那么M、K要如何确定？ 刚度矩阵K的确定首先，先确定K，这里要排除外力以准静态方程的方式施加于系统，就是缓慢的加力，此时的加速度为零 $\\ddot X(t)=0$ 带入到作用力方程： 此时的$P(t)$ 为准静态外力列向量 。 假设作用于系统的是这样一组外力：它们使系统只在第 j 个坐标上产生单位的位移，而在其他各个坐标上不产生位移 即： 这里的理论基础就是 $F=kx$ \\boldsymbol{X}(t)=\\left[x_{1}(t), \\ldots, x_{j-1}(t), x_{j}(t), x_{j+1}(t), \\ldots, x_{n}(t)\\right]^{T}=[0, \\ldots, 0,1,0, \\ldots, 0]^{T} 其中得X矩阵只有第 j 项为1，其余都是0 带入得，（只有K矩阵第 j 列的数保留了下来） \\boldsymbol{P}(t)=\\left[\\begin{array}{l}P_{1}(t) \\\\ P_{2}(t) \\\\ \\vdots \\\\ P_{n}(t)\\end{array}\\right]=\\left[\\begin{array}{l}k_{11} \\ldots k_{1 j} \\ldots k_{1 n} \\\\ k_{21} \\ldots k_{2 j} \\ldots k_{2 n} \\\\ \\ldots \\ldots \\ldots \\ldots \\ldots . \\\\ k_{n 1} \\ldots k_{n j} \\ldots k_{n n}\\end{array}\\right]\\left[\\begin{array}{c}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}k_{1 j} \\\\ k_{2 j} \\\\ \\vdots \\\\ k_{n j}\\end{array}\\right] 所施加的这组外力数值上正是刚度矩阵K 的第 j 列; 其中的$k_{ij} $ $(i=1,2,…n)$是在第 i 各坐标上面施加的力。 根据矩阵得性质可得，这组外力是唯一的。 结论：刚度矩阵K 中的元素 $k_{ij}$ 是使系统仅在第j 个坐标上产生单位位移而相应于第 i 个坐标上所需施加的力。 于是就将K矩阵确定，下面讨论M矩阵的确定。 质量矩阵M的确定之后，确定M矩阵，假设系统受到外力作用的瞬时，只产生加速度而不产生任何位移，这样可以使得$X(t)=0$ 假设作用于系统的是这样一组外力：它们使系统只在第 j 个坐标上产生单位加速度，而在其他各个坐标上不产生加速度。 这里的理论基础就是 $F=ma$ \\boldsymbol{P}(t)=\\left[\\begin{array}{l}P_{1}(t) \\\\ P_{2}(t) \\\\ \\vdots \\\\ P_{n}(t)\\end{array}\\right]=\\left[\\begin{array}{l}m_{11} \\ldots m_{1 j} \\ldots m_{1 n} \\\\ m_{21} \\ldots m_{2 j} \\ldots m_{2 n} \\\\ \\ldots \\ldots \\ldots \\ldots \\ldots \\ldots \\\\ m_{n 1} \\ldots m_{n j} \\ldots m_{n n}\\end{array}\\right]\\left[\\begin{array}{c}0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\vdots \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}m_{1 j} \\\\ m_{2 j} \\\\ \\vdots \\\\ m_{n j}\\end{array}\\right]同理，最后就剩下了M矩阵的第 j 列，其含义就是其他坐标施加的外力。 结论：质量矩阵 M 中的元素$m_{ij}$是使系统仅在第 j 个坐标上产生单位加速度而相应于第 i 个坐标上所需施加的力 。 结论 刚度矩阵 K 中的元素 $k_{ij}$ 是使系统仅在第 j 个坐标上产生单位位移而相应于第 i 个坐标上所需施加的力; 质量矩阵 M 中的元素 $m_{ij}$ 是使系统仅在第 j 个坐标上产生单位加速度而相应于第 i 个坐标上所需施加的力; $m_{ij}、k_{ij}$又分别称为质量影响系数和刚度影响系数。根据它们的物理意义可以直接写出系统质量矩阵M和刚度矩阵K，从而建立作用力方程，这种方法称为影响系数方法. 经典例题 先考虑准静态的情况： 令 $X=[1\\quad 0\\quad 0]^T$ ,只使 $m_1$ 产生单位位移，$m_2$ 和 $m_3$ 不动，可以求出K阵的第一列； $k_{11}$ ——使 $m_1$ 产生单位位移所需施加的力：$k_{11}=k_1+k_2$ $k_{21}$ ——保持$m_2$不动所需施加的力：$k_{21}=-k_2$ $k_{31}$ ——保持$m_3$不动所需施加的力：$k_{31}=0$ 同理，令 $X=[0\\quad 1\\quad 0]^T$ ,只使 $m_2$ 产生单位位移，$m_1$ 和 $m_3$ 不动，可以求出K阵的第二列； $k_{22}$ ——使 $m_2$ 产生单位位移所需施加的力：$k_{22}=k_2+k_3+k_5+k_6$ $k_{12}$ ——保持$m_1$不动所需施加的力：$k_{21}=-k_2$ $k_{32}$ ——保持$m_3$不动所需施加的力：$k_{33}= -k_3$ 令 $X=[0\\quad 0\\quad 1]^T$ ,只使 $m_3$ 产生单位位移，$m_1$ 和 $m_2$ 不动，可以求出K阵的第三列； $k_{33}$ ——使 $m_3$ 产生单位位移所需施加的力：$k_{33}=k_3+k_4$ $k_{23}$ ——保持$m_2$不动所需施加的力：$k_{23}= -k_3$ $k_{13}$ ——保持$m_1$不动所需施加的力：$k_{13}= 0$ 只考虑动态的情况： 运动微分方程：此时的M阵是解耦的，K阵是耦合的。 \\left[\\begin{array}{ccc}m_{1} & 0 & 0 \\\\ 0 & m_{2} & 0 \\\\ 0 & 0 & m_{3}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t) \\\\ \\ddot{x}_{3}(t)\\end{array}\\right]+\\left[\\begin{array}{ccc}k_{1}+k_{2} & -k_{2} & 0 \\\\ -k_{2} & k_{2}+k_{3}+k_{5}+k_{6} & -k_{3} \\\\ 0 & -k_{3} & k_{3}+k_{4}\\end{array}\\right]\\left[\\begin{array}{c}x_{1}(t) \\\\ x_{2}(t) \\\\ x_{3}(t)\\end{array}\\right]=\\left[\\begin{array}{c}P_{1}(t) \\\\ P_{2}(t) \\\\ P_{3}(t)\\end{array}\\right]例：双混合摆，这里刚度的质量为$m_1,m_2$ ，质心为$c_1,c_2$ ，绕通过自身的质心的z轴的转动惯量为$I_1,I_2$，求以微小转角$\\theta_1,\\theta_2$为坐标，写出x-y平面内的摆动作用力方程。 问：刚体2的转角是相对于刚体1的、还是相对于参考系的？相对于参考系的 先求质量影响系数 令 $\\ddot \\theta_1=1,\\ddot \\theta_2=0$ ，则此时需要在两杆上施加力矩 $m_{11},m_{21}$ 下摆对A取矩：$m_{21}=m_{2} l h_{2}$ 整体对B取矩：$m_{11}=I_{1}+m_{1} h_{1}^{2}+m_{2} l\\left(l+h_{2}\\right)-m_{21}=I_{1}+m_{1} h_{1}^{2}+m_{2} l^{2}$ 为什么不考虑重力的影响，——因为我们考虑的是瞬时动态，也就是此时的系统还没来得及变形，因此重力在此时可以忽略不记。上面的图只是个示意图。 令 $\\ddot \\theta_1=0,\\ddot \\theta_2=1$ ，则此时需要在两杆上施加力矩 $m_{12},m_{22}$ 下摆对A取矩：$m_{22}=I_{2}+m_{2} h_{2}^{2}$ 整个对B取矩：$m_{12}=I_{2}+m_{2} h_{2}\\left(l+h_{2}\\right)-m_{22}=m_{2} l h_{2}$ 综上可得M矩阵为： 这里M为对阵矩阵，不是对角矩阵。 \\boldsymbol{M}=\\left[\\begin{array}{cc}I_{1}+m_{1} h_{1}^{2}+m_{2} l^{2} & m_{2} l h_{2} \\\\ m_{2} l h_{2} & I_{2}+m_{2} h_{2}^{2}\\end{array}\\right] 求刚度影响系数 由于恢复力是重力，所以实际上是求重力影响系数 令 $\\theta_1=1,\\theta_2=0$ ，则此时需要在两杆上施加力矩 $k_{11},k_{21}$ 下摆对A取矩：$k_{21}=0$ 此时为准静态，重力此时还没有产生恢复力矩 整体对B取矩：$k_{11}=m_{1} g h_{1}+m_{2} g l$ 令 $\\theta_1=0,\\theta_2=1$ ，则此时需要在两杆上施加力矩 $k_{12},k_{22}$ 下摆对A取矩：$k_{22}=m_2gh_2 $ 整体对B取矩：$k_{21}=m_{2} g h_{2}-k_{22}=0$ 综上，刚度矩阵为： \\boldsymbol{K}=\\left[\\begin{array}{cc}\\left(m_{1} h_{1}+m_{2} l\\right) g & 0 \\\\ 0 & m_{2} g h_{2}\\end{array}\\right] 得到运动方程为：(此时的弹性不存在耦合，惯性是耦合的) \\left[\\begin{array}{cc}I_{1}+m_{1} h_{1}^{2}+m_{2} l^{2} & m_{2} l h_{2} \\\\ m_{2} l h_{2} & I_{2}+m_{2} h_{2}^{2}\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{\\theta}_{1}(t) \\\\ \\ddot{\\theta}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}\\left(m_{1} h_{1}+m_{2} l\\right) g & 0 \\\\ 0 & m_{2} g h_{2}\\end{array}\\right]\\left[\\begin{array}{c}\\theta_{1}(t) \\\\ \\theta_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right] 例：双杆系统，每杆质量m，杆长度l，水平弹簧刚度k，弹簧距离固定端a，求以微小转角$\\theta_1,\\theta_2$为坐标，写出x-y平面内的摆动作用力方程 先求刚度影响系数 这里，由于是微小振动，因此$\\sin \\theta \\approx\\theta$ 得出刚度影响矩阵为： \\boldsymbol{K}=\\left[\\begin{array}{cc}\\dfrac{1}{2} m g l+k a^{2} & -k a^{2} \\\\ -k a^{2} & \\dfrac{1}{2} m g l+k a^{2}\\end{array}\\right]后求惯性影响系数 运动的微分方程为 \\left[\\begin{array}{cc}\\dfrac{1}{3} m l^{2} & 0 \\\\ 0 & \\dfrac{1}{3} m l^{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{\\theta}_{1}(t) \\\\ \\ddot{\\theta}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}\\dfrac{1}{2} m g l+k a^{2} & -k a^{2} \\\\ -k a^{2} & \\dfrac{1}{2} m g l+k a^{2}\\end{array}\\right]\\left[\\begin{array}{c}\\theta_{1}(t) \\\\ \\theta_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]例：两自由度系统，摆长l，无质量，微摆动，求运动微分方程 首先求刚度影响系数 系统刚度矩阵：$\\boldsymbol{K}=\\left[\\begin{array}{cc}k_{1}+k_{2} &amp; 0 \\\\ 0 &amp; m_{2} g l\\end{array}\\right]$ 之后求惯性影响系数 系统水平方向力平衡：$m_{11}=\\left(m_{1}+m_{2}\\right) \\times \\ddot{x}=m_{1}+m_{2}$ 杆对A点力矩平衡：$m_{21}=\\left(m_{2} \\ddot{x}\\right) \\times l=m_{2} l$ 令 $\\ddot{x}=0 \\quad \\ddot{\\theta}=1$ 此时瞬时状态分析可得：$\\ddot{\\vec{x}}_{B}=\\ddot{\\vec{x}}_{A}+\\ddot{\\vec{\\theta}} l+\\vec{\\omega}^{2} l$ 这里的$\\ddot{\\vec{x}}_{A}、\\vec{\\omega}^{2} l$ 都为0。 水平方向力平衡：$m_{12}=m_2l$ 杆A点力矩平衡：$m_22=m_2l^2$ 质量矩阵：$\\boldsymbol{M}=\\left[\\begin{array}{cc}m_{1}+m_{2} &amp; m_{2} l \\\\ m_{2} l &amp; m_{2} l^{2}\\end{array}\\right]$ 运动方程为： \\left[\\begin{array}{cc}m_{1}+m_{2} & m_{2} l \\\\ m_{2} l & m_{2} l^{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}(t) \\\\ \\ddot{\\theta}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}k_{1}+k_{2} & 0 \\\\ 0 & m_{2} g l\\end{array}\\right]\\left[\\begin{array}{l}x(t) \\\\ \\theta(t)\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]","link":"/2022/03/09/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E6%8C%AF%E5%8A%A8-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度振动-多自由度系统的动力学方程 第二部分","text":"hljs.initHighlightingOnLoad(); 本文主要讲的是多自由振动系统中动力学方程的第二部分，主要介绍位移方程和柔度矩阵、质量矩阵与刚度矩阵的正定性质以及耦合与坐标变换。 位移方程和柔度矩阵基本理论 对于静定结构，有时通过柔度矩阵建立位移方程比通过刚度矩阵建立作用力方程来得更方便些 柔度定义为弹性体在单位力作用下产生的变形 这里和刚度是相反的，刚度的定义是单位变形下所需要的力的大小，而柔度是单位作用力/力矩下所产生的变形 物理意义及量纲与刚度恰好相反 集中质量——梁只有刚度，没有质量 求柔度矩阵 假设 $P_1、P_2$ 是常力 以准静态方式作用在梁上梁只产生位移(即挠度)，不产生加速度 取质量 $m_1 、m_2$ 的静平衡位置为坐标 $x_1、x_2$ 的原点 具体的计算可以参考材料力学的手册。 改写成矩阵形式可得： \\boldsymbol{X}=\\boldsymbol{F}\\boldsymbol{P}\\\\ \\boldsymbol{X}=\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right] \\quad \\boldsymbol{F}=\\left[\\begin{array}{ll}f_{11} & f_{12} \\\\ f_{21} & f_{22}\\end{array}\\right] \\quad \\boldsymbol{P}=\\left[\\begin{array}{l}P_{1} \\\\ P_{2}\\end{array}\\right] 其中F被称为是柔度矩阵，和显然其就是K矩阵的倒数。 综上，等到柔度的物理意义是： 系统仅在第 j 个坐标受到单位力作用时，相应于第 i 个坐标上产生的位移 求刚度矩阵 当 $P_1$、$P_2$ 是动载荷时集中质量上有惯性力存在，这里通过达朗贝尔原理将其转化为惯性力施加在对于的质量上面。 考虑惯性力之后的位移方程： {\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right]=\\left[\\begin{array}{ll}f_{11} & f_{12} \\\\ f_{21} & f_{22}\\end{array}\\right]\\left[\\begin{array}{c}P_{1}(t)-m_{1} \\ddot{x}_{1} \\\\ P_{2}(t)-m_{2} \\ddot{x}_{2}\\end{array}\\right] } 将上述方程分解可得： {\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right]=\\left[\\begin{array}{ll}f_{11} & f_{12} \\\\ f_{21} & f_{22}\\end{array}\\right]\\left(\\left[\\begin{array}{l}P_{1}(t) \\\\ P_{2}(t)\\end{array}\\right]-\\left[\\begin{array}{cc}m_{1} & 0 \\\\ 0 & m_{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1} \\\\ \\ddot{x}_{2}\\end{array}\\right]\\right) } 得出最后的位移方程为： \\boldsymbol{X}=\\boldsymbol{F}(\\boldsymbol{P}-\\boldsymbol{M} \\ddot{\\boldsymbol{X}}) 作用力方程——左右两边都是力的量纲； 位移方程——左右两边都是位移的量纲。 要注意的问题： 对于允许刚体运动产生的系统(即具有刚体自由度的系统)，柔度矩阵不存在。即位移方程不适用于具有刚体自由度的系统。 即系统在特定的作用下有位移的平衡位置，整个系统和前面有一点的连接，但是如图所示，我们的施加力以后系统直接运动，其平衡位置相当于有无数个。 如左边的系统，其存在一个模态，弹簧是上面没有力，三个以系统的速度相同的加速度以同一个方向移动过去。 原因：在任意一个坐标上施加单位力，系统将产生刚体运动而无法计算各个坐标上的位移，此时系统的刚度矩阵K是奇异的。 典型例题例： 求图示两自由度简支梁横向振动的位移方程梁不计质量，抗弯刚度$EJ$ 由材料力学知，当B点作用有单位力时，A点的挠度为： f_{AB}=\\frac {ab} {6EJl} (l^2-a^2-b^2) 柔度影响系数为：$f_{11}=f_{22}=8f \\quad f_{21}=f_{12}=7f \\quad f=\\dfrac {l^3} {486EJ}$ 得出位移方程为： {\\left[\\begin{array}{l}x_{1} \\\\ x_{2}\\end{array}\\right]=\\left[\\begin{array}{ll}8f & 7f \\\\ 7f & 8f\\end{array}\\right]\\left(\\left[\\begin{array}{l}P_{1} \\\\ P_{2}\\end{array}\\right]-\\left[\\begin{array}{cc}m_{1} & 0 \\\\ 0 & m_{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1} \\\\ \\ddot{x}_{2}\\end{array}\\right]\\right) } 在坐标 $x_1$ 上对质量 $m_1$ 作用单位力系统在坐标$x_1$、$x_2$、$x_3$ 上产生位移: $f_{11}=f_{21}=f_{31}=\\dfrac 1 {k_1}$ 在坐标 $x_2$上对质量 $m_2$ 作用单位力 $ f_{12}=\\dfrac{1}{k_{1}} \\quad f_{22}=\\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} \\quad f_{32}=\\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} $ 在坐标 $x_3$上对质量 $m_3$ 作用单位力 $ f_{13}=\\dfrac{1}{k_{1}} \\quad f_{23}=\\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} \\quad f_{33}=\\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}}+\\dfrac{1}{k_{3}} $ 这里和之前求刚度矩阵有不同，之前求刚度和惯性矩阵时，只是让某个这里发生位移而其他的部分样施加相应的力来保证位移是0. 于是得出柔度矩阵为： F=\\left[\\begin{array}{ccc}\\dfrac{1}{k_{1}} & \\dfrac{1}{k_{1}} & \\dfrac{1}{k_{1}} \\\\ \\dfrac{1}{k_{1}} & \\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} & \\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} \\\\ \\dfrac{1}{k_{1}} & \\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}} & \\dfrac{1}{k_{1}}+\\dfrac{1}{k_{2}}+\\dfrac{1}{k_{3}}\\end{array}\\right] \\quad \\begin{array}{c}\\text { 可以验证，有 : } \\\\ \\boldsymbol{F}\\boldsymbol{K}=\\boldsymbol{I}\\end{array} 小结： 反命题，如果K阵为奇异的，此时一定为刚体运动。$\\omega^2=k/m$ 当K为0时一定有$\\omega=0$，系统不振动。 质量矩阵和刚度矩阵的正定性质n 阶方阵 A 正定，即 $A_{n\\times n}&gt;0$ , 对于任意的 n 维列向量 y，总有 $y^TA\\ y&gt;0$ (标量) 成立 并且等号仅在 $y=0$ 时才成立 如果 $y \\ne 0$ 时，等号也成立，那么称矩阵A是半正定的 $A \\ge0 $ 对于动能，动能始终是大于等于0的。 对于势能，振动系统的刚度矩阵至少为半正定。 理解：$\\omega^2=k/m$ 根据公式可得，对于一般的系统来说，k都是大于0的；对于随遇平衡系统，其K=0，因此可以为半正定。 振动问题中主要讨论（1）M阵正定、K 阵正定 $\\Leftrightarrow$ 正定振动系统（2）M阵正定、K 阵半正定 $\\Leftrightarrow$ 半正定振动系统 耦合与坐标变换基本理论 不出现惯性耦合时，一个坐标上产生的加速度只在该坐标上引起惯性力。 \\left[\\begin{array}{cc}m_{11} & 0 \\\\ 0 & m_{22}\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{1} \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{c}m_{11} \\ddot{x}_{1} \\\\ 0\\end{array}\\right] 出现惯性耦合时，一个坐标上产生的加速度还会在别的坐标上引起惯性力。 \\left[\\begin{array}{cc}m_{11} & m_{12} \\\\ m_{21} & m_{22}\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{1} \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{c}m_{11} \\ddot{x}_{1} \\\\ m_{21} \\ddot{x}_{1}\\end{array}\\right]同理，不出现弹性耦合时，一个坐标上产生的位移只在该坐标上引起弹性恢复力；而出现弹性耦合时，一个坐标上产生的位移还会在别的坐标上引起弹性恢复力。 耦合的表现形式取决于坐标的选择 选取D点的垂直位移$x_D$和绕D点的角位移 $\\theta_D$ 为坐标，写出车体微振动的微分方程。 解法一：(理论力学的方法） 由于杆是刚体，因此其上面的点上的角速度是处处相等的 $\\theta_C = \\theta_D$ 这里涉及到刚体上力如果从一个点平移到另一个点会产生一个伴随力偶来确保整个系统的力矩平衡。 这里的广义坐标的取得是D点。 同理对于势能来说有： \\begin{aligned} V= \\frac{1}{2} k_{1}\\left(x_{D}-a_{1} \\theta_{D}\\right)^{2} +\\frac{1}{2} k_{2}\\left(x_{D}+a_{2} \\theta_{D}\\right)^{2} \\end{aligned} 将动能和势能带入拉格朗日方程可得： m \\ddot{x}_{D}+m e \\ddot{\\theta}_{D}+\\left(k_{1}+k_{2}\\right) x_{D}+\\left(k_{2} a_{2}-k_{1} a_{1}\\right) \\theta_{D}=P_{D}\\\\m e \\ddot{x}_{D}+\\left(I_{C}+m e^{2}\\right) \\ddot{\\theta}_{D}+\\left(k_{1}+k_{2}\\right) x_{D}+\\left(k_{1} a_{1}^{2}+k_{2} a_{2}^{2}\\right) \\theta_{D}=M_{D}写成矩阵形式可得： 使用D点为广义坐标，求解出来的振动方程，质量和刚度矩阵同时耦合，下面我们使用影响系数法来求解。 解法二：（振动力学的方法） 首先求解刚度矩阵 之后求解质量矩阵 这里的C点是质心，瞬间的惯性的就作用在C上面，由于没有角加速度，因此只有惯性力而没有力偶。 为了保证力的平衡，要在D处相应的施加一个等大的力以及一个防止出现角速度的力偶。 产生达朗贝尔惯性力偶的同时也会产生对应的惯性力。 得出微分方程为： \\left[\\begin{array}{cc}m & m e \\\\ m e & I_{C}+m e^{2}\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{D} \\\\ \\ddot{\\theta}_{D}\\end{array}\\right]+\\left[\\begin{array}{cc}k_{1}+k_{2} & k_{2} a_{2}-k_{1} a_{1} \\\\ k_{2} a_{2}-k_{1} a_{1} & k_{1} a_{1}^{2}+k_{2} a_{2}^{2}\\end{array}\\right]\\left[\\begin{array}{c}x_{D} \\\\ \\theta_{D}\\end{array}\\right]=\\left[\\begin{array}{c}P_{D} \\\\ M_{D}\\end{array}\\right]解耦分析： 将弹性耦合进行解耦可得，此时右端项为所有的外力向D点简化的结果： 将惯性耦合进行解耦可得，此时右端项为所有的外力向C点简化的结果： 问：能否找到这样一种坐标使得系统的运动微分方程既不出现惯性耦合，也不出现弹性耦合？ 使系统运动微分方程的全部耦合项全部解耦的坐标称为主坐标,，此内容为下次课程模态这边的重点。 不同广义坐标下方程的联系讨论：能对同一个系统选取两个不同的坐标，它们所描述的运动微分方程之间有着怎样的联系？ 首先，寻找D点和C点之间的联系： 可以得到：$\\left[\\begin{array}{l}x_{D} \\\\ \\theta_{D}\\end{array}\\right]=\\left[\\begin{array}{cc}1 &amp; -e \\\\ 0 &amp; 1\\end{array}\\right]\\left[\\begin{array}{l}x_{C} \\\\ \\theta_{C}\\end{array}\\right]$ 力在移动以后会产生附加力偶 $-eP_D$ T为非奇异的，因此：$F_D=(T^T)^{-1}F_C$ 总结 模态叠加：将物理空间的耦合的问题可以通过某个转换的关系来，转化到另一个解耦的空间进而实现求解两个单自由度的系统，之后再转换回到原来的物理空间。 广义坐标的选择不同，不影响刚度和惯性矩阵的性质，如正定性和对称性。","link":"/2022/03/10/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E6%8C%AF%E5%8A%A8-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%96%B9%E7%A8%8B-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度系统振动 多自由度系统的自由振动 第一部分","text":"hljs.initHighlightingOnLoad(); 本文主要讲的是多自由振动系统中的多自由度系统的自由振动，主要介绍了固有频率的计算以及模态(主振型)的计算方法等。 固有频率 在考虑系统的固有振动时，最感兴趣的是系统的同步振动，即系统在各个坐标上除了运动幅值不相同外，随时间变化的规律都相同的运动 同步振动：系统在各个坐标上除了运动幅值不相同外，随时间变化的规律都相同的运动。 第一阶模态的任意时间及距离平衡位置的距离成满足一定的比例 一般来说，实际的振动时这三种主振动的线性叠加 注意：这里的常值列阵$\\Phi$ 代表运动的形状，$f(t)$ 为时间的函数。 随遇平衡的系统，只能建立半正定系统，此时可能没有平衡位置。 这里M的一定时大于0的，正定；但是K阵是不一定的，因为对于随遇平衡位置来说肯定是K阵的行列式为0，此时频率为0. (1) 正定系统 $\\omega&gt;0$ 只可能出现形如 $X(t)=\\Phi a\\sin(\\omega t+\\varphi)$ 的同步运动 系统在各个坐标上都是按相同频率及初相位作简谐振动 (2) 半正定系统 $\\omega \\ge 0$ 可能出现形如 $X(t)=\\Phi a\\sin(\\omega t+\\varphi)$ 的同步运动 也可能出现形如 $X(t)=\\Phi (a t+ b)$ 的同步运动（不发生弹性变形 ） 正定系统的主振动 \\left|\\begin{array}{cccc}k_{11}-\\omega^{2} m_{11} & k_{12}-\\omega^{2} m_{12} & \\cdots & k_{1 n}-\\omega^{2} m_{1 n} \\\\ k_{21}-\\omega^{2} m_{21} & k_{22}-\\omega^{2} m_{22} & \\cdots & k_{2 n}-\\omega^{2} m_{2 n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ k_{n 1}-\\omega^{2} m_{n 1} & k_{n 2}-\\omega^{2} m_{n 2} & \\cdots & k_{n n}-\\omega^{2} m_{n n}\\end{array}\\right|=0 这里特征根对于的是系统的固有频率，特征向量对于的是模态。 将特征方程化简可得： 结构建立起来之后固有频率就确定了，和是否振动时无关的。 采用位移方程求解固有频率 由此可得特征方程为：$|FM-\\lambda I|=0$ 特征根按照降序排列：$\\lambda_1\\ge\\lambda_2\\ge\\cdots\\ge\\lambda_n&gt;0 \\Rightarrow \\lambda_i=\\dfrac 1 {\\omega_i^2}$ 注意，这里一定要按照升序进行排列。 $\\omega_{1}=\\sqrt{k / m} \\quad \\omega_{2}=1.732 \\sqrt{k / m} \\quad \\omega_{3}=2 \\sqrt{k / m}$ 多自由度系统的模态(主振型)求解特征方程的特征根问题主要求解的是特征根，而求解特征向量就是求解模态(主振型)的过程。 间接法求模态 与特征根与特征方程类似，我们的这里的固有频率和模态也是一一对应的关系。 $\\omega_{i} 、 \\boldsymbol{\\phi}^{(i)}$ 代入到特征方程中可得：$\\left(\\boldsymbol{K}-\\omega_{i}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}^{(i)}=\\boldsymbol{0}$ 第 i 阶模态特征值问题 当 $\\omega_i$ 不是特征多项式重根时(即此时没有重根)，上式 n 个方程只有一个不独立 。 设最后一个方程不独立，把最后一行划去，并且把含有 $\\phi^{(i)} $ 的某个元素（例如$\\phi^{(i)}_n $）的项全部移到等号右端，即将最后一列元素放到等号的也右边，此时左边为一个$(n-1) \\times (n-1)$ 的矩阵。 为了简化计算，可以将$\\phi^{(i)}_n=1 $， $\\boldsymbol{\\phi}^{(i)}=\\left[\\begin{array}{lllll}\\phi_{1}^{(i)} &amp; \\phi_{2}^{(i)} &amp; \\cdots &amp; \\phi_{n-1}^{(i)} &amp; 1\\end{array}\\right]^{T}$ \\left[\\begin{array}{ccc}3-\\alpha & -1 & 0 \\\\ -1 & 2-\\alpha & -1 \\\\ 0 & -1 & 3-\\alpha\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2} \\\\ \\phi_{3}\\end{array}\\right]=0 \\quad \\alpha_{1}=1 \\quad \\alpha_{2}=3 \\quad \\alpha_{3}=4以 $\\alpha=1$ 为例进行说明，将其带入到方程中可得： $\\phi^{(i)}_n$的取值也可以取任意非零的常数$\\alpha_i$ 将其求解可得：$a_i\\phi^{(i)}$ (也可以是特征向量) 在特征向量中规定某个元素的值以确定其他各元素的值的过程称为归一化处理。 这里要注意，归一化以后，前面的方程不能为0 系统在各个坐标上都将以第 i 阶固有频率 $\\omega_i$ 做简谐振动，并且同时通过静平衡位置。 \\left[\\begin{array}{c}x_{1}^{(i)}(t) \\\\ x_{2}^{(i)}(t) \\\\ \\vdots \\\\ x_{n}^{(i)}(t)\\end{array}\\right]=\\left[\\begin{array}{c}\\phi_{1}^{(i)} \\\\ \\phi_{2}^{(i)} \\\\ \\vdots \\\\ \\phi_{n}^{(i)}\\end{array}\\right] a_{i} \\sin \\left(\\omega_{i} t+\\varphi_{i}\\right) \\Rightarrow\\\\ \\frac{x_{1}^{(i)}(t)}{\\phi_{1}^{(i)}}=\\frac{x_{2}^{(i)}(t)}{\\phi_{2}^{(i)}}=\\ldots \\ldots=\\frac{x_{n}^{(i)}(t)}{\\phi_{n}^{(i)}}=a_{i} \\sin \\left(\\omega_{i} t+\\varphi_{i}\\right) 第 i 阶特征向量$\\phi^{(i)}$中的一列元素，就是系统做第 i 阶主振动时各个坐标上位移（或振幅）的相对比值。 $\\phi^{(i)}$ 描述了系统做第 i 阶主振动时具有的振动形态，称为第 i 阶主振型，或是振型。 虽然各坐标上振幅的精确值并没有确定，但是所表现的系统振动形态已确定。 主振动仅取决于系统的M 阵、K 阵等物理参数，这一重要概念是单自由度系统所没有的。 将模态叠加可得：(n个模态的叠加，模态叠加法) \\begin{align} \\boldsymbol{X}(t)&=\\boldsymbol{\\phi}^{(1)} a_{1} \\sin \\left(\\omega_{1} t+\\varphi_{1}\\right)+\\boldsymbol{\\phi}^{(2)} a_{2} \\sin \\left(\\omega_{2} t+\\varphi_{2}\\right)+\\ldots+\\boldsymbol{\\phi}^{(n)} a_{n} \\sin \\left(\\omega_{n} t+\\varphi_{n}\\right)\\\\ &=\\sum_{i=1}^{n} \\boldsymbol{\\phi}^{(i)} a_{i} \\sin \\left(\\omega_{i} t+\\varphi_{i}\\right) \\qquad a_{i}, \\varphi_{i}(i=1 \\sim n): 初始条件决$ \\end{align} 由于各个主振动的固有频率不相同，多自由度系统的固有振动一般不是简谐振动，甚至不是周期振动 直接法求模态 \\left(\\boldsymbol{K}-\\omega_{i}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}^{(i)}=\\boldsymbol{0} \\quad \\boldsymbol{\\phi}^{(i)}=\\left[\\begin{array}{lll}\\phi_{1}^{(i)} & \\cdots & \\phi_{n}^{(i)}\\end{array}\\right]^{T} 注意这种方法只限于求解无重根情况下的，即特征矩阵的秩为矩阵的维数，或特征多项式的行列式为0。但是，其实在手算的情况下，伴随矩阵不是很好求的，一般还是用间接法。 例：两自由度弹簧－质量系统，求：固有频率和主振型 动力学方程: \\left[\\begin{array}{cc}m & 0 \\\\ 0 & 2 m\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}2 k & -k \\\\ -k & 3 k\\end{array}\\right]\\left[\\begin{array}{c}x_{1}(t) \\\\ x_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right]令主振动为： \\left[\\begin{array}{l}x_{1}(t) \\\\ x_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2}\\end{array}\\right] \\sin (\\omega t+\\varphi) \\\\ \\left(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0}最后解得： \\left[\\begin{array}{cc}2 k-m \\omega^{2} & -k \\\\ -k & 3 k-2 m \\omega^{2}\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right] 特征方程为： \\left|\\begin{array}{cc}2-\\alpha & -1 \\\\ -1 & 3-2 \\alpha\\end{array}\\right|=2 \\alpha^{2}-7 \\alpha+5=0 \\quad \\Rightarrow \\quad \\alpha_{1}=1, \\quad \\alpha_{2}=2.5 分别带入特征根，即特征频率，求解模态： 画图：横坐标表示静平衡位置，纵坐标表示主振型中各元素的值 两个质量以$\\omega_1$ 为振动频率，同时经过各自的平衡位置，方向相同，而且每一时刻的位移量都相同。 两个质量以 $ \\omega_2$为振动频率，同时经过各自的平衡位置，方向相反，每一时刻第一个质量的位移都第二个质量的位移的两倍。 如果传感器放在节点位置，则测量的信号中将不包含有第二阶模态的信息。 例：三自由度弹簧－质量系统，求：固有频率和主振型 动力学方程： \\left[\\begin{array}{ccc}m & 0 & 0 \\\\ 0 & m & 0 \\\\ 0 & 0 & m\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1} \\\\ \\ddot{x}_{2} \\\\ \\ddot{x}_{3}\\end{array}\\right]+\\left[\\begin{array}{ccc}3 k & -k & 0 \\\\ -k & 2 k & -k \\\\ 0 & -k & 3 k\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right] 此时设主振动力： \\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2} \\\\ \\phi_{3}\\end{array}\\right] \\sin (\\omega t+\\varphi) \\quad or \\quad\\left(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0} \\left[\\begin{array}{ccc}3 k-m \\omega^{2} & -k & 0 \\\\-k & 2 k-m \\omega^{2} & -k \\\\0 & -1 & 3 k-m \\omega^{2}\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{1} \\\\\\phi_{2} \\\\\\phi_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\0 \\\\0\\end{array}\\right] 或者，我们求解特征矩阵的伴随矩阵可得： adj \\left[\\begin{array}{ccc}3-\\alpha & -1 & 0 \\\\ -1 & 2-\\alpha & -1 \\\\ 0 & -1 & 3-\\alpha\\end{array}\\right]=\\left[\\begin{array}{ccc}(3-\\alpha)(2-\\alpha)-1 & 3-\\alpha & 1 \\\\ 3-\\alpha & (3-\\alpha)^{2} & 3-\\alpha \\\\ 1 & 3-\\alpha & (3-\\alpha)(2-\\alpha)-1\\end{array}\\right]之后，将$\\alpha_{1}=1, \\quad \\alpha_{2}=3, \\quad \\alpha_{3}=4$ 分别代入到方程中： \\phi^{(1)}=\\left[\\begin{array}{l}1 \\\\ 2 \\\\ 1\\end{array}\\right], \\quad \\phi^{(2)}=\\left[\\begin{array}{c}-1 \\\\ 0 \\\\ 1\\end{array}\\right], \\quad \\phi^{(3)}=\\left[\\begin{array}{c} 1\\\\ -1 \\\\ 1\\end{array}\\right] 第一节振型没有节点，第二阶振型有1 个节点，第三阶振型有2 个节点，这由主振型内元素符号变号的次数可以判断出 要设计传感器的位置，使得我们的测量的信息尽可能的全。 小结：","link":"/2022/03/13/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%87%AA%E7%94%B1%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度系统振动 多自由度系统的自由振动 第二部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是多自由振动系统中多自由度系统的自由振动部分，主要介绍模态的正交性、主质量和主刚度、模态叠加法的使用以及模态截断法的应用。 模态的正交性，主质量和主刚度 K \\phi^{(i)}=\\omega_{i}^{2} M \\phi^{(i)} \\Rightarrow \\phi^{(i)^{T}} K \\phi^{(j)}=\\omega_{i}^{2} \\phi^{(i)^{T}} M \\phi^{(j)} \\quad 标量 \\\\ K \\phi^{(j)}=\\omega_{i}^{2} M \\phi^{(j)} \\Rightarrow \\phi^{(i)^{T}} K \\phi^{(j)}=\\omega_{j}^{2} \\phi^{(i)^{T}} M \\phi^{(j)} \\quad 标量 \\\\ 当 $i \\ne j$ 时，$\\omega_i \\ne \\omega_j$ ,此时 $\\phi^{(i)^{T}} M \\phi^{(j)}=0 $ 即有质量的正交性； 同理带入原式中可得：$\\phi^{(i)^{T}} K \\phi^{(j)}=0 $ 此时刚度具有正交性。 当 $i=j$ 时， $\\phi^{(i)^{T}} M \\phi^{(j)}=m_{pi}$ 第 i 阶模态的主质量 $\\phi^{(i)^{T}} K \\phi^{(j)}=k_{pi} $ 第 i 阶模态的主刚度 第 i 阶固有频率：$\\omega_i=\\dfrac {k_{pi}} {m_{pi}} \\quad (i=1,2\\cdots n)$ 这里给模态乘以一个系数，让主质量变为1，主刚度也变成了固有频率的平方。 利用模态之间的正交性可以实现质量矩阵和刚度矩阵实现解耦。 对于主振型将 $ \\boldsymbol{\\phi}^{(i)}(i=1 \\sim n) $ 组成矩阵 $ \\boldsymbol{\\Phi}=\\left[\\begin{array}{lll}\\boldsymbol{\\phi}^{(1)} &amp; \\cdots &amp; \\boldsymbol{\\phi}^{(n)}\\end{array}\\right] \\in R^{n \\times n} $振型矩阵 \\left\\{\\begin{array}{ll}\\boldsymbol{\\Phi}^{T} \\boldsymbol{M} \\boldsymbol{\\Phi}=\\operatorname{diag}\\left(m_{p 1}, \\cdots, m_{p n}\\right)=\\boldsymbol{M}_{p} & \\text { 主质量矩阵 } \\\\ \\boldsymbol{\\Phi}^{T} \\boldsymbol{K} \\boldsymbol{\\Phi}=\\operatorname{diag}\\left(k_{p 1}, \\cdots, k_{p n}\\right)=\\boldsymbol{K}_{p} & \\text { 主刚度矩阵 }\\end{array}\\right. \\begin{align} \\boldsymbol{\\Phi}^{T} \\boldsymbol{M} \\boldsymbol{\\Phi}&=\\left[\\begin{array}{llll}\\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(n)}\\end{array}\\right]^{T} \\boldsymbol{M}\\left[\\begin{array}{lll}\\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(n)}\\end{array}\\right] \\\\ &=\\left[\\begin{array}{c}\\boldsymbol{\\phi}^{(1)^{T}} \\\\ \\vdots \\\\ \\boldsymbol{\\phi}^{(n)^{T}}\\end{array}\\right] \\boldsymbol{M}\\left[\\begin{array}{lll}\\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(n)}\\end{array}\\right] =\\left[\\begin{array}{c}\\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\\\ \\vdots \\\\ \\boldsymbol{\\phi}^{(n)^{T}} \\boldsymbol{M}\\end{array}\\right]\\left[\\begin{array}{lll}\\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(n)}\\end{array}\\right] \\\\ &=\\left[\\begin{array}{ccc}\\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(n)} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\boldsymbol{\\phi}^{(n)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(1)} & \\cdots & \\boldsymbol{\\phi}^{(n)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(n)}\\end{array}\\right]=\\left[\\begin{array}{ccc}m_{p 1} & & 0 \\\\ & \\ddots & \\\\ 0 & & m_{p n}\\end{array}\\right] \\end{align}对正则振型将 $ \\boldsymbol{\\phi}^{(i)}_N(i=1 \\sim n) $ 组成矩阵 $ \\boldsymbol{\\Phi}_N=\\left[\\begin{array}{lll}\\boldsymbol{\\phi}^{(1)}_N &amp; \\cdots &amp; \\boldsymbol{\\phi}^{(n)}_N\\end{array}\\right] \\in R^{n \\times n} $正则振型矩阵 \\left\\{\\begin{array}{ll}\\boldsymbol{\\Phi}^{T}_N \\boldsymbol{M} \\boldsymbol{\\Phi}_N=I & \\text { 单位矩阵 } \\\\ \\boldsymbol{\\Phi}^{T}_N \\boldsymbol{K} \\boldsymbol{\\Phi}_N= \\boldsymbol{\\Lambda}& \\text { 谱矩阵 }\\end{array}\\right.\\qquad \\boldsymbol{\\Lambda}=\\left[\\begin{array}{lll}\\omega_{1}^{2} & & \\\\ & \\ddots & \\\\ & & \\omega_{n}^{2}\\end{array}\\right] 依次取$i=1 \\sim n$ ，得到的 n 个方程，可合写为：$K \\Phi=M \\Phi \\Lambda$ 左乘 $\\boldsymbol{\\Phi}^{T}$ 可得: $\\boldsymbol{K}_{p}=\\boldsymbol{M}_{p} \\boldsymbol{\\Lambda} ,\\quad \\boldsymbol{\\Lambda}=\\boldsymbol{M}_{p}^{-1} \\boldsymbol{K}_{p}$ 不难验证, 有: $\\boldsymbol{\\Phi}_{N}^{T} \\boldsymbol{K} \\boldsymbol{\\Phi}_{N}=\\boldsymbol{\\Lambda} \\quad \\boldsymbol{\\Phi}_{N}^{T} \\boldsymbol{M}\\boldsymbol{\\Phi}_{N}=\\boldsymbol{I}$ 此时的正则振型矩阵为： \\boldsymbol{\\Phi}_{N}=\\left[\\frac{\\phi^{(1)}}{\\sqrt{m_{p 1}}}, \\frac{\\phi^{(2)}}{\\sqrt{m_{p 2}}}, \\frac{\\phi^{(3)}}{\\sqrt{m_{p 3}}}\\right]=\\frac{1}{\\sqrt{6 m}}\\left[\\begin{array}{ccc}1 & -\\sqrt{3} & \\sqrt{2} \\\\ 2 & 0 & -\\sqrt{2} \\\\ 1 & \\sqrt{3} & \\sqrt{2}\\end{array}\\right]\\\\正则振型和主振型之间的关系：$\\boldsymbol{\\phi}_{N}^{(i)}=\\dfrac{1}{\\sqrt{m_{p i}}} \\boldsymbol{\\phi}^{(i)} $ 模态叠加法主要的两种模态坐标两种主要的模态的坐标： 主模态坐标：$X_P=[x_{p1},x_{p2},\\cdots,x_{pn}]^T$； 正则模态坐标：$X_N=[x_{N1},x_{N2},\\cdots,x_{Nn}]^T$ 模态 $\\phi^{(i)} \\ (i=1 \\sim n)$ ，相互正交，表明它们是线性独立的，可用于构成n 维空间的基。 系统的任意 n 维自由振动可唯一地表示为各阶振型的线性组合 \\boldsymbol{X}=\\sum_{i=1}^{n} \\boldsymbol{\\phi}^{(i)} x_{p i} \\quad \\boldsymbol{X} \\in R^{n} \\quad \\boldsymbol{\\phi}^{(i)} \\in R^{n \\times 1} 系统在正则模态的坐标下，响应的表达式为： \\boldsymbol{X}=\\boldsymbol{\\Phi_N}X_N=\\sum_{i=1}^{n} \\boldsymbol{\\phi}^{(i)}_N x_{N i} \\quad \\boldsymbol{X} \\in R^{n} \\quad \\boldsymbol{\\phi}^{(i)} \\in R^{n \\times 1} 小结： 求解无阻尼系统对初始条件的响应求解的基本问题： \\left\\{\\begin{array}{l}\\boldsymbol{M} \\ddot{\\boldsymbol{X}}+\\boldsymbol{K} \\boldsymbol{X}=0 \\quad \\boldsymbol{X} \\in R^{n}\\\\ \\boldsymbol{X}(0)=\\boldsymbol{X}_{0}, \\dot{\\boldsymbol{X}}(0)=\\dot{\\boldsymbol{X}}_{0} \\quad \\boldsymbol{M} 、 \\boldsymbol{K} \\in R^{n \\times n}\\\\ \\end{array}\\right.\\\\ \\boldsymbol{X}_{0}=\\left[x_{1}(0), x_{2}(0), \\cdots, x_{n}(0)\\right]^{T} \\quad \\dot{\\boldsymbol{X}}_{0}=\\left[\\dot{x}_{1}(0), \\dot{x}_{2}(0), \\cdots, \\dot{x}_{n}(0)\\right]^{T}可分别采用两类模态坐标求解 主模态坐标求解 此时可以转换为n个单自由度的问题，即将物理空间耦合的问题，转化为模态空间的解耦的问题。 m_{p i} \\ddot{x}_{p i}+k_{p i} x_{p i}=0 \\quad(i=1 \\sim n)\\\\ x_{p i}=x_{p i}(0) \\cos \\omega_{i} t+\\frac{\\dot{x}_{p i}(0)}{\\omega} \\sin \\omega_{i} t \\quad(i=1 \\sim n)在求得 $x_{pi} (i=1,2,… n)$ 后，可利用 $\\boldsymbol{X}=\\boldsymbol{\\Phi}\\boldsymbol{X_P} $ 式求得原系统的解。 正则模态坐标求解 \\ddot{x}_{N i}+\\omega_{i}^{2} x_{N i}=0, \\quad(i=1 \\sim n)\\\\ x_{N i}=x_{N i}(0) \\cos \\omega_{i} t+\\frac{\\dot{x}_{N i}(0)}{\\omega_{i}} \\sin \\omega_{i} t, \\quad(i=1 \\sim n)在求得 $x_{Ni} (i=1,2,… n)$ 后，可利用 $\\boldsymbol{X}=\\boldsymbol{\\Phi}\\boldsymbol{X_N} $ 式求得原系统的解。 例题例：三自由度弹簧－质量系统，求：系统在初始条件下的响应。 首先，求解出动力学方程： \\left[\\begin{array}{ccc}m & 0 & 0 \\\\ 0 & m & 0 \\\\ 0 & 0 & m\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1} \\\\ \\ddot{x}_{2} \\\\ \\ddot{x}_{3}\\end{array}\\right]+\\left[\\begin{array}{ccc}3 k & -k & 0 \\\\ -k & 2 k & -k \\\\ 0 & -k & 3 k\\end{array}\\right]\\left[\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]固有频率：$\\omega_{1}=\\sqrt{k / m}、\\omega_{2}=\\sqrt{3 k / m}、\\omega_{3}=2 \\sqrt{k / m}$ 正则振型的矩阵：$\\boldsymbol{\\Phi}_N=\\dfrac 1 {\\sqrt{6m}}\\left[\\begin{array}{ccc}1 &amp; -\\sqrt 3 &amp; \\sqrt 2 \\\\ 2 &amp; 0 &amp; -\\sqrt 2 \\\\ 1 &amp; \\sqrt 3 &amp; \\sqrt 2\\end{array}\\right] $ 此时，带入模态坐标的初始条件： \\boldsymbol{X}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\boldsymbol{X}_{0}=\\sqrt{m / 6}\\left[\\begin{array}{c}6 \\\\ -2 \\sqrt{3} \\\\ 0\\end{array}\\right] \\quad \\dot{\\boldsymbol{X}}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\dot{\\boldsymbol{X}}_{0}=\\left[\\begin{array}{l}0 \\\\ 0 \\\\ 0\\end{array}\\right]代入公式得模态坐标的响应： \\boldsymbol{X}_{N}=\\left[\\begin{array}{c}x_{N 1} \\\\ {x_{N 2}} \\\\ {x_{N 3}}\\end{array}\\right]=\\sqrt{\\frac{m}{6}}\\left[\\begin{array}{c}6 \\cos \\omega_{1} t \\\\ -2 \\sqrt{3} \\cos \\omega_{2} t \\\\ 0\\end{array}\\right]\\\\x_{N i}=x_{N i}(0) \\cos \\omega_{i} t+\\frac{\\dot{x}_{N i}(0)}{\\omega_{i}} \\sin \\omega_{i} t此时原物理空间的响应为： \\begin{aligned} \\boldsymbol{X}(t) &=\\left[\\begin{array}{l}x_{1}(t) \\\\ x_{2}(t) \\\\ x_{3}(t)\\end{array}\\right]=\\boldsymbol{\\Phi}_{N} \\boldsymbol{X}_{N}=\\frac{1}{\\sqrt{6 m}}\\left[\\begin{array}{ccc}1 & -\\sqrt{3} & \\sqrt{2} \\\\ 2 & 0 & -\\sqrt{2} \\\\ 1 & \\sqrt{3} & \\sqrt{2}\\end{array}\\right] \\times \\sqrt{\\frac{m}{6}}\\left[\\begin{array}{c}6 \\cos \\omega_{1} t \\\\ -2 \\sqrt{3} \\cos \\omega_{2} t \\\\ 0\\end{array}\\right] \\\\ &=\\left[\\begin{array}{l}\\cos \\omega_{1} t+\\cos \\omega_{2} t \\\\ 2 \\cos \\omega_{1} t \\\\ \\cos \\omega_{1} t-\\cos \\omega_{2} t\\end{array}\\right] \\end{aligned} 小结： 这里的叠加其实就是将模态空间的振型分别于对于的转换矩阵相乘后叠加来求解最后在物理空间的振动情况。 模态截断法 对自由度数n 很大的复杂振动系统，不可能求出全部的固有频率和相应的主振型，然后用振型叠加法分析系统对激励的响应 当激励频率主要包含低频成分时，可以撇去高阶振型及固有频率对响应的贡献，而只利用较低的前面若干阶固有频率及主振型近似分析系统响应 受迫振动时，高阶频率的振动对系统的影响非常小，可以忽略。 对于n 自由度系统，将前 r 阶振型$\\Phi^{(r)} (i=1,2,…,r)$ 组成的截断振型矩阵记为 : $\\boldsymbol{\\Phi}^{*}=\\left[\\begin{array}{llll}\\boldsymbol{\\phi}^{(1)} &amp; \\boldsymbol{\\phi}^{(2)} &amp; \\ldots &amp; \\boldsymbol{\\phi}^{(r)}\\end{array}\\right] \\in R^{n \\times r}$ 这里$\\boldsymbol{\\Phi}^{*}$ 要和M相乘，这里M是$n\\times n$的. 截断的主质量矩阵和主刚度矩阵 系统的任意n 阶振动近似地表示为截断后的 r 阶模态的线性组合： \\boldsymbol{X}=\\sum_{i=1}^{r} \\boldsymbol{\\phi}^{(i)} \\boldsymbol{X}_{p i}=\\boldsymbol{\\Phi}^{*} \\boldsymbol{X}_{p}^{*}​ $ \\boldsymbol{X}_{p}^{*} $ 截断后的主坐标列阵 $\\quad \\boldsymbol{X}_{p}^{*}=\\left[\\begin{array}{llll}x_{p 1} &amp; x_{p 2} &amp; \\ldots &amp; x_{p r}\\end{array}\\right]^{T} $ 利用模态截断法可将n 自由度系统原有的n 个坐标变换成较少的前 r 个主坐标","link":"/2022/03/13/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%87%AA%E7%94%B1%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度系统振动 多自由系统的受迫振动","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是多自由振动系统中多自由度系统的受迫振动，主要介绍系统对简谐力激励的响应、动力吸振器和振型叠加法求解任意激励下响应。 系统对简谐力激励的响应回顾单自由度的简谐激励的情况： 多自由度系统受到外力激励所产生的运动为受迫运动 设 n 自由度系统沿各个广义坐标均受到频率和相位相同的广义简谐力的激励 受迫振动方程：$ \\boldsymbol{M} \\ddot{\\boldsymbol{X}}(t)+\\boldsymbol{K} \\boldsymbol{X}(t)=\\boldsymbol{F}_{0} e^{i \\omega t} \\quad \\boldsymbol{X} \\in R^{n} $ 稳态解：$\\boldsymbol{X}(t)=\\overline{\\boldsymbol{X}} e^{i \\omega t} \\quad \\overline{\\boldsymbol{X}} \\in R^{n} $ 振幅列向量 ：$ \\overline{\\boldsymbol{X}}=\\left[\\begin{array}{llll}\\bar{X}_{1} &amp; \\bar{X}_{2} &amp; \\ldots \\ldots &amp; \\bar{X}_{n}\\end{array}\\right]^{T} $ 最后可得： \\left(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right) \\overline{\\boldsymbol{X}}=\\boldsymbol{F}_{0} 此时记$H(\\omega)=\\left[\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right]^{-1}$ 多自由度系统的幅频响应矩阵 则有$\\overline{\\boldsymbol{X}}=HF_0 \\quad X(t)=HF_0e^{i\\omega t}$ 简谐激励下，系统稳态响应也为简谐响应，并且振动频率为外部激励的频率，但是各个自由度上的振幅各不相同。 在实际的工程中：阻抗矩阵$\\left(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right)$ ,导纳矩阵 $H(\\omega)=\\left[\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right]^{-1}$ $H_{ij}$的物理意义为仅沿 j 坐标作用频率为 $\\omega$ 的单位幅度简谐力时，沿 i 坐标所引起的受迫振动的复振幅。 H(\\omega)=\\left[\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right]^{-1}=\\dfrac {adj(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M})} {|\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}|}当外部激励频率接近系统任一阶固有频率时：$|\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}|=0$ ，受迫振动的振幅无限增大，产生共振。 如果系统又n个不同的固有频率，也就又n个共振点。 动力吸振器动力吸振器的基本原理许多机器或部件由于旋转部分的质量偏心而产生强迫振动，为减小这种振动有时可以采用动力吸振器 忽略主系统阻尼 ， 主系统固有频率：$\\omega_1=\\sqrt{\\dfrac {k_1}{m_1}}$ 为了抑制主系统的振动，在主系统上附加一个弹簧-质量系统 系统强迫振动的方程： \\left[\\begin{array}{cc}m_{1} & 0 \\\\ 0 & m_{2}\\end{array}\\right]\\left[\\begin{array}{l}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}c & -c \\\\ -c & c\\end{array}\\right]\\left[\\begin{array}{c}\\dot{x}_{1}(t) \\\\ \\dot{x}_{2}(t)\\end{array}\\right]+\\left[\\begin{array}{cc}k_{1}+k_{2} & -k_{2} \\\\ -k_{2} & k_{2}\\end{array}\\right]\\left[\\begin{array}{l}x_{1}(t) \\\\ x_{2}(t)\\end{array}\\right]=\\left[\\begin{array}{c}F_{0} \\sin \\omega t \\\\ 0\\end{array}\\right]以下为有阻尼的动力吸振器系统： 简化系统，去掉阻尼后的无阻尼吸振器： 对于简谐振动可以使用直接使用公式$\\left(\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right) \\overline{\\boldsymbol{X}}=\\boldsymbol{F}_{0}$ 来求解，但是对于一般的系统就不可以了。 \\left[\\begin{array}{c}\\bar{x}_{1} \\\\ \\bar{x}_{2}\\end{array}\\right]=\\left[\\begin{array}{cc}k_{1}+k_{2}-m_{1} \\omega^{2} & -k_{2} \\\\ -k_{2} & k_{2}-m_{2} \\omega^{2}\\end{array}\\right]^{-1}\\left[\\begin{array}{c}F_{0} \\\\ 0\\end{array}\\right]=\\frac{F_{0}}{\\Delta(\\omega)}\\left[\\begin{array}{c}{k_{2}-m_{2} \\omega^{2}} \\\\{k_{2}} \\end{array}\\right]其中，$\\Delta(\\omega) $: 系统特征多项式 $\\Delta(\\omega)=\\left|\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right| $ \\begin{aligned} \\Delta(\\omega) &=\\left(k_{1}+k_{2}-m_{1} \\omega^{2}\\right)\\left(k_{2}-m_{2} \\omega^{2}\\right)-k_{2}^{2} \\\\ &=m_{1} m_{2} \\omega^{4}-\\left(k_{1} m_{2}+k_{2} m_{1}+k_{2} m_{2}\\right) \\omega^{2}+k_{1} k_{2} \\end{aligned}于是当$\\omega=\\sqrt{\\dfrac {k_2} {m_2}}$ 时，外部激励频率等于吸振器的固有频率，此时与有$\\bar{x}_1=0$ ,主系统不再振动，这种现象称为反共振。 此时，$\\Delta(\\omega)=-k_2^2$ 吸振器的振幅为 $\\bar{x}_2=-\\dfrac {F_0} {k_2}$ 主系统上受到的激振力恰好被来自吸振器的弹性恢复力平衡。 上图为时滞吸尘器，就是将反共振的频带放宽，之后反共振的范围就扩大到更广泛的频率上了，尽量减少由于激励的改变而造成的但不到反共振情况。 在建筑领域，一般在楼的顶层加装弹性小车充当动力吸振器，ATMD、TMD、MTMD(在每一层都加一个阻尼吸振器)，这样可以防震。 若机械在使用动力吸振器之前，在接近共振的情况下工作，即：$\\omega\\approx \\omega_1=\\sqrt{k_1/m_1}$ 因此若在设计吸振器时，使得$\\omega=\\sqrt{k_2/m_2}=\\sqrt{k_1/m_1}$ 。则当机械在它原始的共振频率下运行时，振幅将会为零。 此时，$k_2\\bar{x}_2=-F_0=m_2\\omega^2\\bar{x}_2$ , 其中$k_2，m_2$ 时由$\\bar{x}_2$的允许值决定，一般参数选为： \\frac {k_2} {k_1} = \\frac {m_2} {m_1}=\\mu使吸振器的固有频率和主系统的固有频率相等。 记 $\\omega_0=\\sqrt{k_1/m_1}=\\sqrt{k_2/m_2} \\quad s=\\omega/\\omega_0$ 于是带入到特征表达式可得：$\\Delta(\\omega)=k_1k_2[s^4-(2+\\mu)s^2+1]$ 于是可以推得： \\Delta(\\omega)=m_{1} m_{2} \\omega^{4}-\\left(k_{1} m_{2}+k_{2} m_{1}+k_{2} m_{2}\\right) \\omega^{2}+k_{1} k_{2}\\\\ \\frac{k_{2}}{k_{1}}=\\frac{m_{2}}{m_{1}}=\\mu, \\quad \\omega_{0}=\\sqrt{\\frac{k_{1}} {m_{1}}}=\\sqrt{\\frac{k_{2}} {m_{2}}}, \\quad s=\\frac{\\omega}{\\omega_0}\\\\ \\Rightarrow \\Delta(\\omega)=k_{1} k_{2}\\left[s^{4}-(2+\\mu) s^{2}+1\\right]详细的推导： \\Delta(\\omega)=k_{1} k_{2}\\left[\\frac{m_{1} m_{2} \\omega^{4}}{k_{1} k_{2}}-\\frac{\\left(k_{1} m_{2}+k_{2} m_{1}+k_{2} m_{2}\\right) \\omega^{2}}{k_{1} k_{2}}+1\\right]其中的第二项的化简为 \\frac{\\left(k_{1} m_{2}+k_{2} m_{1}+k_{2} m_{2}\\right) \\omega^{2}}{k_{1} k_{2}}=\\frac{\\left(\\mu k_{1} m_{1}+\\mu k_{1} m_{1}+\\mu^{2} k_{1} m_{1}\\right) \\omega^{2}}{\\mu k_{1} k_{1}} \\\\=\\frac{\\left(m_{1}+m_{1}+\\mu m_{1}\\right) \\omega^{2}}{k_{1}}=\\frac{(2+\\mu) \\omega^{2}}{k_{1} / m_{1}}=\\frac{(2+\\mu) \\omega^{2}}{\\omega_{0}^{2}}=(2+\\mu) s^{2} 将$\\Delta(\\omega)$ 代入，并且设主质量的静变形为：$\\bar{x}_0=F_0/k_1$ 于是可得： \\frac{\\bar{x}_{1}}{\\bar{x}_{0}}=\\frac{1-s^{2}}{s^{4}-(2+\\mu) s^{2}+1} \\quad \\frac{\\bar{x}_{2}}{\\bar{x}_{0}}=\\frac{1}{s^{4}-(2+\\mu) s^{2}+1} 出现反共振，但在反共振两旁存在两个共振点。 注意： (1) 若机械的运行速度在主系统固有频率附近，机械启动和制止时必然经过s1，这会引起大振幅。 (2) 因为吸振器时根据一个特定的激励频率设计的，只有当固有频率等于外部激励频率时主系统振幅才为零。如果机械在其他频率下运行或者机械上的力包含几个不同的频率成分，则其振幅也可能会很大。 为了允许激励频率$\\omega $ 在 s =1 附近有一定范围的变化，s1、s2 应当相距远些。需要$\\mu$ 的值很大，此时只需要将$k_2，m_2$的间距调大即可，但是这样会使得动力吸振器很笨拙，因此我们采用阻尼动力吸振器。 非线性动力吸振器 稳态响应振幅（复振幅）: \\begin{align} {\\left[\\begin{array}{l}\\bar{x}_{1} \\\\\\bar{x}_{2}\\end{array}\\right] \\left[\\begin{array}{cc}k_{1}+k_{2}-m_{1} \\omega^{2}+i c \\omega & -\\left(k_{2}+i c \\omega\\right) \\\\-\\left(k_{2}+i c \\omega\\right) & k_{2}-m_{2} \\omega^{2}+i c \\omega\\end{array}\\right]^{-1}\\left[\\begin{array}{c}F_{0} \\\\0\\end{array}\\right] } \\frac{F_{0}}{\\Delta(\\omega)}\\left[\\begin{array}{c}k_{2}-m_{2} \\omega^{2}+i c \\omega \\\\k_{2}+i c \\omega\\end{array}\\right] \\\\ \\end{align} \\begin{align} 其中: \\Delta(\\omega)&=\\left(k_{1}+k_{2}-m_{1} \\omega^{2}+i c \\omega\\right)\\left(k_{2}-m_{2} \\omega^{2}+i c \\omega\\right)-\\left(k_{2}+i c \\omega\\right)^{2} \\\\ &=\\left(k_{1}-m_{1} \\omega^{2}\\right)\\left(k_{2}-m_{2} \\omega^{2}\\right)-k_{2} m_{2} \\omega^{2}+i c \\omega\\left(k_{1}-m_{1} \\omega^{2}-m_{2} \\omega^{2}\\right) \\end{align}主系统复振幅： \\bar{x}_{1}=\\frac{F_{0}\\left(k_{2}-m_{2} \\omega^{2}+i c \\omega\\right)}{\\left(k_{1}-m_{1} \\omega^{2}\\right)\\left(k_{2}-m_{2} \\omega^{2}\\right)-k_{2} m_{2} \\omega^{2}+i c \\omega\\left(k_{1}-m_{1} \\omega^{2}-m_{2} \\omega^{2}\\right)} 转化为无量纲的形式可得： \\frac{\\bar{x}_{1}}{\\delta_{s t}}=\\frac{\\sqrt{\\left(s^{2}-\\alpha^{2}\\right)^{2}+\\left(2 \\xi_{s}\\right)^{2}}}{\\sqrt{\\left[\\mu s^{2} \\alpha^{2}-\\left(s^{2}-1\\right)\\left(s^{2}-\\alpha^{2}\\right)\\right]^{2}+\\left(2 \\xi{s}\\right)^{2}\\left(s^{2}-1+\\mu s^{2}\\right)^{2}}} 对上面图像在不同阻尼下的结果进行分析可得： 当 $\\xi=0$ 时， 系统中无阻尼，有两个共振频率点，s1=0.895,s2=1.12 在s=1时，发生反共振现象 当$\\xi=\\infty$ 时，系统 变成力一个单自由度系统，共振点为s=0.976 阻尼非常大，可以看作时主质量和吸振器焊接在一起了。 当 $\\xi=0.10$ 和 $\\xi=0.32$ 时，可见当 s＝1 时，主系统振幅并不为零，但是和无阻尼系统的两个共振振幅相比，共振振幅明显下降。 无论阻尼取多少，所有曲线都过 S、T 两点。实际设计有阻尼动力吸振器时，一般选取适当的 m2 与 k2，使曲线在 S 和 T点有相同的幅值，并且适当选取阻尼，使曲线在 S、T 两点具有水平切线 动力吸振器典型例题例: 一台重3000N的柴油机，安装在支架上。当其以6000 r/min转速工作时，可以观察到它的振动通过支架对周围环境造成了影响。试确定需要安装在支架上的吸振器的参数。激振力的幅值为250N，辅助质量的最大许可位移为2mm。 例: 一台电动机-发电机组，设计运行速度为2000-4000 r/min。但是由于转子存在微小的不平衡，该机械在运行速度为3000 r/min时发生剧烈振动。为此设计安装一个悬臂式集中质量吸振器来消除振动。当一个有2kg实验载荷的悬臂安装到机器上之后，所得系统的固有频率为2500 r/min和3500 r/min。设计吸振器的质量和刚度，使得整个系统的固有频率在电动机-发电机组的转速范围之外。 注意，这里的2kg时实验的载荷，最优的载荷需要我们求解。 根据实验载荷的质量以及共振频率，可以求出主质量的将质量，下面来计算所需的吸振器的刚度和主力。 这里设计时，先确定的是频率比的下限，由下限s1来求出质量比$\\mu$ ,之后带回来求解共振质量比的上限，并求解出共振频率，判断是是否满足要求，不满足则先确定上限后确定下限。 工程应用(调谐质量阻尼器) 调谐质量阻尼器（Tuned Mass Damper，TMD），最早的结构振动被动控制装置之一，在建筑、桥梁、车辆、船舶及机械领域均有广泛应用。 TMD三个基本单元： 吸收振动能量的质量块m、耗散能量的阻尼装置C，以及刚度单元K。 从能量角度说是一种能量转移，即将主系统的振动能量转移到TMD上。 TMD减振方法的优点： (1) 减振效果显著。只要TMD固有频率与主系统固有频率一致，理论上主系统的振动可完全消除，实际工程中也能达到80％以上。 (2) 结构设计灵活。可根据振动主系统结构特征和安装空间，将TMD外形结构设计成不同形状，如可设计成摆式TMD，悬臂式TMD或环形TMD。 TMD减振方法的缺点： (1) 最大缺点是减振频带较窄。只当TMD固有频率与被控对象激振频率一致或接近时，减振才有效，若偏离时便会产生两个新共振峰。因此，如果TMD使用不当，不但不能吸振，反而易产生新的共振，即＂频率失调＂现象。 (2) TMD－旦发生频率失调，主系统会因共振而产生更大振动。该现象在当TMD质量较小时更为明显，这对TMD固有频率的设计提出了更高要求。 (3) TMD启动速度具有一定滞后性，对冲击或突发载荷等无法迅速反应。由于TMD对调谐频率的敏感型，许多学者对其有效频带拓宽方法进行研究，出现了各种主动、半主动调谐质量阻尼器便调谐其目标频率。 对于风振有很好的抑制作用，但是对于地震就由一定的延迟。 几种最新的调谐质量阻尼器： 主动型调谐质量阻尼器(ATMD)：从20世纪90年代初期起由于现代控制理论的普及如PID控制、LQR控制等，得到广泛的研究。ATMD最有魅力的地方就是可利用一个TMD就可实现多个振动模态的控制。ATMD优于被动TMD，但ATMD需要提供额外能量来获得期望的减振效果。 发生地震会断电，没有能量来源基本就废了，跟被动的没什么区别。 半自动型调谐质量阻尼器(SATMD)：由于具有刚度和阻尼可调的能力，及较强的适应能力，自1980年被提出后便得到了广泛关注。SATMD维护简单，对外部能源需求比ATMD低，可靠性和鲁棒性好。相关文献指出，SATMD比TMD和ATMD更有效和更具有发展潜力。 振型叠加法 求解多自由度系统的问题的本质，就是求解一个模态矩阵，使得M、K矩阵可以实现正交，对角化。 振型叠加法可用于分析多自由度系统的受迫振动 前面讨论的外部激励为简谐激励，因此可采用$X(t)=\\bar{X}e^{i\\omega t} $ 直接法进行求解 当外部激励不是简谐激励时，则不能用直接法，此时可采用振型叠加法下面先用振型叠加法对简谐激励的多自由度系统的受迫振动进行求解，以进一步阐述多自由度系统的共振特性 然后采用振型叠加法对任意外部激励时系统的响应进行求解 简谐激励时的情况考虑简谐激励的情况，使用标准的模态叠加法求解，就是先将空间由物理空间转换为模态空间，分别求解之后在转化成为物理空间。 $ n $ 自由度系统: $\\boldsymbol{M} \\ddot{\\boldsymbol{X}}(t)+\\boldsymbol{K} \\boldsymbol{X}(t)=\\boldsymbol{F}_{0} e^{i \\omega t} \\quad \\boldsymbol{X} \\in R^{n} \\quad \\boldsymbol{F}_{0} \\in R^{n \\times 1} $ 将物理空间转换为模态空间可得： \\boldsymbol{X}(t)=\\boldsymbol{\\Phi} \\boldsymbol{X}_{p}(t) \\quad \\boldsymbol{M}_{p} \\ddot{\\boldsymbol{X}}_{p}(t)+\\boldsymbol{K}_{p} \\boldsymbol{X}_{p}(t)=\\boldsymbol{F}_{p} e^{i \\omega t} \\quad \\boldsymbol{F}_{p}=\\boldsymbol{\\Phi}^{T} \\boldsymbol{F}_{0} \\in R^{n \\times 1} \\\\m_{p j} \\ddot{x}_{p j}(t)+k_{p j} x_{p j}(t)={F_{p j} e^{i \\omega t}}\\quad j=1 \\sim n\\\\ F_{p j}=\\phi^{(j)^{T}} \\boldsymbol{F}_{0}作用力模态变换的具体解释: F_p=\\left[\\begin{array}{c}F_{p_{1}} \\\\ \\vdots \\\\ F_{p} \\\\ \\vdots \\\\ F_{p_{n}}\\end{array}\\right]=\\boldsymbol{\\Phi}^{T} \\boldsymbol{F}_{0}=\\left[\\begin{array}{lllll}\\phi^{(1)} & \\cdots & \\phi^{(j)} & \\cdots & \\phi^{(n)}\\end{array}\\right]^{T}\\left[\\begin{array}{c}F_{01} \\\\ \\vdots \\\\ F_{0 n}\\end{array}\\right]=\\left[\\begin{array}{c}\\phi^{(1)^{T}} \\\\ \\vdots \\\\ \\phi^{(j)^{T}} \\\\ \\vdots \\\\ \\phi^{(n)^{T}}\\end{array}\\right] \\boldsymbol{F}_{0}=\\left[\\begin{array}{c}\\phi^{(1)^{T}} \\boldsymbol{F}_{0} \\\\ \\vdots \\\\ \\phi^{(j)^{T}} \\boldsymbol{F}_{0} \\\\ \\vdots \\\\ \\phi^{(n)^{T}} \\boldsymbol{F}_{0}\\end{array}\\right]模态坐标解： x_{p j}(t)=\\frac{F_{p j}}{k_{p j}} \\cdot \\frac{1}{1-s_{j}^{2}} e^{i \\omega t}=\\frac{\\phi^{(j)^{T}} \\boldsymbol{F}_{0}}{k_{p j}\\left(1-s_{j}^{2}\\right)} e^{i \\omega t} 其中，外部激励频率与第 j 阶固有频率之比 :$ s_{j}=\\dfrac{\\omega}{\\omega_{j}}$ 各坐标的受迫振动规律完全类似于单自由度系统的受迫振动规律： \\boldsymbol{X}(t)=\\boldsymbol{\\Phi} \\boldsymbol{X}_{p}(t)=\\sum_{j=1}^{n} \\boldsymbol{\\phi}^{(j)} x_{p j}(t)=\\sum_{j=1}^{n} \\frac{\\phi^{(j)} \\phi^{(j)^{T}}}{k_{p j}\\left(1-s_{j}^{2}\\right)} \\boldsymbol{F}_{0} e^{i \\omega t}当 $\\omega \\rightarrow \\omega_j$ 时， $s_j \\rightarrow 1$ 于是就会发生共振。即第 j 阶主坐标的受迫振动幅度将急剧增大，导致第 j 阶频率的共振。 系统具有 n 个不相等的固有频率时，可以出现 n 种不同频率的共振 直接法就是将复数项去掉，直接建立模长$\\bar{X},\\bar{F_0}$ 的关系来求解。但是直接法的前提是必须是简谐激励。 例：三自由度系统 求：系统稳态响应 这里可以看出，其第二阶的固有频率和系统外部激励比较接近。 激振频率接近第二阶固有频率，在稳态响应中第二阶振型占主要成分。 任意外部激励时的情况$ n $ 自由度系统的动力学方程为： \\boldsymbol{M} \\ddot{\\boldsymbol{X}}(t)+\\boldsymbol{K} \\boldsymbol{X}(t)=\\boldsymbol{F}_{0}(t) \\quad \\boldsymbol{X} \\in R^{n} \\quad \\boldsymbol{X}(0), \\dot{\\boldsymbol{X}}(0) \\\\ \\boldsymbol{X}(t)=\\ddot{\\boldsymbol{\\Phi}}_{N} \\boldsymbol{X}_{N}(t) \\quad \\boldsymbol{\\Lambda} \\boldsymbol{X}_{N}(t)=\\boldsymbol{F}_{N}(t)正则振型矩阵: \\ddot{x}_{N i}(t)+\\omega_{i}^{2} x_{N i}(t)=F_{N i}(t), \\quad i=1 \\sim n \\quad 模态广义力\\\\ \\boldsymbol{F}_{N}=\\boldsymbol{\\Phi}_{N}^{T} \\boldsymbol{F}_{0}=\\left[F_{N 1}, F_{N 2}, \\cdots, F_{N n}\\right]^{T}模态坐标初始条件：$ \\boldsymbol{X}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\boldsymbol{X}(0) \\quad \\dot{\\boldsymbol{X}}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\dot{\\boldsymbol{X}}(0)$ \\\\ x_{N i}(t)=x_{N i}(0) \\cos \\omega_{i} t+\\frac{\\dot{x}_{N i}(0)}{\\omega_{i}} \\sin \\omega_{i} t+\\frac{1}{\\omega_{i}} \\int_{0}^{t} F_{N i}(\\tau) \\sin \\omega_{i}(t-\\tau) d \\tau \\quad (i=1 \\sim n)在得到 $\\boldsymbol{X}_{N}$后，利用 $\\boldsymbol{X}=\\boldsymbol{\\phi}_{N}\\boldsymbol{X}_{N}$得出原系统的解。 主振型矩阵： \\boldsymbol{M} \\ddot{\\boldsymbol{X}_p}(t)+\\boldsymbol{K} \\boldsymbol{X}_p(t)=\\boldsymbol{F}_{p}(t) \\\\ m_{p i} \\ddot{x}_{p i}(t)+k_{p i} x_{p i}(t)=F_{p i}(t), \\quad i=1 \\sim n \\quad \\\\ 模态广义力 \\quad\\boldsymbol{F}_{p}=\\boldsymbol{\\Phi}^{T} \\boldsymbol{F}_{0}=\\left[F_{p 1}, F_{p 2}, \\cdots, F_{p n}\\right]^{T}模态坐标初始条件: $ \\boldsymbol{X}_{p}(0)=\\boldsymbol{\\Phi}^{-1} \\boldsymbol{X}(0) \\quad \\dot{\\boldsymbol{X}}_{p}(0)=\\boldsymbol{\\Phi}^{-1} \\dot{\\boldsymbol{X}}(0) $ x_{P i}(t)=x_{P i}(0) \\cos \\omega_{i} t+\\frac{\\dot{x}_{P i}(0)}{\\omega_{i}} \\sin \\omega_{i} t+\\frac{1}{m_{p i} \\omega_{i}} \\int_{0}^{t} F_{P i}(\\tau) \\sin \\omega_{i}(t-\\tau) d \\tau在得到$ X_{p} $后, 利用 $\\boldsymbol{X}=\\boldsymbol{\\Phi} \\boldsymbol{X}_{p} $ 得出原系统的解。 例：四自由度系统，在第一个和第四个质量上作用有阶梯力F，零初始条件，求：系统响应。 解：由题可得动力方程为： {\\left[\\begin{array}{cccc}m & 0 & 0 & 0 \\\\ 0 & m & 0 & 0 \\\\ 0 & 0 & m & 0 \\\\ 0 & 0 & 0 & m\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t) \\\\ \\ddot{x}_{3}(t) \\\\ \\ddot{x}_{4}(t)\\end{array}\\right]+k\\left[\\begin{array}{cccc}1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1\\end{array}\\right]\\left[\\begin{array}{c}x_{1}(t) \\\\ x_{2}(t) \\\\ x_{3}(t) \\\\ x_{4}(t)\\end{array}\\right]=\\left[\\begin{array}{l}F \\\\ 0 \\\\ 0 \\\\ F\\end{array}\\right]=F_{0}(t) } \\\\\\omega_{1}^{2}=0, \\quad \\omega_{2}^{2}=(2-\\sqrt{2}) \\frac{k}{m}, \\quad \\omega_{3}^{2}=\\frac{2 k}{m}, \\quad \\omega_{4}^{2}=(2+\\sqrt{2}) \\frac{k}{m} \\ddot{x}_{N i}(t)+\\omega_{i}^{2} x_{N i}(t)=F_{N i}(t), \\quad i=1 \\sim 4 \\\\ i=1: \\quad \\omega_{1}^{2}=0 \\quad \\Rightarrow \\quad \\ddot{x}_{N1}(t)=F_{Ni} \\quad \\Rightarrow \\quad x_{Ni}=\\frac 1 2F_{Ni}t^2 \\\\i \\neq 1: \\quad x_{N i}(t)=\\frac{1}{\\omega_{1}} \\int_{0}^{t} F_{N i} \\sin \\omega_{i}(t-\\tau) d \\tau=\\frac{F_{N i}}{\\omega_{i}^{2}}\\left(1-\\cos \\omega_{i} t\\right)\\\\矩阵形式: $\\quad \\boldsymbol{X}_{N}(t)=\\left[\\begin{array}{c}x_{N 1}(t) \\\\ x_{N 2}(t) \\\\ x_{N 3}(t) \\\\ x_{N 4}(t)\\end{array}\\right]=\\frac{F}{\\sqrt{m}}\\left[\\begin{array}{c}0.5 t^{2} \\\\ 0 \\\\ m\\left(1-\\cos \\omega_{3} t\\right) / 2 k \\\\ 0\\end{array}\\right]$ 原系统响应: \\boldsymbol{X}(t)= \\boldsymbol{\\Phi}_{N} \\boldsymbol{X}_{N}(t) =\\frac{F}{4 m}\\left[\\begin{array}{l}t^{2}+\\left(1-\\cos \\omega_{3} t\\right) m / k \\\\ t^{2}-\\left(1-\\cos \\omega_{3} t\\right) m / k \\\\ t^{2}-\\left(1-\\cos \\omega_{3} t\\right) m / k \\\\ t^{2}+\\left(1-\\cos \\omega_{3} t\\right) m / k\\end{array}\\right]","link":"/2022/03/14/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E5%A4%9A%E8%87%AA%E7%94%B1%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%8F%97%E8%BF%AB%E6%8C%AF%E5%8A%A8/"},{"title":"第四章 多自由度系统振动 频率方程的零根和重根情形","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是多自由振动系统中频率方程的零根和重根的处理，主要介绍零根和重根的处理以及刚体模态的存在条件。 对于零根的情形K的逆不存在，此时模态矩阵的逆也是不存在的。对于重根的情形，其模态矩阵可能有两列是一致的。 本质都是对模态矩阵的存在性和模态矩阵的秩进行分析的，只要模态矩阵存在且满秩，就存在质量和刚度的正交性的转换，就可以用叠加法来求解。 频率方程零根的情形 \\omega=0 \\quad \\Rightarrow \\quad\\boldsymbol|{K}|=0 K 为奇异矩阵是零固有频率存在的充要条件，满足此条件时系统的刚度矩阵 K 是半正定的。 \\omega=0 \\quad \\Rightarrow \\quad\\boldsymbol{K}\\phi=0 当半正定系统按刚体振型运动时，不发生弹性变形，因此不产生弹性恢复力。 因此假定系统中,$\\omega_1=0$ 一般情况下只有一阶频率为0，因为一阶频率在其中是最小的，此时其对应的主坐标方程为： m_{p 1} \\ddot{x}_{p 1}(t)+k_{p 1} x_{p 1}(t)=0 \\Rightarrow \\ddot{x}_{p 1}(t)+\\omega_{1}^2 x_{p 1}(t)=0\\\\ \\Rightarrow \\ddot{x}_{p 1}(t)=0 \\Rightarrow {x}_{p 1}(t)=at+b其中，a,b是由初始条件决定的。 此时对应的模态矩阵$\\phi^{(1)}=[1 \\quad 1 \\quad 1]^T, $ 三个质量的运动是同步的，且没有相对的位移。 注意$\\omega=0$ 可以得到 $\\phi^{(1)}=[1 \\quad 1 \\quad 1]^T, $ 是不为0的，但是反之不一定成立。即根据频率可以判断模态的情况，但是无法根据模态判断频率。 表明此主振动为随时间匀速增大的刚体位移。 本节主要考虑的是在有零根和重根的条件下，如何来求解模态矩阵，使得其正交性存在。 系统的刚体自由度可以利用振型的正交性条件消除。 设 $\\phi^{(1)}$ 为与 $\\omega_1=0$ 对应的刚体位移振型，由于正交性条件 $\\boldsymbol{\\phi}^{(i) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(j)}=0(i \\neq j)$ 可得： \\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(i)}=0\\quad (i=2,3,...,n)其中$\\boldsymbol{\\phi}^{(j)}(i=2,3,…,n)$ 除刚体位移之外的其它振型。 ${x}_{p i}(t)(i=2,3,…,n)$ 为与 $\\boldsymbol{\\phi}^{(i)}(i=2,3,…,n)$ 所对应的主坐标 右乘${x}_{p i}(t)$ ：$\\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(i)}=0\\quad (i=2,3,…,n)$ 令$\\boldsymbol{X}=\\sum\\limits_{i=1}^{n} \\boldsymbol{\\phi}^{(i)} x_{p i} $系统消除刚体位移后的自由振动可得约束条件： \\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{X(t)}=0\\quad (i=2,3,...,n) 利用此约束条件可消去系统的一个自由度，得到不含刚体位移的缩减系统，缩减系统的刚度矩阵是非奇异的。 相当于站在$m_1$上看系统，单三者是刚体运动时从$m_1$上看就是那两个就是静止不动的。 例：四自由度系统，求系统响应。 方法一(根据实际情况分情况求解): 由系统可得动力方程为 : \\left[\\begin{array}{cccc}m & 0 & 0 & 0 \\\\ 0 & m & 0 & 0 \\\\ 0 & 0 & m & 0 \\\\ 0 & 0 & 0 & m\\end{array}\\right]\\left[\\begin{array}{c}\\ddot{x}_{1}(t) \\\\ \\ddot{x}_{2}(t) \\\\ \\ddot{x}_{3}(t) \\\\ \\ddot{x}_{4}(t)\\end{array}\\right]+k\\left[\\begin{array}{cccc}1 & -1 & 0 & 0 \\\\ -1 & 2 & -1 & 0 \\\\ 0 & -1 & 2 & -1 \\\\ 0 & 0 & -1 & 1\\end{array}\\right]\\left[\\begin{array}{l}x_{1}(t) \\\\ x_{2}(t) \\\\ x_{3}(t) \\\\ x_{4}(t)\\end{array}\\right]=0固有频率为： \\omega_{1}^{2}=0 \\quad \\omega_{2}^{2}=(2-\\sqrt{2}) \\frac{k}{m} \\quad \\omega_{3}^{2}=2 \\frac{k}{m} \\quad \\omega_{4}^{2}=(2+\\sqrt{2}) \\frac{k}{m} 正则模态方程: \\ddot{\\boldsymbol{X}}_{N}(t)+\\boldsymbol{\\Lambda} \\boldsymbol{X}_{N}(t)=\\boldsymbol{0} \\quad \\boldsymbol{X}_{N}=\\left[\\begin{array}{llll}x_{N 1} & x_{N 2} & x_{N 3} & x_{N 4}\\end{array}\\right]^{T} \\\\ \\ddot{x}_{N i}(t)+\\omega_{i}^{2} x_{N i}(t)=0 \\quad(i=1 \\sim 4)将物理坐标初始化的条件带入到，模态坐标进行初始化: \\boldsymbol{X}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\boldsymbol{X}_{0}=\\left[\\begin{array}{llll}0 & 0 & 0 & 0\\end{array}\\right]^{T} \\\\ \\dot{\\boldsymbol{X}}_{N}(0)=\\boldsymbol{\\Phi}_{N}^{-1} \\dot{\\boldsymbol{X}}_{0}=\\sqrt{m} \\times v\\left[\\begin{array}{llll}1 & 0 & 1 & 0\\end{array}\\right]^{T} 物理空间响应: \\boldsymbol{X}(t)=\\boldsymbol{\\Phi}_{N} \\boldsymbol{X}_{N}(t)=\\frac{v}{2}\\left[\\begin{array}{c}t+\\frac{1}{\\omega_{3}} \\sin \\omega_{3} t \\\\t-\\frac{1}{\\omega_{3}} \\sin \\omega_{3} t \\\\t-\\frac{1}{\\omega_{3}} \\sin \\omega_{3} t \\\\t+\\frac{1}{\\omega_{3}} \\sin \\omega_{3} t\\end{array}\\right]方法二(以第一个为参考系减去一个自由度): 解出系统的固有频率为： \\omega_{2}^{2}=(2-\\sqrt{2}) \\frac{k}{m} \\quad \\omega_{3}^{2}=2 \\frac{k}{m} \\quad \\omega_{4}^{2}=(2+\\sqrt{2}) \\frac{k}{m} 两种方法的结果不同，因为法二是站在$m_1$ 上面看其他刚体的振动的，因此法二的结果中消除的刚体的振动。 此模型一样看成是刚体运动和简谐振动的叠加。 同样的方法可以来分析柔性机械臂的变形，先假设机械臂是刚性的，之后我们相对第一个刚体建立浮动基，其主要是相对于第一个刚性基变形的。 方法一的结果： 正两种方法对应的广义坐标和物理含义是不同的。 系统存在刚体模态的条件 系统存在刚体模态的条件： 某阶固有频率为零或刚度矩阵奇异 所对应的振型元素全相等 频率方程的重根情形 在前面引入振型矩阵（或模态矩阵）的概念时，曾假设所有的特征值都是特征方程的单根 复杂的系统中会出现某些特征根彼此很接近甚至相等的情况工程背景之一：柔性航天结构 例如：n自由度系统中 $\\omega_1=\\omega_2 \\quad \\Rightarrow \\quad \\phi^{(1)}=\\phi^{(2)}$ 刚度矩阵奇异。 假设 $\\omega_{1}$ 为 r 重根 $\\omega_{1}^{2}=\\omega_{2}^{2}=\\cdots=\\omega_{r}^{2} $ 其余 $\\omega_{r+1},…,\\omega_n$ 都是单根。 $\\omega^{2}=\\omega_{1}^{2}$ 带入到特征方程之中：$\\left(\\boldsymbol{K}-\\omega_{1}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0}$ 此时特征矩阵的秩为：$rank[\\boldsymbol{K}-\\omega_{1}^{2} \\boldsymbol{M}]=n-r$ 例如当 $\\omega_1^2$ 是单根时，$r=1$ , n 个方程中只有 n – 1 个是独立的为简单计，令$\\omega_1=\\omega_2$ ,$r=2$。 则计算 $\\omega_1$ 对应的振型时，$\\left(\\boldsymbol{K}-\\omega_{1}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0}$ 中有2个是不独立的方程 模态之间的正交性证明： 对$\\phi^{(1)}$ 满足：$ \\left(\\boldsymbol{K}-\\omega_{1}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0} \\Rightarrow \\boldsymbol{K}\\phi^{(1)}=\\omega_{1}^{2} \\boldsymbol{M}\\phi^{(1)}$ ,两边右乘 $\\phi^{(j)}$ 对$\\phi^{(j)} (j=3,4,…,n)$ 满足：$ \\left(\\boldsymbol{K}-\\omega_{j}^{2} \\boldsymbol{M}\\right) \\boldsymbol{\\phi}=\\boldsymbol{0} \\Rightarrow \\boldsymbol{K}\\phi^{(j)}=\\omega_{1}^{2} \\boldsymbol{M}\\phi^{(j)}$ ,两边左乘 $\\phi^{(1)^T}$ \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{K} \\boldsymbol{\\phi}^{(j)}=\\omega_{i}^{2} \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(j)} \\quad \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{K} \\boldsymbol{\\phi}^{(j)}=\\omega_{1}^{2} \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(j)} 整理可得： $(\\omega_{1}^{2}-\\omega_{j}^{2}) \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{M} \\boldsymbol{\\phi}^{(j)}=0$ 由于$\\omega_{1} \\ne \\omega_{j}$ 所以正交 此时，除了前两阶模态之外其余的模态的之间有正交的关系，下面来处理前两阶的模态关系。 这边的描述太过抽象，具体的理解参考下面的例题。 $ \\omega_{1}=\\omega_{2}\\left[\\begin{array}{l}\\phi_{n-1}^{(1)} \\\\ \\phi_{n}^{(1)}\\end{array}\\right]=\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\quad\\left[\\begin{array}{l}\\phi_{n-1}^{(2)} \\\\ \\phi_{n}^{(2)}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right] \\quad \\boldsymbol{\\phi}_{i}^{(1)}, \\boldsymbol{\\phi}_{i}^{(2)} $ 为保证它们之间满足正交性条件令：$\\bar{\\phi}^{(2)}=\\phi^{(2)}+c\\phi^{(1)}$ ，则$\\bar{\\phi}^{(2)}$ 也为最后的结果。 为了保证正交性，要满足$\\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{K} \\boldsymbol{\\bar{\\phi}^{(2)}}=0 \\Rightarrow \\boldsymbol{\\phi}^{(1)^{T}} \\boldsymbol{K} (\\phi^{(2)}+c\\phi^{(1)})=0$ 解得： $ c=-\\dfrac{\\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(2)}}{\\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(1)}}=-\\dfrac{1}{m_{p 1}} \\boldsymbol{\\phi}^{(1) T} \\boldsymbol{M} \\boldsymbol{\\phi}^{(2)} $ 此时满足$\\bar{\\phi}^{(2)},\\phi^{(1)}$ 正交。 由于线性叠加原理，$\\bar{\\phi}^{(2)}$ 也是关于其他模态也是正交的。 例题 例：四自由度系统，求：系统振型矩阵。 首先，确定动力学方程以及固有频率。 划去后两个方程，将前两个方程写为： {\\left[\\begin{array}{ll}-1 & -1 \\\\ -1 & -2\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2}\\end{array}\\right]+\\left[\\begin{array}{cc}-1 & -1 \\\\ -1 & 0\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{3} \\\\ \\phi_{4}\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right] } \\\\ {\\left[\\begin{array}{l}\\phi_{1} \\\\ \\phi_{2}\\end{array}\\right]=-\\left[\\begin{array}{ll}-1 & -1 \\\\ -1 & -2\\end{array}\\right]^{-1}\\left[\\begin{array}{cc}-1 & -1 \\\\ -1 & 0\\end{array}\\right]\\qquad \\left[\\begin{array}{l}\\phi_{3} \\\\ \\phi_{4}\\end{array}\\right]=\\left[\\begin{array}{cc}-1 & -2 \\\\ 0 & 1\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{3} \\\\ \\phi_{4}\\end{array}\\right] }\\\\ \\Rightarrow\\left[\\begin{array}{l}\\phi_{1}\\\\ \\phi_{2} \\\\ \\phi_{3} \\\\ \\phi_{4}\\end{array}\\right]=\\left[\\begin{array}{cc}-1 & -2 \\\\ 0 & 1\\\\ 1 &0 \\\\ 0 & 1\\end{array}\\right]\\left[\\begin{array}{l}\\phi_{3} \\\\ \\phi_{4}\\end{array}\\right]于是由于前两阶的模态没有重根，直接求解可得出前两阶的模态为：$\\phi_{1}=[1 \\quad 1 \\quad 1\\quad 1]^T$,$\\phi_{2}=[0 \\quad -1 \\quad 0 \\quad 1]^T$ 。 根据刚才的求解可得后两阶模态为：$\\phi_{3}=[-1 \\quad 0 \\quad 1\\quad 0]^T$,$\\phi_{2}=[-2 \\quad 1 \\quad 0 \\quad 1]^T$ 不是正交的，其对应的主振型为： 零根的情形：（1）零根只有一个；（2）对应的是存在刚体自由度； 重根的情形：（1）重根可以有很多个；（2）对应的是柔性航天器。","link":"/2022/03/14/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E9%A2%91%E7%8E%87%E6%96%B9%E7%A8%8B%E7%9A%84%E9%9B%B6%E6%A0%B9%E5%92%8C%E9%87%8D%E6%A0%B9%E6%83%85%E5%BD%A2/"},{"title":"第五章 线性振动的近似计算方法 第一部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是线性振动的近似计算方法中邓克利法，瑞利法，里茨法。 在线性多自由度系统振动中，振动问题归结为刚度矩阵和质量矩阵的广义特征值问题 。缺点之一：当系统自由度较大时，求解计算工作量非常大 对于航天的帆板自由度有40万个，对于高铁过隧道时振动的自由度为上亿个。Ansys里面求解特征根是有其核心的算法的，和自己在Matlab里面自己编程求解的结果不同。 本章介绍几种近似计算方法，可作为实用的工程计算方法对系统的振动特性作近似计算 其中主要包括邓克利法，瑞利法，里茨法，传递矩阵法。 早些年计算设备不强大时主要使用上述的方法进行近似计算，而现在的计算方法和软件比较的发达，因此本章的方法只作为了解不是重点。 但是传递矩阵法是重点，现在也有人研究。 邓克利法 由邓克利（Dunkerley）在实验确定多圆盘的横向振动固有频率时提出的 便于作为系统基频的计算公式 上述方程求解得到的模态的结果是相同的，不会因为作用力方程和位移方程而有不同，其结果是相同的。 其中，这里的$a_1$是D矩阵的迹，即对角线元素之和。 M 阵一般是对角的，只有在集中质量和摆动耦合的情况下是对称非对角的. 物理意义：沿第 i 个坐标施加单位力时所产生的第 i 个坐标的位移 对于刚度：$F=kx \\Rightarrow k = \\dfrac F x$ 单位需要的力的大小；对于柔度：$\\dfrac 1 k=\\dfrac x F$ 单位力产生位移。 如果只保留第 i 个质量，所得单自由度系统的固有频率为： $\\bar{\\omega}$ 都是单自由度系统的频率。 \\sum_{i=1}^{n} \\frac{1}{\\omega_{i}^{2}}=\\sum_{i=1}^{n} f_{i i} m_{i} \\qquad \\bar{\\omega}_{i}^{2}=\\frac{k_{i}}{m_{i}}=\\frac{1}{f_{i i} m_{i}}\\\\ \\sum_{i=1}^{n} \\frac{1}{\\omega_{i}^{2}}=\\frac{1}{\\bar{\\omega}_{1}^{2}}+\\frac{1}{\\bar{\\omega}_{2}^{2}}+\\cdots+\\frac{1}{\\bar{\\omega}_{n}^{2}}当第二阶及以上固有频率远大于基频时，得到的基频是精确值的下限： \\frac{1}{\\omega_{1}^{2}} \\approx \\frac{1}{\\bar{\\omega}_{1}^{2}}+\\frac{1}{\\bar{\\omega}_{2}^{2}}+\\cdots+\\frac{1}{\\bar{\\omega}_{n}^{2}}邓克利法：利用单自由度固有频率近似求解多自由度系统基频的方法 \\frac{1}{\\omega_{1}^{2}}+a=b \\quad a=\\frac{1}{\\omega_{2}^{2}}+\\frac{1}{\\omega_{3}^{2}}+\\cdots+\\frac{1}{\\omega_{n}^{2}}\\\\ \\omega_{1}^{2}=\\frac{1}{b-a} \\quad b=\\frac{1}{\\bar{\\omega}_{1}^{2}}+\\frac{1}{\\bar{\\omega}_{2}^{2}}+\\cdots+\\frac{1}{\\bar{\\omega}_{n}^{2}}因在邓克利法中忽略了a，因此所得结果为基频下限 总结：邓克利法的主要是用来求解基频的，将用n个单自由度的频率来估计n自由度系统的基频的下限。 瑞利法 基于能量原理的一种近似方法 可用于计算系统的基频 算出的近似值为实际基频的上限 配合邓克利法算出的基频下限，可以估计实际基频的大致范围 \\phi=\\varphi=a_{1} \\boldsymbol{\\phi}_{N}^{(1)}+a_{2} \\boldsymbol{\\phi}_{N}^{(2)}+\\cdots+a_{N} \\boldsymbol{\\phi}_{N}^{(n)}=\\sum_{j=1} a_{j} \\boldsymbol{\\phi}_{N}^{(j)}=\\boldsymbol{\\Phi}_{N} \\boldsymbol{a}\\\\\\boldsymbol{\\Phi}_{N}=\\left[\\boldsymbol{\\phi}_{N}^{(1)}, \\boldsymbol{\\phi}_{N}^{(2)}, \\cdots, \\boldsymbol{\\phi}_{N}^{(n)}\\right] \\quad \\boldsymbol{a}=\\left[a_{1}, a_{2}, \\cdots, a_{n}\\right]^{T}\\\\R(\\varphi)=\\frac{\\boldsymbol{a}^{T} \\boldsymbol{\\Phi}_{N}^{T} \\boldsymbol{K} \\boldsymbol{\\Phi}_{N} \\boldsymbol{a}}{\\boldsymbol{a}^{T} \\boldsymbol{\\Phi}_{N}^{T} \\boldsymbol{M} \\boldsymbol{\\Phi}_{N} \\boldsymbol{a}}=\\frac{\\boldsymbol{a}^{T} \\boldsymbol{\\Lambda} \\boldsymbol{a}}{\\boldsymbol{a}^{T} \\boldsymbol{I} \\boldsymbol{a}}=\\sum_{j=1}^{n} a_{j}^{2} \\omega_{j}^{2} / \\sum_{j=1}^{n} a_{j}^{2}可以证明， $\\omega_1^2$ 和 $\\omega_n^2$分别为瑞利商的极小值和极大值。 \\omega_{1}^{2} \\leq R(\\varphi) \\leq \\omega_{n}^{2}分析：如果将$\\omega_j$换成$\\omega_1$，此时$\\omega_1$是最低阶固有频率 R(\\varphi)>\\sum_{j=1}^{n} a_{j}^{2} \\omega_{1}^{2} / \\sum_{j=1}^{n} a_{j}^{2}=\\omega_{1}^{2}由瑞利商公式知，当$\\varphi=\\phi^{(1)}$ 确为第一阶振型时：$R(\\varphi)=\\omega_1^2$ 因此，瑞利商的极小值为$\\omega_1^2$，同理可证明，瑞利商的极大值为$\\omega_{n}^2$ 代入瑞利商可得： R(\\varphi) \\approx \\omega_{k}^{2}+\\sum_{j=1}^{n}\\left(\\omega_{j}^{2}-\\omega_{k}^{2}\\right) \\varepsilon_{j}^{2} 因此，若 $\\varphi$与 $\\phi^{(k)}$ 的差异为一阶小量，则瑞利商与 $\\omega_k^2$ 的差别为二阶小量 对于基频的特殊情况，令k＝1，则由于 $\\omega_j^2-\\omega_1^2&gt;0 \\ (j=2,…,n)$ ，瑞利商在基频处取极大值 利用瑞利商估计系统的基频所得的结果必为实际基频的上限 愈接近系统的真实振型，算出的固有频率愈准确 在实际计算的时候，我们直接将静变形作为第一阶模态之后代入到上面式子中进行计算，得出基频的上限。 如果可以得出第二阶的模态，那么我们也可去很好的估计第二阶模态。 里茨法 里兹法是瑞利法的改进 用里兹法不仅可以计算系统的基频，还可以算出系统的前几阶频率和振型 瑞利法算出的基频的精度取决于假设的振型对第一阶主振型的近似程度，而且得到的基频总是精确值的上限 里兹法将对近似振型给出更合理的假设，从而使算出的基频值进一步下降 里兹法基于与瑞利法相同的原理，但将瑞利使用的单个假设振型改进为若干个独立的假设振型的线性组合： \\varphi=a_{1} \\boldsymbol{\\eta}^{(1)}+a_{2} \\boldsymbol{\\eta}^{(2)}+\\ldots+a_{r} \\boldsymbol{\\eta}^{(r)}=\\sum_{j=1} a_{j} \\boldsymbol{\\eta}^{(j)}=\\boldsymbol{\\Pi} \\boldsymbol{A} \\quad \\boldsymbol{\\eta}^{(i)} \\in R^{n \\times 1}\\\\ \\boldsymbol{\\Pi}=\\left[\\boldsymbol{\\eta}^{(1)}, \\boldsymbol{\\eta}^{(2)}, \\cdots, \\boldsymbol{\\eta}^{(r)}\\right] \\in R^{n \\times r} \\quad \\boldsymbol{A}=\\left[a_{1}, a_{2}, \\cdots, a_{r}\\right]^{T} \\in R^{r \\times 1}其中$\\eta^{(i)}$ 为假设模态，A中的元素数量时待定的。 R(\\varphi)=R(\\boldsymbol{\\Pi} \\boldsymbol{A})=\\frac{\\boldsymbol{A}^{T} \\boldsymbol{\\Pi}^{T} \\boldsymbol{K} \\boldsymbol{\\Pi} \\boldsymbol{A}}{\\boldsymbol{A}^{T} \\boldsymbol{\\Pi}^{T} \\boldsymbol{M} \\boldsymbol{\\Pi} \\boldsymbol{A}}=\\frac{\\boldsymbol{A}^{T} \\overline{\\boldsymbol{K}} \\boldsymbol{A}}{\\boldsymbol{A}^{T} \\overline{\\boldsymbol{M}} \\boldsymbol{A}}=\\bar{\\omega}^{2}\\\\ \\overline{\\boldsymbol{K}}=\\boldsymbol{\\Pi}^{T} \\boldsymbol{K} \\boldsymbol{\\Pi} \\in R^{r \\times r} \\quad \\overline{\\boldsymbol{M}}=\\boldsymbol{\\Pi}^{T} \\boldsymbol{M} \\boldsymbol{\\Pi} \\in R^{r \\times r}由于 $R(\\varphi)$ 在系统中的真实主振型处取驻值,所以 A 的各个元素应当从下式确定: \\frac {\\partial R}{\\partial a_j} = 0 \\quad (j=1,2,...,r) $\\dfrac {\\partial} {\\partial A}$ :为将函数分别对 A 各个元素依次求偏导，排成列向量。 同理, 有：$\\dfrac{\\partial}{\\partial A}\\left(A^{T} \\bar{M} A\\right)=2 \\bar{M} A$ \\frac{\\partial}{\\partial \\boldsymbol{A}}\\left(\\boldsymbol{A}^{T} \\overline{\\boldsymbol{K}} \\boldsymbol{A}\\right)-\\bar{\\omega}^{2} \\frac{\\partial}{\\partial \\boldsymbol{A}}\\left(\\boldsymbol{A}^{T} \\overline{\\boldsymbol{M}} \\boldsymbol{A}\\right)=\\mathbf{0}\\\\ \\Rightarrow \\left(\\overline{\\boldsymbol{K}}-\\bar{\\omega}^{2} \\overline{\\boldsymbol{M}}\\right) A=0 由于$\\bar{K}、\\bar{M}$ 阶数 r 一般远小于系统自由度数 n，上式矩阵特征值问题比原系统的矩阵特征值问题解起来容易得多 里兹法是一种缩减系统自由度求解固有振动的近似方法 频率求出即可，但是计算出的模态必须满足正交的要求。 下面证明正交性： 与我们假设的这个$\\varphi$ 是若干个独立的假设振型，因此可知A的不同分量间是线性独立的： i \\neq j \\text { 时 } \\boldsymbol{A}^{(i) T} \\overline{\\boldsymbol{M}} \\boldsymbol{A}^{(j)}=0, \\quad \\boldsymbol{A}^{(i) T} \\overline{\\boldsymbol{K}} \\boldsymbol{A}^{(j)}=0 \\quad \\text { 成立 } \\\\\\varphi^{(i) T} \\boldsymbol{M} \\varphi^{(j)} =\\boldsymbol{A}^{(i) T} \\boldsymbol{\\Pi}^{T} \\boldsymbol{M} \\boldsymbol{\\Pi} \\boldsymbol{A}^{(j)}=\\boldsymbol{A}^{(i) T} \\overline{\\boldsymbol{M}} \\boldsymbol{A}^{(j)}=0\\\\ \\varphi^{(i) T} \\boldsymbol{K} \\varphi^{(j)}=\\boldsymbol{A}^{(i) T} \\boldsymbol{\\Pi}^{T} \\boldsymbol{K} \\boldsymbol{\\Pi} \\boldsymbol{A}^{(j)}=\\boldsymbol{A}^{(i) T} \\overline{\\boldsymbol{K}} \\boldsymbol{A}^{(j)}=0得出的近似主振型式关于矩阵 M 和 K 相互正交 首先假设振型： \\boldsymbol{\\Pi}=\\left[\\begin{array}{ll}\\boldsymbol{\\eta}^{(1)} & \\boldsymbol{\\eta}^{(2)}\\end{array}\\right]=\\left[\\begin{array}{cc}1 & 1 \\\\2 & 2 \\\\3 & -1\\end{array}\\right]\\\\缩减后的新系统的刚度矩阵和质量矩阵： \\overline{\\boldsymbol{K}}=\\boldsymbol{\\Pi}^{T} \\boldsymbol{K} \\boldsymbol{\\Pi}=\\left[\\begin{array}{cc}4 k & -4 k \\\\-4 k & 20 k\\end{array}\\right] \\quad \\overline{\\boldsymbol{M}}=\\boldsymbol{\\Pi}^{T} \\boldsymbol{M} \\boldsymbol{\\Pi}^{T}=\\left[\\begin{array}{cc}23 m & -m \\\\-m & 7 m\\end{array}\\right]特征值问题: \\quad\\left[\\begin{array}{cc}4-23 \\alpha & -4+\\alpha \\\\ -4+\\alpha & 20-7 \\alpha\\end{array}\\right]=\\left[\\begin{array}{l}0 \\\\ 0\\end{array}\\right] \\quad \\alpha=\\frac{m \\bar{\\omega}^{2}}{k} \\begin{array}{c}\\alpha_{1}=0.139853 \\quad \\alpha_{2}=2.860147 \\\\\\boldsymbol{A}^{(1)}=\\left[\\begin{array}{ll}4.927547, & 1\\end{array}\\right]^{T} \\quad \\boldsymbol{A}^{(2)}=\\left[\\begin{array}{ll}-0.018449, & 1\\end{array}\\right]^{T}\\end{array} $\\beta_1$ 和 $\\beta_2$ 是主振型归一化时产生的常数，不必考虑 里兹法所得基频精度比瑞利法高，但第二阶固有频率精度欠佳，但是我们可以将估计出的值迭代多次逐渐逼近最优值。","link":"/2022/03/15/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%8C%AF%E5%8A%A8%E7%9A%84%E8%BF%91%E4%BC%BC%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第四章 多自由度系统振动 有阻尼的多自由度系统","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是多自由振动系统中有阻尼的多自由度系统的处理，主要介绍多自由度系统阻尼的求解以及一般粘性阻尼系统的响应。 多自由度系统的阻尼 实际机械系统中不可避免地存在着阻尼 材料的结构阻尼，介质的粘性阻尼等 阻尼力机理复杂，难以给出恰当的数学表达 在阻尼力较小时，或激励远离系统的固有频率时，可以忽略阻尼力的存在，近似地当作无阻尼系统 当激励的频率接近系统的固有频率，激励时间又不是很短暂的情况下，阻尼的影响是不能忽略的 一般情况下，可将各种类型的阻尼化作等效粘性阻尼 有阻尼的 n 自由度系统： \\boldsymbol{M} \\ddot{\\boldsymbol{X}}(t)+C \\dot{\\boldsymbol{X}}(t)+\\boldsymbol{K} \\boldsymbol{X}(t)=\\boldsymbol{F}(t) \\quad \\boldsymbol{X} \\in R^{n}其中，C为阻尼矩阵，元素 $c_{ij}$ ：阻尼影响系数 物理意义：使系统仅在第 j 个广义坐标上产生单位速度而相应于第 i 个坐标上所需施加的力。 阻尼力为广义速度的线性函数 : Q_{d i}(t)=-\\sum_{j=1}^{n} c_{i j} \\dot{x}_{j}(t)阻尼矩阵一般是正定或半正定的对称矩阵 阻尼矩阵的建模方法 这里C阵的建模方法是，先照抄K阵，之后将没有阻尼的地方的$c_i=0$即可，如上图所示，只有$c_1$存在因此保留，其余的全部为0. 虽然主质量矩阵与主刚度矩阵是对角阵，但阻尼矩阵一般非对角阵，因而主坐标Y下的强迫振动方程仍然存在耦合。 此时的C阵是不解耦的，因此模态叠加法就无法使用。 若 $C_p$ 非对角，则在无阻尼系统中介绍的主坐标方法或正则坐标方法都不再适用，振动分析将变得十分复杂。 对于实模态分析而言，非解耦的情况时无法分析的，但是可以用复模态分析的方法来处理。 为了能沿用无阻尼系统中的分析方法，工程中常采用下列近似处理方法 (1) 忽略$C_p$ 矩阵中全部的非对角线元素 其中$C_{Pi}$为第i阶主振型的阻尼矩阵，或第i阶振型的阻尼或模态阻尼 \\boldsymbol{C}_{P}=\\left[\\begin{array}{ccc}c_{p 1} & & \\\\ & \\ddots & \\\\ & & c_{p n}\\end{array}\\right] 但是当我们的阻尼很大，相比之下无法忽略。 其中，$\\xi_i$: 第i阶振型阻尼比或模态阻尼比 (2) 将矩阵 C 假设为比例阻尼 假设C由以下的形式：$C=aM+bK$ ，其中a、b为常数，C为物理空间的阻尼矩阵。 这边的a,b的确定需要进行实验，如航空航天的地面试验，通过测试不同的参数和最后的结果进行对比来验证。 代入 $C_{p}=\\boldsymbol{\\Phi}^{T} \\boldsymbol{C} \\boldsymbol{\\Phi}$ 中可得： \\boldsymbol{C}_{p}=\\boldsymbol{\\Phi}^{T}(a \\boldsymbol{M}+b \\boldsymbol{K}) \\boldsymbol{\\Phi}=a \\boldsymbol{M}_{p}+b \\boldsymbol{K}_{p} \\quad 对角阵相对阻尼系数: $\\quad \\xi_{i}=\\dfrac{c_{p i}}{2 \\omega_{i} m_{p i}}=\\dfrac{a m_{p i}+b k_{p i}}{2 \\omega_{i} m_{p i}}=\\dfrac{1}{2}\\left(\\dfrac{a}{\\omega_{i}}+b \\omega_{i}\\right)$ (3) 由实验测定n阶振型阻尼系数 $\\xi_i \\quad (i=1,2,…,n)$ 实验法在工程中使用的比较多。一般情况下低阶模态占了主要的成分，测量悬臂梁的模态时，一般对末端加一个激励得出振型为第一阶占了主要成分，对中间的结点给一个激励，得出的响应的第二阶为主要成分。 实验测量高阶很好测，但低阶得模态不易做。 一般粘性阻尼系统的响应当阻尼矩阵 C 不允许忽略非对角元素，并且阻尼无法简化为等效粘性阻尼（橡胶存在粘性滞回现象，无法进行等效处理），为以上近似方法不成立， 须用复模态进行求解。 2n 个特征值：$\\lambda_1,\\lambda_2,…,\\lambda_{2n},$ 这2n个特征根中，有复数也有实数 2n个特征向量：$\\phi^{(1)},\\phi^{(2)},…,\\phi^{(2n)}$ $\\phi^{(i)}\\in R^{n \\times 1}$ 因为特征方程的系数都是实的,所以特征值为复数时，必定以共轭形式成对出现。相应地，特征向量也是共轭成对的复向量复振型，这种现象为复模态或复振型。 这是一种具有相位关系的振型，不再具有原来主振型的意义。 之前的模态代表着一种振型的形状。 当特征值为具有负实部的复数时，每一对这样的共轭特征值对应系统中具有特定的频率和衰减系数的自由衰减振动。 这里通过补充一个方程，将原方程从一个二阶微分方程，转化为一个一阶状态方程。 讨论自由振动此时的状态方程为： \\hat{\\boldsymbol{M}} \\dot{\\boldsymbol{Y}}(t)+\\hat{\\boldsymbol{K}} \\boldsymbol{Y}(t)=\\mathbf{0} \\\\ \\text{初始条件为:} \\boldsymbol{X_0} ,\\boldsymbol{\\dot X_0} \\quad \\boldsymbol{Y}=\\left[\\begin{array}{l}\\dot{\\boldsymbol{X}} \\\\ \\boldsymbol{X}\\end{array}\\right] \\in R^{2 n} \\\\代入以下形式的解: $Y(t)=\\psi e^{\\lambda t}\\quad (\\psi\\in R^{2n \\times1})$ 特征值问题: $(\\hat{\\boldsymbol{K}}+\\lambda \\hat{\\boldsymbol{M}}) \\psi=0 $ 将方程转化成为状态方程之后，方程的维数和特征根的数目增加了一倍。 $2n$ 个特征向量: \\psi^{(1)}, \\psi^{(2)}, \\cdots, \\psi^{(2 n)} \\quad\\left(\\psi^{(i)} \\in R^{2 n \\times 1}\\right)\\\\ \\boldsymbol{\\Psi}=\\left[\\psi^{(1)}, \\psi^{(2)}, \\cdots, \\psi^{(2 n)}\\right] \\in R^{2 n \\times 2 n}求解出来的复模态具有正交性: \\boldsymbol{\\Psi}^{T} \\hat{\\boldsymbol{M}} \\boldsymbol{\\Psi}=\\hat{\\boldsymbol{M}}_{p} \\in R^{2 n \\times 2 n}=\\operatorname{diag}\\left(\\hat{m}_{p 1}, \\cdots, \\hat{m}_{p(2 n)}\\right) \\\\\\boldsymbol{\\Psi}^{T} \\hat{\\boldsymbol{K}} \\boldsymbol{\\Psi}=\\hat{\\boldsymbol{K}}_{p} \\in R^{2 n \\times 2 n}=\\operatorname{diag}\\left(\\hat{k}_{p 1}, \\cdots, \\hat{k}_{p(2 n)}\\right)复特征值列为对角阵(谱矩阵) : $ \\boldsymbol{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\cdots, \\lambda_{2 n}\\right) \\in R^{2 n \\times 2 n} $ 此时的动力学方程为： M \\ddot{X}(t)+\\boldsymbol{C} \\dot{\\boldsymbol{X}}(t)+\\boldsymbol{K} \\boldsymbol{X}(t)=\\boldsymbol{0} \\quad \\boldsymbol{X} \\in R^{n}代入得方程的特解: $\\boldsymbol{X}(t)=\\phi e^{\\lambda t} \\quad\\left(\\phi \\in R^{n \\times 1}\\right)$ 特征值问题： $\\left(M \\lambda^{2}+C \\lambda+\\boldsymbol{K}\\right) \\boldsymbol{\\varphi}=\\mathbf{0} $ $ \\psi$ 与 $\\phi$ 之间关系，即 $ \\Psi$ 与 $\\Phi$ 之间关系为： \\psi=\\left[\\begin{array}{c}\\lambda \\phi \\\\\\phi\\end{array}\\right] \\quad \\Psi=\\left[\\begin{array}{c}\\Phi\\Lambda \\\\\\Phi\\end{array}\\right]之后对Y进行变换 $Y=\\Psi Z$, 代入到原来的状态方程中可得： \\boldsymbol{\\Psi}^{T} \\hat{\\boldsymbol{M}} \\boldsymbol{\\Psi} \\dot{\\boldsymbol{Z}}(t)+\\boldsymbol{\\Psi}^{T} \\hat{\\boldsymbol{K}} \\boldsymbol{\\Psi} \\boldsymbol{Z}(t)=\\mathbf{0}\\\\ \\begin{array}{l}\\hat{m}_{p_{i}} \\dot{z}_{i}(t)+\\hat{k}_{p_{i}} z_{i}(t)=0, \\quad i=1 \\sim 2 n \\\\\\dot{z}_{i}(t)-\\lambda_{i} z_{i}(t)=0 \\quad z_{i}(t)=z_{i}(0) e^{\\lambda_{i} t}\\end{array}初始条件：$\\boldsymbol{Z}(0)=\\boldsymbol{\\Psi}^{-1} \\boldsymbol{Y}(0)$ 这里的c,阻尼矩阵的项被并入到 $\\hat{\\boldsymbol{M}}$ 中。 得$z_i(i=1,2,…,2n)$ 于是$Z、Y、X、\\dot X$ 就可以全部求出。 \\boldsymbol{\\psi}^{(i)^{T}} \\hat{\\boldsymbol{M}} \\boldsymbol{\\psi}^{(j)}=\\delta_{i j} \\hat{m}_{p i} \\quad \\boldsymbol{\\psi}^{(i)^{T}} \\hat{\\boldsymbol{K}} \\boldsymbol{\\psi}^{(j)}=\\delta_{i j} \\hat{k}_{p i} 对于$Z(0)$ 矩阵的来说，其第 i 个分量为： z_{i}(0)=\\frac{1}{\\hat{m}_{p i}} \\boldsymbol{\\Psi}^{T} \\hat{\\boldsymbol{M}} \\boldsymbol{Y}(0) =\\frac{1}{\\hat{m}_{p i}}\\left[\\begin{array}{ll}\\lambda_{i} \\phi_{i}^{T} & \\phi_{i}^{T}\\end{array}\\right]\\left[\\begin{array}{cc}\\boldsymbol{0} & \\boldsymbol{M} \\\\ \\boldsymbol{M} & \\boldsymbol{C}\\end{array}\\right]\\left[\\begin{array}{c}\\dot{\\boldsymbol{X}}_{0} \\\\ \\boldsymbol{X}_{0}\\end{array}\\right] \\\\ =\\frac{1}{\\hat{m}_{p i}} \\boldsymbol{\\phi}_{i}^{T}\\left(\\lambda_{i} \\boldsymbol{M} \\boldsymbol{X}_{0}+\\boldsymbol{M} \\dot{\\boldsymbol{X}}_{0}+\\boldsymbol{C} \\boldsymbol{X}_{0}\\right)求解完$z_i(0)$ 代入公式 $z_i(t)=z_i(0)e^{\\lambda_i t}$ 中来求解$z_i(t)$，进而得出$Z(t)$ ，将$Z(t)$ 代入到下列方程得： \\boldsymbol{Y}(t)=\\left[\\begin{array}{c}\\dot{\\boldsymbol{X}}(t) \\\\ \\boldsymbol{X}(t)\\end{array}\\right]=\\boldsymbol{\\Psi} \\boldsymbol{Z}(t)=\\left[\\begin{array}{c}\\boldsymbol{\\Phi} \\boldsymbol{\\Lambda} \\\\ \\boldsymbol{\\Phi}\\end{array}\\right] \\boldsymbol{Z}(t)\\\\ \\boldsymbol{X}(t)=\\boldsymbol{\\Phi} \\boldsymbol{Z}(t) \\quad \\dot{\\boldsymbol{X}}(t)=\\boldsymbol{\\Phi} \\boldsymbol{\\Lambda} \\boldsymbol{Z}(t)\\\\ \\boldsymbol{\\Lambda}=\\operatorname{diag}\\left(\\lambda_{1}, \\cdots, \\lambda_{2 n}\\right) \\in R^{2 n \\times 2 n} \\\\ \\boldsymbol{\\Phi}=\\left[\\boldsymbol{\\phi}^{(1)}, \\quad \\boldsymbol{\\phi}^{(2)}, \\cdots, \\quad \\boldsymbol{\\phi}^{(2 n)}\\right] \\in R^{n \\times 2 n} \\quad\\left(\\boldsymbol{\\phi}^{(i)} \\in R^{n \\times 1}\\right)最终解出$X(t)$的表达式为： \\boldsymbol{X}(t)=\\boldsymbol{\\Phi} \\boldsymbol{Z}(t)=\\sum_{i=1}^{2 n} \\boldsymbol{\\phi}_{i} z_{i}(t)=\\sum_{i=1}^{2 n} \\frac{e^{\\lambda_{i} t}}{\\hat{m}_{p i}} \\boldsymbol{\\phi}_{i} \\boldsymbol{\\phi}_{i}^{T}\\left(\\lambda_{i} \\boldsymbol{M} \\boldsymbol{X}_{0}+\\boldsymbol{M}_{0}\\dot{\\boldsymbol{X_0}}+{\\boldsymbol{C}}{\\boldsymbol{X}}_{0}\\right) 复模态不再具有主模态的意义，而是具有相位关系的振型，实部代表衰减率，虚部代表衰减系统的固有频率。 其模长才是真正的固有频率。 复模态分析的是一阶的状态方程，并且维度扩大了一倍。 典型例题 这里首先求出在正常情况下系统的固有频率和模态，注意这里解出的固有特征根和模态都是复数的形式，其真正的固有频率为特征根的模长。 之后，根据系统对初始状态下的响应可得，代入到刚才求出的表达式中。 \\boldsymbol{X}(t)=\\sum_{i=1}^{2 n} \\frac{e^{\\lambda_{i} t}}{\\hat{m}_{p i}} \\boldsymbol{\\phi}_{i} \\boldsymbol{\\phi}_{i}^{T}\\left(\\lambda_{i} \\boldsymbol{M} \\boldsymbol{X}_{0}+\\boldsymbol{M}_{0}\\dot{\\boldsymbol{X_0}}+{\\boldsymbol{C}}{\\boldsymbol{X}}_{0}\\right)\\\\ \\boldsymbol{X}(t)=\\sum_{i=1}^{4} \\frac{e^{\\lambda_{i} t}}{\\hat{m}_{p i}} \\boldsymbol{\\phi}_{i} \\boldsymbol{\\phi}_{i}^{T} \\boldsymbol{M} \\dot{\\boldsymbol{X}}_{0}=\\sum_{i=1}^{4} e^{\\lambda_{, t}} \\frac{\\left(\\boldsymbol{\\phi}_{i}^{T} \\boldsymbol{M} \\dot{\\boldsymbol{X}}_{0}\\right)}{\\hat{m}_{p i}} \\boldsymbol{\\phi}_{i} 2和4由共轭关系可以直接推导出： 虚部之间相互抵消可得最后的结果表达式。 使用忽略非对角项方法求解：","link":"/2022/03/15/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E6%9C%89%E9%98%BB%E5%B0%BC%E7%9A%84%E5%A4%9A%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F/"},{"title":"第五章 线性振动的近似计算方法 传递矩阵法","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是线性振动的近似计算方法中传递矩阵法，主要从圆盘的扭转振动和梁的横向振动来介绍，主要的思想就是将结构转化成为基本的集中质量的结构，进而求解传递矩阵，最后利用边界条件来求解固有频率。 传递矩阵法适用于计算链状结构的固有频率和主振型 多个圆盘的扭振，连续梁，气轮机和发电机的转轴系统 特征：可简化为无质量的梁上带有若干个集中质量的横向振动 如将汽轮机轴的质量简化到轮盘上面。 特点：将链状结构划分为一系列单元，每对单元之间的传递矩阵的阶数等于单元的运动微分方程的阶数，因此传递矩阵法对全系统的计算分解为阶数很低的各个单元的计算，然后加以综合，从而大大减少计算工作量 圆盘的扭转振动 轴不计质量，只计刚度，圆盘和轴自左至右编号 第 i-1 个和第 i 个圆盘以及连接两盘的轴段构成第 i 个单元 $J_{i-1}、J_{i}$ 圆盘转动惯量；$l_i$：轴段长度 ；$k_i$：轴段扭转刚度 \\begin{aligned} {\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{i}^{R} } &=\\left[\\begin{array}{cc}1 & 0 \\\\ -\\omega^{2} J_{i} & 1\\end{array}\\right]\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{i}^{L} \\quad \\boldsymbol{X}_{i}^{R}=\\boldsymbol{S}_{i}^{P} \\boldsymbol{X}_{i}^{L} \\\\ \\boldsymbol{S}_{i}^{P} &=\\left[\\begin{array}{cc}1 & 0 \\\\ -\\omega^{2} J_{i} & 1\\end{array}\\right] \\quad \\text { 点传递矩阵 } \\end{aligned}其中，$\\boldsymbol{X}=(\\theta,T)^T$为状态的变量。 第 i 个轴段来分析： \\begin{aligned} {\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{i}^{L} } &=\\left[\\begin{array}{cc}1 & 1/k_i \\\\ 0 & 1\\end{array}\\right]\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{i-1}^{R} \\quad \\boldsymbol{X}_{i}^{L}=\\boldsymbol{S}_{i}^{F} \\boldsymbol{X}_{i-1}^{R} \\\\ \\boldsymbol{S}_{i}^{F} &=\\left[\\begin{array}{cc}1 & 1/k_i \\\\ 0 & 1\\end{array}\\right] \\quad \\text { 场传递矩阵 } \\end{aligned} 第 i-1 个圆盘右侧到第 i 个圆盘右侧的状态变量传递关系： \\boldsymbol{X}_{i}^{R}=\\boldsymbol{S}_{i}^{P} \\boldsymbol{S}_{i}^{F} \\boldsymbol{X}_{i-1}^{R}=\\boldsymbol{S}_{i} \\boldsymbol{X}_{i-1}^{R} \\\\ S_{i}=S_{i}^{P} S_{i}^{F} \\Rightarrow 单元传递矩阵\\\\\\boldsymbol{S}_{i}=\\boldsymbol{S}_{i}^{P} \\boldsymbol{S}_{i}^{F}=\\left[\\begin{array}{cc}1 & 0 \\\\ -\\omega^{2} J_{i} & 1\\end{array}\\right]\\left[\\begin{array}{cc}1 & 1 / k_{i} \\\\ 0 & 1\\end{array}\\right]=\\left[\\begin{array}{cc}1 & 1 / k_{i} \\\\ -\\omega^{2} J_{i} & 1-\\omega^{2}\\left(J_{i} / k_{i}\\right)\\end{array}\\right]n 个圆盘的轴系，最左端和最右端状态变量传递关系：$\\boldsymbol{X}_n^R=S\\boldsymbol{X}_1^R$ S：第1至第n单元通路中所有单元传递矩阵的连乘积 $S=S_nS_{n-1}…S_1 \\ (\\omega)$的函数。 最后再去利用两端边界条件可确定固有频率和模态。 第一个圆盘左端状态：$\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{1}^{L} =\\left[\\begin{array}{c}1 \\\\ 0 \\end{array}\\right]$ 第一个圆盘右端状态：$\\left[\\begin{array}{l}\\theta \\\\ T\\end{array}\\right]_{1}^{R} =\\left[\\begin{array}{cc}1 &amp; 0 \\\\ -\\omega^{2} J &amp; 1\\end{array}\\right]\\left[\\begin{array}{c}1 \\\\ 0 \\end{array}\\right]=\\left[\\begin{array}{c}1 \\\\ -\\omega^2J \\end{array}\\right]$ 梁的横向弯曲振动系统 传递矩阵法可用于分析梁的横向弯曲振动； 梁上有 n-1 个集中质量，即只考虑质量，而不考虑其形状 ；梁段质量不计，只计刚度； 支座、梁段、集中质量自左向右分别编号； 第 i-1 个和第 i 个质量以及连接两质量的梁段构成第 i 个单元 梁段长$l_i$，抗弯刚度 $E_iI_i$，集中质量$m_{i-1}$、$m_i$ 状态变量构成：$\\boldsymbol{X} = (y \\quad \\theta\\quad M\\quad F_s)^T$ 分别对应的是：集中质量处梁的横向位移(挠度)、截面转角、弯矩和剪力 这几个是有一定对应的关系的，其中第一个$\\theta=\\partial y/\\partial x$ ，$F_s=\\partial M /\\partial x$ ，弯矩M和挠度y是二阶曲率的关系。 第i个质量受力分析 由于几种质量，因此可以不考虑形状特征，此时的挠度、转角和弯矩是相同的，但是由于其质量无法无法忽略，因此其存在惯性力。 当系统以频率 $\\omega$ 作简谐振动时 ：$\\ddot y_i=-\\omega^2y_i$ 质量左右两侧的传递关系：$\\boldsymbol{X}_{i}^{R}=\\boldsymbol{S}_{i}^{P} \\boldsymbol{X}_{i}^{L}$(点传递矩阵) \\left[\\begin{array}{c}y \\\\ \\theta \\\\ M \\\\ F_{s}\\end{array}\\right]_{i}^{R}=\\left[\\begin{array}{cccc}1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ \\omega^{2} m_{i} & 0 & 0 & 1\\end{array}\\right]\\left[\\begin{array}{c}y \\\\ \\theta \\\\ M \\\\ F_{s}\\end{array}\\right]_{i}^{L} \\quad第 i 个梁段受力分析 由于两端没有质量，因此平衡的条件是两端的剪力相同。 同时，材料力学和振动力学涉及的都是微小的变形，微振。 对于转角，由材料力学有： \\begin{aligned}\\theta_{i}(x) &=\\theta_{i-1}^{R}+\\frac{1}{E_{i} I_{i}} \\int_{0}^{x} M_{i}(x) d x \\\\&=\\theta_{i-1}^{R}+\\frac{1}{E_{i} I_{i}} M_{i-1}^{R} x+\\frac{1}{2 E_{i} I_{i}} F_{s, i-1}^{R} x^{2}\\end{aligned}对于挠度，有材料力学得: y_{i}(x)=y_{i-1}^{R}+\\int_{0}^{x} \\theta_{i}(x) d x \\\\=y_{i-1}^{R}+\\theta_{i-1}^{R} x+\\frac{1}{2 E_{i} I_{i}} M_{i-1}^{R} x^{2}+\\frac{1}{6 E_{i} I_{i}} F_{s, i-1}^{R} x^{3}于是令$\\theta_i(x)^L,\\ y_i(x)^L,\\ M_i(x)^L$ 中的$x=l_i$： F_{s,i}^L=F_{s,i-1}^R\\\\ M_i^L=M_i^R+F_{s,i-1}^Rl_i\\\\ \\theta_{i}^L =\\theta_{i-1}^{R}+\\frac{1}{E_{i} I_{i}} M_{i-1}^{R}l_i+\\frac{1}{2 E_{i} I_{i}} F_{s, i-1}^{R} {l_i}^{2}\\\\ y_{i}^L=y_{i-1}^{R}+\\theta_{i-1}^{R} l_i+\\frac{1}{2 E_{i} I_{i}} M_{i-1}^{R} {l_i}^{2}+\\frac{1}{6 E_{i} I_{i}} F_{s, i-1}^{R} {l_i}^{3}结合梁段受力平衡方程，第 i 个梁段左右两端状态变量的传递关系： 第 i -1 个质量右侧至第 i个质量右侧的状态变量传递关系： \\boldsymbol{X}_{i}^{R}=\\boldsymbol{S}_{i}^{P} \\boldsymbol{S}_{i}^{F} \\boldsymbol{X}_{i-1}^{R}=\\boldsymbol{S}_{i} \\boldsymbol{X}_{i-1}^{R} \\\\ S_{i}=S_{i}^{P} S_{i}^{F} \\Rightarrow 单元传递矩阵\\\\对于带 n 个集中质量得梁，总能利用各单元传递矩阵的连乘积导出梁的最左端和最右端状态变量传递关系：$\\boldsymbol{X}_n^R=S\\boldsymbol{X}_1^R$ 最后两端边界条件可确定固有频率和振型 本节主要关注： 一个初始条件：先对模型进行分段，将确定两边的边界条件。 两次受力分析：一次对集中质量，一次对轴段和两端。 三个传递矩阵：点传递矩阵、场传递矩阵、单元传递矩阵。 引入无量纲量的目的是方便计算 之后使用传递矩阵法，计算每个两端和集中质量处的状态变量的值，最后我们将两边的结果代入即可。 两支座之间的状态关系：$ \\boldsymbol{X}_{3}^{L}=\\overline{\\boldsymbol{S}}_{0}^{R} \\quad \\overline{\\boldsymbol{S}}=\\overline{\\boldsymbol{S}}_{3}^{F} \\overline{\\boldsymbol{S}}_{2}^{P} \\overline{\\boldsymbol{S}}_{2}^{F} \\overline{\\boldsymbol{S}}_{1}^{P} \\overline{\\boldsymbol{S}}_{1}^{F} $ 在初始条件下 $y_0^R=0,\\ M_0^R=0$ 代入得到 $y_3^L,M_3^R$ 的表达式，在代入$y_3^L=0,\\ M_3^L=0$ \\overline{\\boldsymbol{S}}=\\left[\\begin{array}{llll}\\alpha_{11} & \\alpha_{12} & \\alpha_{13} & \\alpha_{14} \\\\ \\alpha_{21} & \\alpha_{22} & \\alpha_{23} & \\alpha_{24} \\\\ \\alpha_{31} & \\alpha_{32} & \\alpha_{33} & \\alpha_{34} \\\\ \\alpha_{41} & \\alpha_{42} & \\alpha_{43} & \\alpha_{44}\\end{array}\\right]\\Rightarrow\\left(\\begin{array}{c}\\bar{y} \\\\ \\theta \\\\ \\bar{M} \\\\ \\overline{\\bar{F}}\\end{array}\\right)_{3}^{L}=\\left[\\begin{array}{llll}\\alpha_{11} & \\alpha_{12} & \\alpha_{13} & \\alpha_{14} \\\\ \\alpha_{21} & \\alpha_{22} & \\alpha_{23} & \\alpha_{24} \\\\ \\alpha_{31} & \\alpha_{32} & \\alpha_{33} & \\alpha_{34} \\\\ \\alpha_{41} & \\alpha_{42} & \\alpha_{43} & \\alpha_{44}\\end{array}\\right]\\left(\\begin{array}{c}\\bar{y} \\\\ \\theta \\\\ \\bar{M} \\\\ \\overline{\\bar{F}}\\end{array}\\right)_{0}^{R}两端支座边界条件: \\left\\{\\begin{array}{l}\\alpha_{12} \\theta_{0}+\\alpha_{14} \\bar{F}_{s 0}=0 \\\\ \\alpha_{32} \\theta_{0}+\\alpha_{34} \\bar{F}_{s 0}=0\\end{array}\\right. 非零解条件: $\\quad \\Delta \\lambda(\\omega)=\\left|\\begin{array}{ll}\\alpha_{12} &amp; \\alpha_{14} \\\\ \\alpha_{32} &amp; \\alpha_{34}\\end{array}\\right|=0 \\quad $得出频率方程，求解可得系统的固有频率。 也可利用特征方程，采用数值方法求解固有频率。 频率和模态是由边界决定，因为手动在这些连续体上面的传递可以看作是一种波的传递过程，这就是传递矩阵法的意义所在。 下面看一个悬臂梁的例子。 边界条件： 最左边贴近固定端的一侧，其挠度和转角为0，允许有弯矩和剪力； 对于自由端，由于没有约束其转角和挠度不为0，剪力和弯矩由于没有相关的约束，因此为0。 最后的结果是第一阶模态的准确性还是可以的，但是第二阶就不准确了。 原因是对于之前的简支梁来说，其左右两边是被固定住的，因此肯定是无法发生位移的，集中质量假设对该模型来说影响不大，即简化前后的模态大体上是相近的情况。 但是对于悬臂梁而言，其有一个自由端，之前是一个均匀的悬臂梁的振型和集中质量简化后的求出的模态有在自由端上出现差异。","link":"/2022/03/15/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%BA%94%E7%AB%A0-%E7%BA%BF%E6%80%A7%E6%8C%AF%E5%8A%A8%E7%9A%84%E8%BF%91%E4%BC%BC%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95-%E4%BC%A0%E9%80%92%E7%9F%A9%E9%98%B5%E6%B3%95/"},{"title":"第六章 连续系统的振动  一维波动方程","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是连续系统振动的一维波动方程部分，主要从动力学方程、固有频率和模态函数、常见的边界、主振型和正交性、杆的纵向受迫振动以及横向振动的典型例题等方面就行讲解。 本章主要的假设：1）本章讨论的连续体都假定为线性弹性体，即在弹性范围内服从虎克定律2）材料均匀连续；各向同性3）振动为微振 实际振动系统都是连续体，具有连续分布的质量与弹性，又称连续系统或分布参数系统。 确定连续体上无数质点的位置需要无限多个坐标，因此连续体是具有无限多自由度的系统。 连续体的振动要用时间和空间坐标的函数来描述，其运动方程不再像有限多自由度系统那样是二阶常微分方程组，它是偏微分方程。 在物理本质上，连续体系统和多自由度系统没有什么差别，连续体振动的基本概念与分析方法与有限多自由度系统是完全类似的。 动力学方程这里对于简单的系统来说是可以将上述的三种运动进行解耦分析的，但是对于大推力火箭而言，三种振动相互耦合，不可分开。 杆的纵向振动 $p(x,t)$：为单位长度杆上分布的纵向分布力 假定振动过程中各横截面仍保持为平面，忽略由纵向振动引起的横向变形 航天里面的悬臂结构，在向外伸长的时候，会产生一定的横向振动。 注意在太空中阻尼是可以忽略的 这里的$\\dfrac {\\partial^2u} {\\partial t^2} $ 代表的是微端的加速度；$\\rho Sdx$ 代表的是微端的质量。 横截面上内力：$ F=E S \\varepsilon=E S \\dfrac{\\partial u}{\\partial x} $ 达朗贝尔原理：$ \\rho S d x \\dfrac{\\partial^{2} u}{\\partial t^{2}}=\\left(F+\\dfrac{\\partial F}{\\partial x} d x\\right)-F+p(x, t) d x $ 这里我们使用的是最一般分布力来阐述，因为集中力可以看成是分布力的特殊情况。$F(t)=F_0\\delta(x-x_0)$ ，其中$x_0$为集中力的作用位置。 杆的纵向强迫振动方程为： \\rho S d x \\dfrac{\\partial^{2} u}{\\partial t^{2}}=\\left(F+\\dfrac{\\partial F}{\\partial x} d x\\right)-F+p(x, t) d x \\\\ \\rho S \\frac{\\partial^{2} u}{\\partial t^{2}}=\\frac{\\partial}{\\partial x}\\left(E S \\frac{\\partial u}{\\partial x}\\right)+p(x, t)等直杆ES 为常数： \\frac{\\partial^{2} u}{\\partial t^{2}}=a_{0}^{2} \\frac{\\partial^{2} u}{\\partial x^{2}}+\\frac{1}{\\rho S} p(x, t)其中，$a_0=\\sqrt {E/\\rho}$ 为弹性纵波沿杆的纵向传播速度 弦的横向振动 注意，这里是约等于，因为我们采用了$\\sin\\theta \\approx\\theta$ 的小角近似。 同理可得，弦的横向强迫振动方程： \\frac{\\partial^{2} y}{\\partial t^{2}}=a_{0}^{2} \\frac{\\partial^{2} y}{\\partial x^{2}}+\\frac{1}{\\rho } p(x, t)其中，$y(x,t)$：为弦上x 处横截面t 时刻的横向位移； $a_0$ ：为弹性横波的纵向传播速度 $\\rho$ ：为单位长度弦质量 如传送带皮带，由于传送带的纵向的运动会引起横向的振动问题。 轴的扭转振动 其中，T：为截面处的扭矩；$\\rho I_pdx$：为微端绕着轴线的转动惯量。 达朗贝尔原理： \\begin{array}{c}\\rho I_{p} d x \\dfrac{\\partial^{2} \\theta}{\\partial t^{2}}=\\left(\\mathbf{X}+\\dfrac{\\partial T}{\\partial x} d x\\right)-\\mathbf{K}+p d x \\\\\\rho I_{p} \\dfrac{\\partial^{2} \\theta}{\\partial t^{2}}=\\dfrac{\\partial T}{\\partial x}+p(x, t)\\end{array}材料力学中扭矩的公式可得： $T=G I_{p} \\dfrac{\\partial \\theta}{\\partial x} $ \\rho I_{p} \\frac{\\partial^{2} \\theta}{\\partial t^{2}}=\\frac{\\partial}{\\partial x}\\left(G I_{p} \\frac{\\partial \\theta}{\\partial x}\\right)+p(x, t)圆截面杆的扭转振动强迫振动方等直杆, 抗扭转刚度 $G I_{p} $ 为常数 \\frac{\\partial^{2} \\theta}{\\partial t^{2}}=a_{0}^{2} \\frac{\\partial^{2} \\theta}{\\partial x^{2}}+\\frac{1}{\\rho_{p}} p(x, t)其中，$a_0=\\sqrt{\\dfrac{ G} \\rho}$ ：为剪切弹性波的纵向传播速度 小结： 虽然它们在运动表现形式上并不相同，但它们的运动微分方程是类同的，都属于一维波动方程，因此分析得到的一些性质是可以共用的。 固有频率和模态函数 这里我们简化首先考虑自由振动的情况，此时设外部力的作用为0： \\frac{\\partial^{2} u}{\\partial t^{2}}=a_{0}^{2} \\frac{\\partial^{2} u}{\\partial x^{2}}假设杆的各点作同步运动：$u(x,t)=\\phi(x)q(t)$ 其中$q(t)$：为运动规律的时间函数； $\\phi(x)$：为杆上矩原点x处的截面的纵向振幅 \\frac{\\ddot{q}(t)}{q(t)}=a_{0}^{2} \\frac{\\phi^{\\prime \\prime}(x)}{\\phi(x)}=-\\lambda 解得： q(t)=a \\sin (\\omega t+\\theta) \\quad \\phi(x)=c_{1} \\sin \\frac{\\omega x}{a_{0}}+c_{2} \\cos \\frac{\\omega x}{a_{0}}其中，$c_1,c_2,\\omega$ 由杆的边界条件确定 ,$\\phi(x)$确定杆纵向振动的形态，称为振型。 杆的边界条件确定固有频率。 与有限自由度系统不同，连续系统的模态为坐标的连续函数，表示各坐标振幅的相对比值，由频率方程确定的固有频率$\\omega_i$有无穷多个 这里，固有频率和振动模态是一一对应的。 \\omega_i \\longleftrightarrow \\phi_i(x)第 i 阶主振动为：$u^{(i)}(x, t)=a_{i} \\phi_{i}(x) \\sin \\left(\\omega_{i} t+\\theta_{i}\\right), \\quad(i=1,2 \\cdots)$ 系统的自由振动是无穷多个主振动的叠加：$u(x, t)=\\sum_\\limits{i=1}^{\\infty} a_{i} \\phi_{i} \\sin \\left(\\omega_{i} t+\\theta_{i}\\right)$ 几种常见边界条件下的固有频率和模态函数固有频率的模态是由边界决定的。 求解频率方程可得：$\\omega_i=\\dfrac {i\\pi a_0}{l} \\quad (i=0,1,2,…)$ 无穷多组解。 此时的 i=0，对应的是刚体模态，但是题目两边是固定的，一定存在力，因此这个情况是不存在的。 将$\\omega_i$ 代入振型函数：$\\phi_i(x)=c_i\\sin\\dfrac {i\\pi x}{l} \\quad (i=0,1,2,…)$ 由于零固有频率对应的振型函数为零，因此零固有频率除去，但是对于导弹这种两边可以自由振动的系统，就可以存在这种频率为0的情况，此时对应的是刚体运动。 求解频率方程可得：$\\omega_i=\\dfrac {i\\pi a_0}{l} \\quad (i=0,1,2,…)$ 无穷多组解。 此时的 i=0，对应的是刚体模态。 将$\\omega_i$ 代入振型函数：$\\phi_i(x)=c_i\\cos\\dfrac {i\\pi x}{l} \\quad (i=0,1,2,…)$ 频率方程和固有频率两端固定杆的情况相同，零固有频率对应的常值振型为杆的纵向刚性位移。 边界条件中的第一行代表物理的边界，之后的第二行代表模态的边界。 求解频率方程可得：$\\omega_i=\\dfrac {(2i-1)\\pi a_0}{2l} \\quad (i=1,2,…)$或 $\\omega_i=\\dfrac {i\\pi a_0}{2l} \\quad (i=1,3,…)$ 无穷多组解。 将$\\omega_i$ 代入振型函数：$\\phi_i(x)=c_i\\sin\\dfrac {(2i-1)\\pi x}{2l} \\quad (i=1,2,…)$ 或 $\\phi_i(x)=c_i\\sin\\dfrac {i\\pi x}{2l}\\quad (i=1,3,…)$ 例：推导系统的频率方程 首先令系统的振动方程为：$u(x,t)=\\phi(x)q(t) \\quad \\phi(x)=c_{1} \\sin \\dfrac{\\omega x}{a_{0}}+c_{2} \\cos \\dfrac{\\omega x}{a_{0}}$ 代入边界条件可得： 对于左边界而言，其位移为0： u(0, t)=0 \\Rightarrow \\phi(0)=0 \\Rightarrow c_{2}=0对于去右边界，其受到的轴向力用弹簧的形变来抵消： k u(l, t)=-E S \\frac{\\partial u}{\\partial x}(l, t) \\Rightarrow k \\phi(l)=-E S \\frac{\\partial \\phi}{\\partial x}(l, t)\\\\ \\Rightarrow k \\sin \\frac{\\omega l}{a_{0}}=-E S \\frac{\\omega}{a_{0}} \\cos \\frac{\\omega l}{a_{0}}\\\\即得到频率方程为： \\Rightarrow \\frac{\\operatorname{tg}\\left(\\omega l / a_{0}\\right)}{\\omega l / a_{0}}=-\\frac{E S}{k l} \\equiv \\text { 常数 } 同样的，首先确定振动方程，之后代入到边界条件： u(0, t)=0\\\\M \\frac{\\partial^{2} u}{\\partial t^{2}}(l, t)=-E S \\frac{\\partial u}{\\partial x}(l, t) 总结： 连续体的频率和模态是由边界条件决定的，而多自由度问题的频率和模态是由其质量阵和刚度阵的特征根问题。 N自由度系统中有N个频率和N个模态，但是对于连续体问题的频率和模态是无穷的。 边界问题，上边讲述了三个简单的边界和两个复杂边界。 主振型的正交性只对具有简单边界条件的杆讨论主振型的正交性，杆可以是变截面或匀截面的 变截面：质量密度及截面积S等都可以是x 的函数 动力方程 : \\rho S \\dfrac{\\partial^{2} u}{\\partial t^{2}}=\\dfrac{\\partial}{\\partial x}\\left(E S \\dfrac{\\partial u}{\\partial x}\\right)+p(x, t) 这里由于是变截面的情况，因此将S放大偏微号之内。 自由振动方程： $ \\rho S \\dfrac{\\partial^{2} u}{\\partial t^{2}}=\\dfrac{\\partial}{\\partial x}\\left(E S \\dfrac{\\partial u}{\\partial x}\\right)$ 主振动 ：这里使用分离变量法 u(x, t)=\\phi(x) a \\sin (\\omega t+\\theta) \\\\\\left(E S \\phi^{\\prime}\\right)^{\\prime}=-\\omega^{2} \\rho S \\phi 这里可以对比多自由度系统的特征方程：$|K-\\omega^2M|=0$ 这里的$\\rho S$代表的是微元的质量，$ES$ 代表刚度微元项。 乘 $ \\phi_{j}(x) $ 并沿杆长积分: 由于这里的S可能是变截面的，因此S不能拿出来。 \\int_{0}^{l} \\phi_{j}\\left(E S \\phi_{i}^{\\prime}\\right)^{\\prime} d x=-\\omega_{i}^{2} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x分部积分: \\int_{0}^{l} \\phi_{j}\\left(E S \\phi_{i}^{\\prime}\\right)^{\\prime} d x=\\left.\\phi_{j}\\left(E S \\phi_{i}\\right)\\right|_{0} ^{l}-\\int_{0}^{l} E S \\phi_{i}^{\\prime} \\phi_{j}^{\\prime} d x 对于第一项而言，总有第一项为0，因为当其是简答边界时，第一项中的$\\phi_j$为0，但当其为自由边界时，$ES\\phi_i$为0，所以第一项总是0。 任一端上总有 $\\phi=0$ 或 $\\phi^{\\prime}=0$ 成立 \\quad \\int_{0}^{l} E S \\phi_{i}^{\\prime} \\phi_{j}^{\\prime} d x=\\omega_{i}^{2} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} dx 杆的主振型关于质量的正交性： \\left(\\omega_{i}^{2}-\\omega_{j}^{2}\\right) \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=0 \\\\i \\neq j \\text { 时 } \\ \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=0 \\Rightarrow\\omega_{i} \\neq \\omega_{j}杆的主振型关于刚度的正交性： \\int_{0}^{l} E S \\phi_{i}^{\\prime} \\phi_{j}^{\\prime} d x=-\\int_{0}^{l} \\phi_{j}\\left(E S \\phi_{i}^{\\prime}\\right)^{\\prime} d x=0 \\quad(i \\neq j) 当 $ i=j $ 时, 设第 i 阶模态主质量为$m_{pi}$ ： \\int_{0}^{l} \\rho S \\phi_{i}^{2} d x=m_{p i} 第 i 阶模态主刚度为$k_{pi}$： \\int_{0}^{l} E S\\left(\\phi_{i}^{\\prime}\\right)^{2} d x=-\\int_{0}^{l} \\phi_{i}\\left(E S \\phi_{i}^{\\prime}\\right)^{\\prime} d x=k_{p i}第 i 阶固有频率为 $\\omega_{i}$：$\\omega_{i}^{2}=k_{p i} / m_{p i}$ 对比多自由度系统的主模态和正则模态可得： 主振型归一化得出正则振型: $\\int_{0}^{l} \\rho S \\phi_{i}^{2} d x=m_{p i}=1$ 这里归一化的对象是模态函数前面的那个待定系数$c_1、c_2$ $\\phi(x)=c_{1} \\sin \\dfrac{\\omega x}{a_{0}}+c_{2} \\cos \\dfrac{\\omega x}{a_{0}}$ 则第 i 阶主刚度：$k_{p i}=\\omega_{i}^{2}$ 合写为: \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=\\delta_{i j} \\quad\\int_{0}^{l} E S \\phi_{i}^{\\prime} \\phi_{j}^{\\prime} d x=\\omega_{i}^{2} \\delta_{i j} \\\\ \\int_{0}^{l} \\phi_{i}\\left(E S \\phi_{j}^{\\prime}\\right)^{\\prime} d x=-\\omega_{i}^{2} \\delta_{i j} \\quad \\delta_{i j}=\\left\\{\\begin{array}{ll}1 & i=j \\\\ 0 & i \\neq j\\end{array}\\right.杆的纵向强迫振动 注意：这里的初始时刻是一个函数方程，表示初始状态下的形状分布函数。 其次，这里的$q_i(x)$不可以写成正弦了，因为现在讨论的是强迫振动。 \\rho S \\sum_{i=1}^{\\infty} \\phi_{i} \\ddot{q}=\\sum_{i=1}^{\\infty}\\left(E S \\phi_{i}^{\\prime}\\right)^{\\prime} q_{i}+p(x, t) 这里由于$q_i$是时间的函数，因此可以放到等式的外面。 并且由于正交性，$i \\ne j$的项全部被抵消了。 第j个正则模态坐标广义力：$\\ddot{q}_{j}+\\omega_{j}^{2} q_{j}=Q_{j}(t)$ 模态坐标初始条件求解 ： \\left \\{\\begin{array}{l} u(x, 0)=f_{1}(x)=\\sum_\\limits{i=1}^{\\infty} \\phi_{i}(x) q_{i}(0) \\\\ \\left.\\dfrac{\\partial u}{\\partial t}\\right|_{t=0}=f_{2}(x)=\\sum_\\limits{i=1}^{\\infty} \\phi_{i}(x) \\dot{q}_{i}(0)\\end{array}\\right.乘 $ \\rho S \\phi_{j}(x) $ 并沿杆长对 ( x ) 积分, 由正交性条件： \\left\\{\\begin{array}{ll}q_{j}(0)=\\int_{0}^{l} \\rho S f_{1}(x) \\phi_{j}(x) d x & \\text { 求得 } q_{j}(t) \\text { 后 } \\\\\\dot{q}_{j}(0)=\\int_{0}^{l} \\rho S f_{2}(x) \\phi_{j}(x) d x & \\text { 可得 } u(x, t)\\end{array}\\right.\\\\q_{j}(t)=q_{j}(0) \\cos \\omega_{j} t+\\frac{\\dot{q}_{j}(0)}{\\omega_{j}} \\sin \\omega_{j} t+\\frac{1}{\\omega_{j}} \\int_{0}^{l} Q_{j}(t) \\sin \\omega_{j}(t-\\tau) d \\tau 最后对初始条件的响应进行杜哈梅积分。 上面考虑的都是分布力作用时的情况，现在讨论集中力的情况： 当受到的力为集中力时：$p(x, t)=P(t) \\delta(x-\\xi)$ 正则坐标广义力： Q_{j}(t)=\\int_{0}^{l} P(t) \\delta(x-\\xi) \\phi_{j}(x) d x=P(t) \\phi_{j}(\\xi)横向振动的典型例题例：等直杆，自由端作用有：$P(t)=P_0\\sin\\omega t$ ，其中$P_0$为常数，求：杆的纵向稳态响应。 对于强迫振动还包括自由伴随，但是最后都会随着时间的推移而慢慢消失。这里求解的值稳态的情况，因此直接考虑稳态即可。 注意以下归一化，归一化的参数时模态方程的系数$C_i$ ，因此正则化的作用无非是确定模态方程的前面待定系数。 当外部力频率等于杆的任一阶固有频率时都会发生共振现象 基本的步骤： 由边界条件得到频率和模态函数； 求解模态广义力，即求解力在模态空间的表达式； 求解模态空间的解，即求解模态坐标$q_i(t)$，转换到物理空间。 例：一均质杆两端固定。假定在杆上作用有两个集中力，如图所示，试问：当这些力突然移去时，杆将产生什么样的振动？ 由于力去掉之后没有外部激励了，因此其振动的形式一定是简谐振动。 通过边界条件，即两边为固定端，可得其频率方程与位移方程。 边界条件上，在0时刻位移有一个分布函数，其实就是对应的应变分布。 系统的自由振动是无穷多个主振动的叠加： \\begin{aligned} u(x, t) &=\\sum_{i=1}^{\\infty} a_{i} \\phi_{i}(x) \\sin \\left(\\omega_{i} t+\\theta_{i}\\right) \\\\ &=\\sum_{i=1}^{\\infty}\\left\\{\\sin \\frac{i \\pi x}{l}\\left[B_{1 i} \\cos \\frac{i \\pi a_{0}}{l} t+B_{2 i} \\sin \\frac{i \\pi a_{0}}{l} t\\right]\\right\\} \\end{aligned}应用位移的初始条件 $u(x,0)=f(x)$ 代入到上述的方程中可得： f(x)=\\sum_\\limits{i=1}^{\\infty} B_{1i}\\sin \\frac{i \\pi x}{l}两边乘 $\\rho S\\phi_j(x)$ 并沿杆长积分，然后利用正交性条件： B_{1i}=\\frac 2 l \\int_0^1 f(x)\\sin \\frac{i \\pi x}{l}应用速度初始条件可得：$B_{2i}=0$ \\begin{aligned} B_{1 i}=& \\frac{2}{l} \\int_{0}^{l} f(x) \\sin \\frac{i \\pi x}{l} d x \\\\=& \\frac{2}{l} \\varepsilon_{0}\\left[\\int_{0}^{l / 4} x \\sin \\frac{i \\pi x}{l} d x+\\int_{l / 4}^{3 l / 4}\\left(\\frac{l}{2}-x\\right) \\sin \\frac{i \\pi x}{l} d x\\right.\\left.+\\int_{3 l / 4}^{l}(l-x) \\sin \\frac{i \\pi x}{l} d x\\right] \\\\=& \\frac{P_{0} l}{i^{2} \\pi^{2} E S}(-1)^{(i-2) / 4} \\qquad (i=2,6,10, \\ldots) \\end{aligned}将上述的待定系数，代入到物理空间的方程中解得系统的响应为： \\begin{aligned} u(x, t) &=\\sum_{i=1}^{\\infty}\\left\\{\\sin \\frac{i \\pi x}{l}\\left[B_{1 i} \\cos \\frac{i \\pi a_{0}}{l} t+B_{2 i} \\sin \\frac{i \\pi a_{0}}{l} t\\right]\\right\\} \\\\ &=\\frac{P_{0} l}{\\pi^{2} E S} \\sum_{i=2,6,10, \\ldots}^{\\infty} \\frac{(-1)^{(i-2) / 4}}{i^{2}} \\sin \\frac{i \\pi x}{l} \\cos \\frac{i \\pi a_{0}}{l} t \\end{aligned}这里，第二道例题是自由响应，因此可以将$q_i(t)=a_i\\sin(\\omega_it+\\theta_i)$ 来进行表示，代入边界条件求解待定系数，这种方法称为直接法。 相反，第一道例题的是受迫振动，无法使用直接法代入模态坐标$q_i(t)$的表达式，需要建立正则方程去求解。 思考题： 有一根以常速度v沿x轴运动的杆。如果杆的中点处突然被卡住停止，试求出所产生的自由振动表达式。 在此种情况下，可从杆的中点分开，分开的左右两部分的振动形式相同，因此只分析右半部分即可 边界条件：右半部分为一端固定、另一端自由的杆 杆的自由振动方程：$\\dfrac{\\partial^{2} u}{\\partial t^{2}}=a_{0}^{2} \\dfrac{\\partial^{2} u}{\\partial x^{2}}+\\dfrac{1}{\\rho S} p(x, t) \\quad a_0=\\sqrt{\\dfrac E \\rho}$ 边界条件为：$u(0,t)=0, \\quad \\left. \\dfrac{\\partial u}{\\partial x}\\right|_{x=\\frac l 2}=0$ 初始条件为：$u(x,0)=0, \\quad \\left. \\dfrac{\\partial u}{\\partial t}\\right|_{t=0}=0$ 例：有一根x=0 端为自由、x=l 端处为固定的杆，固定端承受支撑运动 $u_g(t)=d\\sin\\omega t$，d 为振动的幅值，试求杆的稳态响应 这里基础存在位移，参考基在动，因此这里的受力分析和之前不同。 此时$u^*$才是真正的杆的位移。 代入方程可得：$\\rho S\\ddot u^*-ES\\ddot u^*=-\\rho S\\ddot u_g=-\\rho Sd\\omega^2\\sin\\omega t$ 这里，尽管相对的参考系有所变化，但是由于边界条件不变，因此最后解得模态和频率不变，设为 u^{*}=\\sum_\\limits{i=1}^{\\infty} \\phi_{i}(x) q_{i}(t)\\\\ \\phi_{i}(x)=\\sqrt{\\frac{2}{l}} \\cos \\frac{i \\pi}{2 l} x, \\quad i=1,3,5, \\ldots其中 $\\phi_i(x)$为归一化的正则模态函数，$q_i(t)$ 为模态坐标， $u^{*}=u-u_{g}$ 将其代入到方程中，求解模态坐标去可得： \\sum_{i=1,3,5, \\ldots}^{\\infty}\\left(\\rho S \\ddot{q}_{i} \\phi_{i}-E S q_{i} \\phi_{i}\\right)=\\rho S d \\omega^{2} \\sin \\omega t用 $ \\phi_{j}(x) $ 乘上式, 并沿杆长积分：这里就是在利用正交性的特性来求解系数 以上式常规操作，建议记住 \\sum_{i=1}^{\\infty}\\left(\\ddot{q}_{i} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x-q_{i} \\int_{0}^{l} E S \\phi_{i}^{\\prime \\prime} \\phi_{j} d x\\right)=\\rho S d \\omega^{2} \\sin \\omega t \\int_{0}^{l} \\phi_{j}\\ dx利用正交性可得： \\ddot{q}_{i}+\\omega_{i}^{2} q_{i}=\\sqrt{\\frac{2}{l}} \\frac{2 l}{i \\pi}(-1)^{(i-1) / 2} d \\omega^{2} \\sin \\omega t模态坐标稳态解： $q_{i}=\\dfrac{\\omega^{2}}{\\omega_{i}^{2}} \\eta_{i} \\sqrt{\\dfrac{2}{l}} \\dfrac{2 l}{i \\pi}(-1)^{(i-1) / 2} d \\sin \\omega t $ 其中，$\\eta_{i}=\\dfrac{1}{1-\\left(\\omega / \\omega_{i}\\right)^{2}}$ 解得杆的变形方程为： u^{*}=\\frac{16 \\rho l^{2} \\omega^{2}}{\\pi^{3} E} \\sum_{i=1,3,5, \\ldots}^{\\infty} \\frac{(-1)^{(i-1) / 2}}{i^{3}} d \\eta_{i} \\cos \\frac{i \\pi x}{2 l} \\sin \\omega t\\\\最后求解参考基下的绝对位移： \\begin{aligned} u &=u^{*}+u_{g} \\\\ &=\\left[1+\\frac{16 \\rho l^{2} \\omega^{2}}{\\pi^{3} E} \\sum_{i=1,3,5, \\ldots .}^{\\infty} \\frac{\\eta_{i}}{i^{3}}(-1)^{(i-1) / 2} \\cos \\frac{i \\pi x}{2 l}\\right] d \\sin \\omega t \\end{aligned} u(x,t)为杆上面左右点的响应。 这里的基础振动就是参考系的振动。","link":"/2022/03/18/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%BF%9E%E7%BB%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%AF%E5%8A%A8-%E4%B8%80%E7%BB%B4%E6%B3%A2%E5%8A%A8%E6%96%B9%E7%A8%8B/"},{"title":"第六章 连续系统的振动  梁的弯曲振动","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是连续系统振动的近似求解方法部分本文主要介绍的是连续系统振动的一维波动方程部分，主要从动力学方程、固有频率和模态函数、振型函数和正交性以及梁的纵向受迫振动等方面就行讲解。 弯曲振动即横向振动，轴向的振动相比横向的振动时可以忽略的。 动力学方程 对于微振来说，其变形比较小，变形前后在轴向上的投影时相同的。 满足上述条件和假设的梁称为伯努利－欧拉梁（Bernoulli-Euler Beam）即长梁。对于短梁而言，就不能忽略剪切变形以及截面绕中性轴转动惯量的影响 著名的等截面假设：就是弯矩和挠度又二阶偏导的关系 变截面梁的动力学方程： \\frac{\\partial^{2}}{\\partial x^{2}}\\left[E I \\frac{\\partial^{2} y(x, t)}{\\partial x^{2}}\\right]+\\rho S \\frac{\\partial^{2} y(x, t)}{\\partial t^{2}}=f(x, t)-\\frac{\\partial}{\\partial x} m(x, t) $EI$项为刚度项，$\\rho S$项为惯性项，整体的量纲是力的量纲。 等截面梁的动力学方程： E I \\frac{\\partial^{4} y}{\\partial x^{4}}+\\rho S \\frac{\\partial^{2} y}{\\partial t^{2}}=f(x, t)-\\frac{\\partial}{\\partial x} m(x, t) 对于的截面而言，$I$ 是个定值，可以提出来。 固有频率和模态函数基本理论 在自由振动的条件下，我们可以将时间的函数(模态坐标)写成简谐的。 其中，$C_i(i=1,2,3,4)$ 和 $\\omega$ 应该满足的频率方程形式是由梁的边界条件确定的。 这里的$\\omega_i \\longleftrightarrow \\phi_i(x)$ 有无穷多个且一一对应。 第 $ i $ 阶主振动：$ \\quad y^{(i)}(x, t)=a_{i} \\phi_{i}(x) \\sin \\left(\\omega_{i} t+\\theta_{i}\\right) $ $a_{i}$ 和 $\\theta_{i}$ 由系统的初始条件确定系统的自由振动是无穷多个主振动的叠加： y(x, t)=\\sum_{i=1}^{\\infty} a_{i} \\phi_{i}(x) \\sin \\left(\\omega_{i} t+\\theta_{i}\\right)常见的约束和边界条件 对于固定端，由于固定的一端是不受约束的，因此固定的那一端上的转角和位移都是0. 对于简支端，由于其位移是别固定住的，因此位移是0，其次由于简支端是铰接的，可以随便的转动，因此弯矩是0。 对于自由端，其弯矩和剪力都是0。 首先，代入固定端的条件可得：$C_{1}=-C_{3} \\quad C_{2}=-C_{4}$ 之后，代入自由端的条件可得： \\left\\{\\begin{array}{l}C_{1}(\\cos \\beta l+\\cosh \\beta l)+C_{2}(\\sin \\beta l+\\sinh \\beta l)=0 \\\\ -C_{1}(\\sin \\beta l-\\sinh \\beta l)+C_{2}(\\cos \\beta l+\\cosh \\beta l)=0\\end{array}\\right. $ C_{1} 、 C_{2} $ 非零解条件: $ \\left|\\begin{array}{cc}\\cos \\beta l+\\cosh \\beta l &amp; \\sin \\beta l+\\sinh \\beta l \\\\ -\\sin \\beta l+\\sinh \\beta l &amp; \\cos \\beta l+\\cosh \\beta l\\end{array}\\right|=0 $ 等截面梁：$\\phi^{(4)}(x)-\\beta^{4} \\phi(x)=0 \\quad \\beta^{4}=\\dfrac{\\omega^{2}}{a_{0}^{2}} \\quad a_{0}^{2}=\\dfrac{E I}{\\rho S} $ 通解： $\\phi(x)=C_{1} \\cos \\beta x+C_{2} \\sin \\beta x+C_{3} \\cosh \\beta x+C_{4} \\sinh \\beta x $ 代入$\\beta^{4}=\\dfrac{\\omega^{2}}{a_{0}^{2}}$ 中可得，系统的各阶频率： \\omega_{i}=\\left(\\beta_{i} l\\right)^{2} \\sqrt{\\frac{E I}{\\rho S l^{4}}}, \\quad(i=1,2, \\cdots)对应的各阶振型函数： \\phi_{i}(x)=\\cos \\beta_{i} x-\\cosh \\beta_{i} x+\\underline{\\xi_{i}}\\left(\\sin \\beta_{i} x-\\sinh \\beta_{i} x\\right), \\quad(i=1,2, \\cdots) \\\\\\xi_{i}=\\frac{\\cos \\beta_{i} l+\\cosh \\beta_{i} l}{\\sin \\beta_{i} l+\\sinh \\beta_{i} l}, \\quad(i=1,2, \\cdots) \\quad \\beta^{4}=\\frac{\\omega^{2}}{a_{0}^{2}} \\quad a_{0}^{2}=\\frac{E I}{\\rho S}这里的 $\\xi_i$ 其实就是各阶模态下$C_1和C_3$的比值。 测量的传感器要避开节点的位置。 之后，代入边界条件可得：$C_{1}=C_{3}=0$ \\begin{array}{l} \\left\\{\\begin{array}{l}C_{2} \\sin \\beta l+C_{4} \\sinh \\beta l=0 \\\\-C_{2} \\sin \\beta l+C_{4} \\sinh \\beta l=0\\end{array} \\quad \\Longrightarrow \\quad C_{4}=0\\right.\\end{array}频率方程:：$\\sin \\beta l=0 \\quad \\Rightarrow \\quad \\beta_{i} l=i \\pi, \\quad(i=1,2, \\cdots) $ 固有频率: $\\quad \\omega_{i}=\\left(\\dfrac{i \\pi}{l}\\right)^{2} \\sqrt{\\dfrac{E I}{\\rho S}}, \\quad(i=1,2, \\cdots) $ 例：两端自由梁的固有频率和振型函数，背景：导弹飞行系统类别：半正定系统 、存在刚体模态。 注意：这里第二阶的模态和静平衡位置有所不同，由于存在大范围的因此其存在两个结点，对于静平衡的结构来说应该不存在这种结构。 之前是几个简单的结构，不存在重复约束的静定结构，下面我们来看一个静不定结构的例子。 例：试用数值确定一根一端固定另一端简支的梁的频率方程，并且绘出第一阶振型和第二阶振型的挠度曲线。 对于固定端来说，位移和转角的0；对于简支端来说，位移和弯矩为0，弯矩对于的是$M=EI\\dfrac {\\partial^2 y(x,t)} {\\partial x^2}$ 将边界条件代入到振型函数中去： \\phi(x)=C_{1} \\cos \\beta x+C_{2} \\sin \\beta x+C_{3} \\cosh \\beta x+C_{4} \\sinh \\beta x \\\\ \\phi(0)=0 \\Longrightarrow C_{1}+C_{3}=0 \\Longrightarrow C_{3}=-C_{1} \\\\ \\phi^{\\prime}(0)=0 \\Longrightarrow C_{2}+C_{4}=0 \\Longrightarrow C_{4}=-C_{2} \\\\ \\phi(l)=0 \\Longrightarrow C_{1}(\\cos \\beta l-\\cosh \\beta l)+C_{2}(\\sin \\beta l-\\sinh \\beta l)=0 \\\\ \\phi^{\\prime \\prime}(l)=0 \\Longrightarrow C_{1}(\\cos \\beta l+\\cosh \\beta l)+C_{2}(\\sin \\beta l+\\sinh \\beta l)=0 解得：$\\beta_1l=3.927,\\beta_2l=7.069$ 首先，设弯曲变形方程为$y(x,t)=\\phi(x)q(t)$ 边界条件: 固定端：挠度和截面转角为零 $\\phi(0)=0 \\quad \\phi^{\\prime}(0)=0$ 弹性支撑端：剪力、弯矩分别与直线弹簧反力、卷簧反力矩相等 剪力平衡条件：$ \\dfrac{\\partial}{\\partial x}\\left(E I \\dfrac{\\partial^{2} y(l, t)}{\\partial x^{2}}\\right)=k_{2} y(l, t) \\Rightarrow E I \\phi^{\\prime \\prime \\prime}(l)=k_{2} \\phi(l) $ 弯矩平衡条件：$E I \\dfrac{\\partial^{2} y(l, t)}{\\partial x^{2}}=-k_{1} \\dfrac{\\partial y(l, t)}{\\partial x} \\Rightarrow E I \\phi^{\\prime \\prime}(l)=-k_{1} \\phi^{\\prime}(l) $ 由弹性支撑固定端条件解得： \\begin{array}{l}C_{1}\\left[E I \\beta(\\cos \\beta l+\\cosh \\beta l)+k_{1}(\\sin \\beta l+\\sinh \\beta l)\\right]+ \\\\C_{2}\\left[E I \\beta(\\sin \\beta l+\\sinh \\beta l)-k_{1}(\\cos \\beta l-\\cosh \\beta l)\\right]=0 \\\\C_{1}\\left[E I \\beta^{3}(\\sin \\beta l-\\sinh \\beta l)-k_{2}(\\cos \\beta l-\\cosh \\beta l)\\right]- \\\\C_{2}\\left[E I \\beta^{3}(\\cos \\beta l+\\cosh \\beta l)+k_{2}(\\sin \\beta l-\\sinh \\beta l)\\right]=0\\end{array} $C_{1} 、 C_{2} $ 非零解条件导出频率方程： \\cos \\beta l \\cosh \\beta l+1=-\\frac{k_{1}}{E I \\beta}(\\cos \\beta l \\sinh \\beta l+\\sin \\beta l \\cosh \\beta l), \\quad\\left(k_{2}=0\\right) \\\\ \\cos \\beta l \\cosh \\beta l+1=-\\frac{k_{2}}{E I \\beta^{3}}(\\cos \\beta l \\sinh \\beta l-\\sin \\beta l \\cosh \\beta l), \\quad\\left(k_{1}=0\\right) $k_2=0$：代表没有直线弹簧只有扭簧的情况下，系统的振动方程。 $k_1=0$：代表没有扭簧只有直线弹簧的情况下，系统的振动方程。 讨论： 若k1、k2 同时为零，则退化为悬臂梁的情形。$\\cos\\beta l\\cosh \\beta l+1=0$ 若k1＝0、k2 无穷大，则退化为一端固定另一端简支的情形 $k_2=\\dfrac a b \\quad b=0 \\quad \\cos\\beta l\\sinh\\beta l-\\sin\\beta\\cosh \\beta l=0$ 所谓的有质量没尺寸的情况，此时集中质量没有弯矩。 说明： 考虑剪切变形使得梁的刚度降低，考虑转动惯量使得梁的惯性增加，这两个因素都会使梁的固有频率降低 总结：梁的固有频率和模态是由边界来决定的；复杂边界，就是在梁的自由端增加集中质量、直线弹簧和卷簧。 振型函数的正交性 于是设：$\\omega_i \\leftrightarrow \\phi_i(x)$， $\\omega_j \\leftrightarrow \\phi_j(x)$ 在梁的简单边界上，总有挠度$\\phi(x)$或剪力$\\phi^{\\prime\\prime\\prime}(x)$中的一个与转角或$\\phi^{\\prime}(x)$弯矩$\\phi^{\\prime\\prime}(x)$中的一个同时为零 简单边界：自由端，固定端，简支端。 代入边界条件可得：$\\int_{0}^{l} \\phi_{j}\\left(E I \\phi_{i}^{\\prime \\prime}\\right)^{\\prime \\prime} d x=\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime} \\phi_{j}^{\\prime \\prime} d x$ 代入 (3) 式：$\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime} \\phi_{j}^{\\prime \\prime} d x=\\omega_{i}^{2} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x$ 同理, (2)式两边乘 $ \\phi_{i} $ 并沿梁长积分： $ \\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime} \\phi_{j}^{\\prime \\prime} d x=\\omega_{j}^{2} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x $ 相减可得: $ \\quad\\left(\\omega_{i}^{2}-\\omega_{j}^{2}\\right) \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=0 $ 讨论： 当$ i \\neq j $ 时，主振型关于质量的正交性为： \\omega_{i} \\neq \\omega_{j} \\quad \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=0, \\quad i \\neq j主振型关于刚度的正交性为： \\int_{0}^{l} \\phi_{j}\\left(E I \\phi_{i}^{\\prime \\prime}\\right)^{\\prime \\prime} d x=\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime} \\phi_{j}^{\\prime \\prime} d x=0, \\quad i \\neq j当$i=j$ 时，第 j 阶主质量： \\int_{0}^{l} \\rho S \\phi_{j}^{2} d x=m_{p j}第 j 阶主刚度： \\int_{0}^{l} \\phi_{j}\\left(E I \\phi_{j}^{\\prime \\prime}\\right)^{\\prime \\prime} d x=\\int_{0}^{l} E I\\left(\\phi_{j}^{\\prime \\prime}\\right)^{2} d x=k_{p j}第 j 阶固有频率为： $ \\omega_{j}=k_{p j} / m_{p j} $ 主振型中的常数（$\\phi_j$的系数）按归一化条件确定 ：$\\int_{0}^{l} \\rho S \\phi_{j}^{2} d x=m_{p j}=1$ 正则振型的正交性：$\\int_{0}^{l} \\phi_{j}\\left(E I \\phi_{j}^{\\prime \\prime}\\right)^{\\prime \\prime} d x=\\int_{0}^{l} E I\\left(\\phi_{j}^{\\prime \\prime}\\right)^{2} d x=k_{p j}=\\delta_{ij}\\omega_j^2$ 梁横向振动的强迫响应 这边其实默认积分和求和时可以换位置的，因为振动的方程满足一致连续的条件。 得出$q_j(t)$之后，即可以得到梁的响应$y(x,t)$ 。 如果作用在梁上的载荷不是分布力矩，而是集中力和集中力矩。 Q_{j}(t)= \\int_{0}^{l}\\left[f(x, t) \\phi_{j}+m(x, t) \\phi_{j}^{\\prime}\\right] d x \\quad \\int_{0}^{t} f(t) \\delta(t-\\tau) d t=f(\\tau)将其代入到上面的表达式可得： \\begin{aligned} Q_{j}(t) &=\\int_{0}^{l}\\left[F_{0}(t) \\delta\\left(x-\\xi_{1}\\right) \\phi_{j}(x)+M_{0}(t) \\delta\\left(x-\\xi_{2}\\right) \\phi_{j}^{\\prime}(x)\\right] d x \\\\ &=F_{0}(t) \\phi_{j}\\left(\\xi_{1}\\right)+M_{0}(t) \\phi_{j}^{\\prime}\\left(\\xi_{2}\\right) \\\\ \\end{aligned} 解： 首先，由材力得初始条件： y(x, 0)=f_{1}(x)=\\left\\{\\begin{array}{ll}y_{s t}\\left[3\\left(\\dfrac{x}{l}\\right)-4\\left(\\dfrac{x}{l}\\right)^{3}\\right], & 0 \\leq x \\leq \\dfrac{l}{2} \\\\y_{s t}\\left[3\\left(\\dfrac{l-x}{x}\\right)-4\\left(\\dfrac{l-x}{l}\\right)^{3}\\right], & \\dfrac{l}{2} \\leq x \\leq l\\end{array}\\right.\\\\ \\left.\\frac{\\partial y}{\\partial t}\\right|_{t=0}=f_{2}(x)=0 \\quad y_{s t}=-\\frac{P l^{3}}{48 E I} \\quad 梁中点的静挠度 其次，可以边界条件求解固有频率和模态函数： 归一化条件: \\int_{0}^{l} \\rho S\\left(C_{i} \\sin \\frac{i \\pi x}{l}\\right)^{2} d x=\\frac{\\rho S l}{2} C_{i}^{2}=1 \\quad C_{i}=\\sqrt{\\frac{2}{\\rho A l}} 然后，根据初始条件求模态坐标 代入公式将外界广义力转化到模态坐标，这里的$f_1(x)$为初始条件下梁的变形曲线。 q_{j}(0)=\\int_{0}^{l} \\rho S f_{1}(x) \\phi_{j}(x) d x\\\\ \\dot{q}_{j}(0)=\\int_{0}^{l} \\rho S f_{2}(x) \\phi_{j}(x) d x模态坐标初始条件下，当作用力刚刚撤去受到的力产生的位移，即正则广义力为： \\begin{align} q_{i}(0) &=\\int_{0}^{\\frac{l}{2}} \\rho S y_{s t}\\left[3\\left(\\frac{x}{l}\\right)^{3}-4\\left(\\frac{x}{l}\\right)^{3}\\right] C_{i} \\sin \\frac{i \\pi x}{l} d x + \\int_{\\frac{l}{2}}^{l} \\rho S y_{s t}\\left[3\\left(\\frac{l-x}{l}\\right)-4\\left(\\frac{l-x}{l}\\right)^{3}\\right] C_{i} \\sin \\frac{i \\pi x}{l} d x \\\\&=-\\frac{P l^{4} \\rho S}{i^{4} \\pi^{4} E I} \\qquad i=1,3,5, \\cdots \\end{align} 这里根据积分的表达式可得：在 i 为偶数的情况下积分的结果可能为0。 初始条件下，由于外力是刚刚撤去的，因此此时的速度为0，没有激励，因此正则广义力为0： $\\dot{q}_{i}(0)=0 \\quad i=1,3,5, \\cdots $ 最后，代入求解在物理空间的响应 模态坐标响应: $ \\quad q_{i}(t)=q_{i}(0) \\cos \\omega_{i} t $ 物理空间上的梁响应： y(x, t)=\\sum_{i=1}^{\\infty} \\phi_{i}(x) q_{i}(t)=-\\frac{2 P l^{3}}{\\pi^{4} E I} \\sum_{i=1,3,5 \\cdots}^{\\infty} \\frac{(-1)^{\\frac{i-1}{2}}}{i^{4}} \\sin \\frac{i \\pi x}{l} \\cos \\omega_{i} t 例：简支梁，中点受力矩作$M_0\\sin\\omega t$ 用，求：梁的响应 上面的两个题目都是直接求解，固有频率、振型方程、广义正则力，之后得出正则方程，求解正则方程，最后将求解的结果代入得到物理空间，得出最后的模态。 上述两题的方法总结： 首先根据边界条件使用之前例题的一些结论直接得出固有频率和振型函数(刚才我们推导了不同边界下的振动过程) 之后根据外力的公式，直接积分得出正则广义力即模态坐标 最后根据模态函数和模态坐标相乘并累加 其实直接法和定义法是差不多的，但是使用定义法不容易出错。 求稳态强迫振动，以及梁自由端的响应。 两边乘 $ \\phi_{j} $ 并沿梁长对 x 积分： \\sum_{i=1}^{\\infty}\\left(q_{i} \\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime \\prime} \\phi_{j} d x+\\ddot{q}_{i} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x\\right)=P \\sin \\omega t \\int_{0}^{l} \\delta(x-l) \\phi_{j} d x利用正则振型正交性条件 : \\ddot{q}_{i}+\\omega_{i}^{2} q_{i}=P \\phi_{i}(l) \\sin \\omega t模态稳态解 : $q_{i}=\\dfrac{P \\phi_{i}(l)}{\\omega_{i}^{2}\\left[1-\\left(\\omega / \\omega_{i}\\right)^{2}\\right]} \\sin \\omega t, \\quad i=1,2, \\ldots $ 梁的响应： y(x, t) =\\sum_\\limits{i=1}^{\\infty} \\phi_{i}(x) q_{i}(t) =P \\sin \\omega t \\sum_\\limits{i=1}^{\\infty} \\dfrac{\\phi_{i}(l) \\phi_{i}(x)}{\\omega_{i}^{2}\\left[1-\\left(\\omega / \\omega_{i}\\right)^{2}\\right]}, \\quad i=1,2, \\ldots此时自由端的响应为： 令 $ x=l: \\quad y(l, t)=P \\sin \\omega t \\sum_\\limits{i=1}^{\\infty} \\dfrac{\\phi_{i}^{2}(l)}{\\omega_{i}^{2}\\left[1-\\left(\\omega / \\omega_{i}\\right)^{2}\\right]}, \\quad i=1,2, \\ldots $ \\left.\\phi_{i}(x)\\right|_{x=l}=\\cos \\beta_{i} l-\\cosh \\beta_{i} l+\\frac{\\cos \\beta_{i} l+\\cosh \\beta_{i} l}{\\sin \\beta_{i} l+\\sinh \\beta_{i} l}\\left(\\sin \\beta_{i} l-\\sinh \\beta_{i} l\\right), \\quad(i=1,2, \\cdots) \\\\ \\beta_{1} l=1.875 \\quad \\beta_{2} l=4.694 \\quad \\beta_{3} l=7.855 \\quad \\beta_{i} l \\approx \\frac{2 i-1}{2} \\pi, \\quad(i=3,4, \\cdots) \\\\ y(l, t)=\\frac{4 P l^{3}}{E I}\\left[\\frac{\\eta_{1}}{(1.875)^{4}}+\\frac{\\eta_{2}}{(4.694)^{4}}+\\frac{\\eta_{3}}{(7.855)^{4}}+\\ldots . .\\right] \\sin \\omega t \\\\ \\eta_{i}=\\frac{1}{1-\\left(\\omega / \\omega_{i}\\right)^{2}}, \\quad i=1,2, \\ldots例：简支梁，左端承受正弦支撑运动 $g(t)=y_0\\sin(\\omega t)$ 试求梁的响应。 $y_g(x,t)$ 为参考基的位移，$y-y_g$ 代表相对位移，就是假设基础不动是的位移。 以右截面上任一点为矩心，力矩平衡： \\left(M+\\frac{\\partial M}{\\partial x} d x\\right)-M-F_{s} d x+\\rho S d x \\frac{\\partial^{2} y}{\\partial t^{2}} \\frac{d x}{2}=0略去高阶无穷小量：$F_s=\\dfrac {\\partial M}{\\partial x}$ 材料力学的等截面假设，弯矩与挠度的关系： M(x, t)=E I \\dfrac{\\partial^{2}\\left[y(x, t)-y_{g}(x, t)\\right]}{\\partial x^{2}}于是可得梁的振动方程为： E I \\frac{\\partial^{4}\\left(y-y_{g}\\right)}{\\partial x^{4}}+\\rho S \\frac{\\partial^{2} y}{\\partial t^{2}}=0之后令 $y^{}=y-y_{g},\\quad y=y^{}+y_{g}$ ，并代入$y_g(x,t)$ 得： 代入方程： $E I y^{(4)}+\\rho S \\dot{y}^{}=-\\rho S \\dot{y}_{g}=-\\rho S \\omega^{2} y_{0} \\sin \\omega t\\left(1-\\dfrac{x}{l}\\right)$ 设解为: $y^{*}=\\sum_\\limits{i=1}^{\\infty} {\\phi_{i}(x) q_{i}(t)}$ 代入到方程中可得： \\quad \\sum_{i=1}^{\\infty}\\left(E I q_{i} \\phi_{i}^{\\prime \\prime \\prime}+\\rho S \\ddot{q}_{i} \\phi_{i}\\right)=\\rho S \\omega^{2} y_{0}\\left(1-\\frac{x}{l}\\right) \\sin \\omega t其中，归一化正则振型为： $ \\phi_{i}(x) =\\sqrt{\\dfrac{2}{l}} \\sin \\dfrac{i \\pi x}{l}$ 用 $\\phi_{j}(x)$ 乘上式, 并沿杆长积分: \\sum_{i=1}^{\\infty}\\left(q_{i} {\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime \\prime} \\phi_{j} d x}+{\\left.\\ddot{q}_{i} \\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x \\right)}=\\rho S \\omega^{2} y_{0} \\sin \\omega t \\int_{0}^{l} \\phi_{j}\\left(1-\\frac{x}{l}\\right) d x\\right.这里，$\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime \\prime} \\phi_{j} d x=\\omega_i^2$ , $\\int_{0}^{l} \\rho S \\phi_{i} \\phi_{j} d x=1$. 正交性: $\\quad \\ddot{q}_{i}+\\omega_{i}^{2} q_{i}=\\sqrt{\\dfrac{2}{l}} \\dfrac{l \\omega^{2} y_{0} \\sin \\omega t}{i \\pi}, \\quad i=1,2, \\ldots$ 模态坐标稳态解: q_{i}=\\sqrt{\\frac{2}{l}} \\frac{l \\omega^{2} y_{0} \\eta_{i}}{i \\pi \\omega_{i}^{2}} \\sin \\omega t =\\sqrt{\\frac{2}{l}} \\frac{l^{5} \\omega^{2} y_{0}}{i^{5} \\pi^{5} a_{0}^{2}} \\eta_{i} \\sin \\omega t, \\quad i=1,2, \\ldots\\\\ \\eta_{i}=\\frac{1}{1-\\left(\\omega / \\omega_{i}\\right)^{2}}简支梁固有频率：$\\omega_{i}=\\left(\\dfrac{i \\pi}{l}\\right)^{2} \\sqrt{\\dfrac{E I}{\\rho S}}, \\quad(i=1,2, \\cdots), \\quad a_{0}^{2}=\\dfrac{E I}{\\rho S}$$ y^{*}=\\frac{2 l^{4} \\omega^{2} y_{0} \\sin \\omega t}{\\pi^{5} a_{0}^{2}} \\sum_{i=1}^{\\infty} \\frac{\\eta_{i}}{i^{5}} \\sin \\frac{i \\pi x}{l}\\\\y=y^{*}+y_{g}=y_{0} \\sin \\omega t\\left[1-\\frac{x}{l}+\\frac{2 l^{4} \\omega^{2}}{\\pi^{5} a_{0}^{2}} \\sum_{i=1}^{\\infty} \\frac{\\eta_{i}}{i^{5}} \\sin \\frac{i \\pi x}{l}\\right]","link":"/2022/03/18/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%BF%9E%E7%BB%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%AF%E5%8A%A8-%E6%A2%81%E7%9A%84%E5%BC%AF%E6%9B%B2%E6%8C%AF%E5%8A%A8/"},{"title":"第六章 连续系统的振动 近似求解方法 第一部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是连续系统振动的近似求解方法部分，主要介绍其中的集中质量法、假设模态法、瑞利法以及里兹法。 连续系统的精确解仅适用于简单构件形状和边界条件，对于复杂构件来说只能求解数值解。 当构件形状复杂或边界条件复杂时可以采用近似解法 近似方法主要包括：集中质量法、假设振型法、有限元法 集中质量法是将连续系统的质量集中到有限个点或截面上，将连续的问题离散化后求解。 假设振型法是用有限个函数的线性组合来构造连续系统的解 有限元法兼有以上两种方法的特点 各种近似解法的共同特点：用有限自由度的系统对无限自由度的系统进行近似，这样可以将无限自由度的问题转化为有限自由度的多自由度系统求解问题。 集中质量法 工程系统的物理参数常常分布不均匀 惯性和刚性较大的部件可看作质量集中的质点和刚体 惯性小和弹性强的部件可抽象为无质量的弹簧，它们的质量可以不计或折合到集中质量上 如圆盘之间的轴或者滑块之间的弹簧。 物理参数分布均匀的系统，也可近似地分解为有限个集中质量 将悬臂梁等效为一些集中质量的组合 集中质量的数量取决于所要求的计算精度 等效的集中质量越密集个数越多，计算得到的精度就越高 连续系统离散为有限自由度系统后，可以采用多自由度系统的分析方法进行分析 这里，两端边界的质量对挠度不会产生影响，因此两端的质量可以去掉，最后等效万以后的总质量是0.75m，与原来的质量不同。 最终等效为，$m_1=m_2=m_3=\\dfrac m 4$ 的三自由度的系统。 多自由度系统分质量矩阵为$M=\\dfrac m 4 \\left[\\begin{array}\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]$ 三个质点之间的梁段具有相同的弹性性质，由材料力学，得柔度影响系数： f_{11}=f_{33}=\\dfrac {9l^3} {768EI}\\\\ f_{12}=f_{21}=f_{23}=f{32} =\\dfrac {11l^3} {768EI}\\\\ f_{22}=\\dfrac {16l^3} {768EI}\\\\ f_{13}=f_{31}=\\dfrac {7l^3} {768EI}\\\\可得柔度矩阵为：$F=\\dfrac {l^3} {768EI} \\left[\\begin{array}\\\\ 9 &amp; 11 &amp; 7\\\\ 11 &amp; 16 &amp; 11\\\\ 7 &amp; 11 &amp; 9 \\end{array}\\right]$ 连续体近似成了三自由度系统，也可将连续梁离散为两自由度或单自由度系统，在求得系统的质量矩阵和柔度矩阵后，可以计算出相应的系统固有频率： 随着自由度数目的增加，计算精度提高； 基频精度较高，但是其他的本来误差就比较大； 频率阶数增高，误差增大。 注：在采用集中质量法时，计算精度与边界条件有关，例如将同一模型用于悬臂梁系统，计算精度明显下降连续系统的振动 由于悬臂梁的右边是自由的，约束减少了。 假设模态法主要包括动力学方程、瑞利法和里兹法。 利用有限个已知的振型函数来确定系统的运动规律，在采用振型叠加法讨论连续系统的响应时，是将连续系统的解写作全部振型函数的线性组合： y(x, t)=\\sum_\\limits{i=1}^{\\infty} \\phi_{i}(x) q_{i}(t)其中，$\\phi_i(x)$ 为振型函数；$q_i(t)$ 为模态坐标。 若取前 n 个有限项作为近似解： y(x, t)=\\sum_\\limits{i=1}^{n} \\phi_{i}(x) q_{i}(t)其中，$\\phi_i(x)$：应该是系统的振型函数，但实际中由于无法得到等原因而代以假设模态，即满足部分或全部边界条件，但不一定满足动力学方程的试函数族。$q_i(t)$ ：与假设振型所对应的广义坐标。 动能： \\begin{align} T&=\\frac{1}{2} \\int_{0}^{l} \\rho S\\left(\\frac{\\partial y(x, t)}{\\partial t}\\right)^{2} d x=\\frac{1}{2} \\int_{0}^{l} \\rho S\\left(\\dot{\\boldsymbol{q}}^{T} \\boldsymbol{\\Phi}^{T}\\right)(\\boldsymbol{\\Phi} \\dot{\\boldsymbol{q}}) d x\\\\&=\\frac{1}{2} \\dot{\\boldsymbol{q}}^{T}\\left(\\int_{0}^{l} \\rho S \\boldsymbol{\\Phi}^{T} \\boldsymbol{\\Phi} d x\\right) \\dot{\\boldsymbol{q}}=\\frac{1}{2} \\dot{\\boldsymbol{q}}^{T} \\boldsymbol{M} \\dot{\\boldsymbol{q}} \\end{align} 势能： V=\\frac{1}{2} \\int_{0}^{l} E I\\left(\\frac{\\partial^{2} y(x, t)}{\\partial x^{2}}\\right)^{2} d x=\\frac{1}{2} \\int_{0}^{l} E I\\left(\\boldsymbol{q}^{T} \\boldsymbol{\\Phi}^{\\prime \\prime}\\right)\\left(\\boldsymbol{\\Phi}^{\\prime \\prime} \\boldsymbol{q}\\right) d x=\\frac{1}{2} \\boldsymbol{q}^{T} \\boldsymbol{K} \\boldsymbol{q}\\\\\\boldsymbol{K}=\\int_{0}^{l} E I \\boldsymbol{\\Phi}^{\\prime \\prime T} \\boldsymbol{\\Phi}^{\\prime \\prime} d x \\in R^{n \\times n} \\quad 刚度阵\\\\k_{i j}=k_{j i}=\\int_{0}^{l} E I \\phi_{i}^{\\prime \\prime}(x) \\phi_{j}^{\\prime \\prime}(x) d x \\quad 刚度阵为对称有激励存在的拉格朗日方程： \\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial \\dot{q}_{i}}\\right)-\\frac{\\partial T}{\\partial q_{i}}+\\frac{\\partial V}{\\partial q_{i}}=Q_{i}\\\\ \\frac{d}{d t}\\left(\\frac{\\partial L}{\\partial \\dot{q}_{i}}\\right)-\\frac{\\partial L}{\\partial q_{i}}=Q_{i} \\quad L=T-V分布力的虚功: \\quad \\delta W(t)=\\int_{0}^{l} p(x, t) \\delta y d x=\\sum_\\limits{i=1}^{n}\\left({\\int_{0}^{l} p(x, t) \\phi_{i}(x) d x}\\right) \\delta q_{i}= \\sum_{i=1}^{n} Q_{i} \\delta q_{i} \\\\Q_{i}(t) =\\int_{0}^{l} p(x, t) \\phi_{i}(x) d x \\quad \\text { 广义力 }矩阵形式: $\\quad \\boldsymbol{Q}(t)=\\left[Q_{1}(t), Q_{2}(t), \\cdots, Q_{n}(t)\\right]^{T} \\in R^{n \\times 1}$ 拉格朗日方程矩阵形式： \\frac{d}{d t}\\left(\\frac{\\partial T}{\\partial \\dot{q}}\\right)-\\frac{\\partial T}{\\partial q}+\\frac{\\partial V}{\\partial \\boldsymbol{q}}=\\boldsymbol{Q} \\\\{M} \\ddot{q}+\\boldsymbol{K} \\boldsymbol{q}=\\boldsymbol{Q}(t)弹性体的受迫振动转换成了 n 自由度系统的强迫振动问题. 以上均质梁的动能和势能的表示函数。 如果均质上面有多余的卷簧和直线弹簧，则算法为以下所示。 典型例题 若对第三阶固有频率的精度要求不高，取 n＝3，但是一般求前三阶时我们会求前五阶的模态函数，这里对进度的要求不高就先求前三阶。 对于这种边界比较复杂的函数来讲，求解模态就比较复杂了，因此最好使用假设的比较简单的模态来近似逼近，这里我们时假设的是简支梁的模态$\\phi_i(x)=\\sin\\dfrac {i \\pi x}{l}$。 此时其振型的函数阵： 于是其对应的特征根问题为：$(K-\\omega^2M)\\Psi=0$ 解得固有频率为： \\omega_{1}=5.6825 \\sqrt{\\dfrac{E I}{\\rho S l^{4}}} \\quad \\omega_{2}=39.4784 \\sqrt{\\dfrac{E I}{\\rho S l^{4}}} \\quad \\omega_{3}=68.9944 \\sqrt{\\dfrac{E I}{\\rho S l^{4}}}正则化特征向量： $\\psi^{(1)}=\\sqrt{\\frac{2}{\\rho S l}}\\left[\\begin{array}{c}0.5742 \\\\ 0 \\\\ -0.0048\\end{array}\\right] \\psi^{(2)}=\\sqrt{\\frac{2}{\\rho S l}}\\left[\\begin{array}{l}0 \\\\ 1 \\\\ 0\\end{array}\\right] \\quad \\psi^{(3)}=\\sqrt{\\frac{2}{\\rho S l}}\\left[\\begin{array}{c}0.5199 \\\\ 0 \\\\ 0.7746\\end{array}\\right]$ 梁的稳态响应： y(x,t)=\\sum_\\limits{i=1}^3\\phi_i(x)q_i(x)=\\sum_\\limits{i=1}^3q_i(t)\\sin \\frac {i\\pi x}{l} 瑞利法连续系统的瑞利法是基于能量法的假设振型法，是多自由度系统的瑞利法的推广以梁的弯曲振动为例 假设梁以某阶振型函数作频率为 $\\omega$ 的自由振动：$y(x,t)=\\phi\\sin \\omega t$ 设系统为保守系统，机械能守恒，即$T_{max}=V_{max}$ 定义瑞利商：$R(\\phi)=\\dfrac {V_{max}} {T^{*}}=\\omega^2$ 当 $\\phi(x)$ 为准确的第 i 阶振型函数时，瑞利商即为相应的特征值，即第 i 阶固有频率$\\omega_i$ 。 若$\\phi(x)$是试函数，它满足梁的几何边界条件，但不满足动力学方程，则瑞利商是一个依赖于 $\\phi(x)$ 的标量。 试函数 $\\phi(x)$ 越接近某阶真实振型$\\Rightarrow$瑞利商越接近该阶固有频率。 与多自由度系统相同，瑞利商大于基频 $R(\\phi)&gt;\\omega_0^2$ 。 实际计算时可选择梁的静变形函数，或选择条件相近的梁的精确解作为试函数。比如，对于真实情况下的悬臂梁，我们使用标准的匀质，等截面的悬臂来梁来进行近似。 T^{*}=\\dfrac{1}{2}\\left[\\int_{0}^{l} \\rho S \\phi^{2}(x) d x+m \\phi^{2}\\left(x_{a}\\right)\\right] \\quad R(\\phi)=\\dfrac{V_{\\max }}{T^{*}} 选择等截面悬臂梁在均布载荷下的静挠度曲线作为试函数： \\phi(x)=A_{1}\\left(x^{4}-4 l x^{3}+6 l^{2} x^{2}\\right) \\Longrightarrow \\omega_{1}=1.1908 \\sqrt{\\dfrac{E I}{\\rho S l^{4}}} 选择端部集中质量作用下的静挠度曲线作为试函数： \\phi(x)=A_{2}\\left(3 l x^{2}-x^{3}\\right) \\quad \\Longrightarrow \\quad \\omega_{1}=1.1584 \\sqrt{\\frac{E I}{\\rho S l^{4}}} 因集中质量大于梁的分布质量，选用后一种试函数好 瑞利法理论是只要模态估计的好基本都可以很好的估计，尤其估计基频对应的曲线比较的容易，但是估计三阶或者更高阶的变形曲线就比较的困难了。 里茨法 里兹法是瑞利法的改进 瑞利法使用单个试函数，而里兹法使用若干个独立的试函数的线性组合： \\phi(x)=\\sum_\\limits{i=1}^{n} a_i \\eta_i(x) 里兹法的主要精髓就是如何选择参数$a_i\\quad (i=1,2,…,n)$ ，使得瑞利商取驻值： \\dfrac {\\partial R} {\\partial a_i}=0,\\quad (i=1,2,...,n)得到 $a_i$ 的齐次代数方程组，其非零解条件可用来计算系统的固有频率。 考虑梁的弯曲振动，其参考动能为： \\begin{align} T^{*}&=\\frac{1}{2} \\int_{0}^{l} \\rho S \\phi^{2}(x) d x=\\frac{1}{2} \\int_{0}^{l} \\rho S\\left[\\sum_{i=1}^{n} a_{i} \\eta_{i}(x)\\right]\\left[\\sum_{j=1}^{n} a_{j} \\eta_{j}(x)\\right] d x\\\\ &=\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\tilde{m}_{i j} a_{i} a_{j}=\\frac{1}{2} \\boldsymbol{a}^{T} \\widetilde{\\boldsymbol{M}} \\boldsymbol{a} \\\\ &\\widetilde{m}_{i j}=\\frac{1}{2} \\int_{0}^{l} \\rho S \\eta_{i}(x) \\eta_{j}(x) d x, \\quad(i, j=1,2, \\cdots, n) \\quad \\\\&定义: \\tilde{\\boldsymbol{M}}=\\left[\\tilde{m}_{i j}\\right] \\in R^{n \\times n} \\quad \\boldsymbol{a}=\\left[a_{i}\\right] \\in R^{n \\times 1} \\end{align} 梁的弯曲振动，其最大势能： \\begin{align} V_{\\max } &=\\frac{1}{2} \\int_{0}^{l} E I \\phi^{\\prime \\prime 2}(x) d x=\\frac{1}{2} \\int_{0}^{l} E I\\left[\\sum_{i=1}^{n} a_{i} \\eta_{i}^{\\prime \\prime}(x)\\right]\\left[\\sum_{j=1}^{n} a_{j} \\eta_{j}^{\\prime \\prime}(x)\\right] d x\\\\ &=\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\widetilde{k}_{i j} a_{i} a_{j} =\\frac{1}{2} \\boldsymbol{a}^{T} \\widetilde{\\boldsymbol{K}} \\boldsymbol{a}\\\\ \\tilde{k}_{i j} &=\\frac{1}{2} \\int_{0}^{l} E I \\eta_{i}^{\\prime \\prime}(x) \\eta_{j}^{\\prime \\prime}(x) d x, \\quad(i, j=1,2, \\cdots, n) \\end{align}定义: $ \\widetilde{\\boldsymbol{K}}=\\left[\\widetilde{k}_{i j}\\right] \\in R^{n \\times n} $ 固有频率的近似值： R(\\phi)=\\frac{\\boldsymbol{a}^{T} \\widetilde{\\boldsymbol{K}} \\boldsymbol{a}}{\\boldsymbol{a}^{T} \\widetilde{\\boldsymbol{M}} \\boldsymbol{a}}=\\widetilde{\\omega}^{2} 里兹法改善了瑞利法对基频的估计，还可计算高阶固有频率 n 愈大，计算精度愈高 计算精度也与基函数 $\\eta_i(i=1,2,…,n)$ 的选择有关，通常采用幂函数、三角函数、贝塞尔函数或条件相近的有精确解的梁的振型函数作为基函数 对于机械臂而言，我们可以直接使用一个一般的均质等截面梁的各阶模态作为基函数来进行函数逼近。 解：选取无集中质量时的梁的振型函数作为里兹基函数： 这里就是选用简支梁的模态函数来作为上述结构的里兹基。 \\eta_i(x)=\\sin \\dfrac {i\\pi x}{l},(i=1,2,...)基函数满足自然边界条件(两端挠度和弯矩为零)： \\eta_i(x)=0,\\eta_i^{\\prime \\prime}=0 \\quad (x=0\\,\\ or\\,\\ l)若对第三阶固有频率的精度要求不高，取 n＝3 振型试函数：$\\phi(x)=\\sum_\\limits{i=1}^{3} a_i \\eta_i(x)=\\sum_\\limits{i=1}^{3} a_i \\sin \\dfrac {i \\pi x} l$ 离散化强迫振动方程：$\\boldsymbol{M \\ddot q} + \\boldsymbol{Kq}=\\boldsymbol{Q(t)}$ 梁的振型函数近似解： \\begin{aligned} \\phi^{(1)}(x) &=\\sqrt{\\frac{2}{\\rho S l}}\\left(0.5742 \\sin \\frac{\\pi x}{l}-0.0048 \\sin \\frac{3 \\pi x}{l}\\right) \\\\ \\phi^{(2)}(x) &=\\sqrt{\\frac{2}{\\rho S l}} \\sin \\frac{2 \\pi x}{l} \\\\ \\phi^{(3)}(x) &=\\sqrt{\\frac{2}{\\rho S l}}\\left(0.5199 \\sin \\frac{\\pi x}{l}-0.7746 \\sin \\frac{3 \\pi x}{l}\\right) \\end{aligned} 用里兹法求基频。 有材料力学可得根部截面对中性轴的惯性矩为：$I_0=\\dfrac 1 {12} \\left({2b}\\right)^3$ 截面对中性轴的惯性矩：$I_0=\\dfrac 1 {12} \\left(\\dfrac {2bx} {l}\\right)^3=I_0\\dfrac {x^3} {l^3}$ 取基函数：$\\eta_i=\\left(1-\\dfrac x l\\right)^2\\left(\\dfrac x l\\right)^{i-1},(i=1,2,…,n)$ 可以验证，基函数满足所有位移边界条件和力边界条件： 这里我们取$n=2$可得： m_{i j}=\\frac{1}{2} \\int_{0}^{l} \\rho S \\eta_{i}(x) \\eta_{j}(x) d x \\quad k_{i j}=\\frac{1}{2} \\int_{0}^{l} E I \\eta_{i}^{\\prime \\prime}(x) \\eta_{j}^{\\prime \\prime}(x) d x \\left|\\boldsymbol{K}-\\omega^{2} \\boldsymbol{M}\\right|=0 \\Longrightarrow \\omega_{1}=5.319 \\sqrt{\\frac{E I_{0}}{\\rho A_{0} l^{4}}} \\\\ \\text { 若取 } n=1: \\quad \\omega_{1}=5.477 \\sqrt{\\frac{E I_{0}}{\\rho A_{0} l^{4}}} \\\\ \\text { 精确解: } \\quad \\omega_{1}=5.315 \\sqrt{\\frac{E I_{0}}{\\rho A_{0} l^{4}}}","link":"/2022/03/18/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%BF%9E%E7%BB%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%AF%E5%8A%A8-%E8%BF%91%E4%BC%BC%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"第六章 连续系统的振动 近似求解方法 第三部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是连续系统振动的近似求解方法部分，主要介绍其中的模态综合法的进一步分析和阐述、动态子结构方法的基本思想、自由界面模态综合法和固定模态综合法。本节内容是课本内容的深入和拔高。 模态综合法的进一步阐述 有限单元法可成功将一连续系统转化为一个多自由度系统问题。 可求得：系统的固有频率，振型，动力响应； 现代工程结构特征：庞大，复杂； 飞机，大型轮船，高层建筑，大型机械，航天器； 系统自由度成千上万阶，甚至几十万阶； 传统的动力特性和动力响应分析往往十分困难； 自由度过高，造成系统的控制比较困难。 对策：从量级上大幅缩减整体结构自由度而不改变问题的本质，在保证问题的主要特征的情况下进行减小问题的自由度。 本节是对模态综合法的补充，主要介绍的是模态综合法/动态子结构法/模型缩聚。 上世纪60年代初，人们为了解决大型复杂结构系统整体动力分析困难问题而提出了模态综合技术 Hurty和GladWell等人于上世纪60年代初奠定了模态综合技术的理论基础 60年代末至70年代间，Craig和Bampton、Rubin、Hou、Hintz等人先后从各个不同侧面对古典的模态综合技术进行了改进和总结 我国学者王文亮、王永岩、张汝清等人也做了大量研究工作，使模态综合方法得到了进一步发展 模态综合法主要分为固定界面模态综合法和自由界面模态综合法。 动态子结构方法的基本思想按照工程的观点或结构的几何轮廓，遵循某些原则要求，把完整的大型复杂结构人为地抽象成若干个子结构。首先对自由度少得多的各个子结构进行动态分析，然后经由各种方案，把它们的主要模态信息予以保留，以综合总体结构的动态特性 再现子结构：于整体结构中再现由模态坐标返回到物理坐标后的各子结构，以得到实际结构的主振型和位移及应力等动态响应 实际案例分析 每个子结构的自由度分为内部自由度${u_I}$和界面自由度${u_J}$： \\left\\{u^{a}\\right\\}=\\left\\{\\begin{array}{l}u_{I}^{a} \\\\u_{J}^{a}\\end{array}\\right\\}, \\quad\\left\\{u^{b}\\right\\}=\\left\\{\\begin{array}{l}u_{I}^{b} \\\\u_{J}^{b}\\end{array}\\right\\}根据界面连续性条件, 有: $\\quad\\left\\{u_{J}^{a}\\right\\}=\\left\\{u_{J}^{b}\\right\\}$ 由力的对接条件, 有: $\\left\\{f_{J}^{a}\\right\\}+\\left\\{f_{J}^{b}\\right\\}=\\{0\\}$ 界面内力合力为 这里$\\left\\{u_a \\right\\},\\left\\{u_b \\right\\}$ 为物理坐标，可以写成模态乘以模态坐标的形式，但是这里是近似求解，$\\left\\{u_a \\right\\},\\left\\{u_b \\right\\}$ 都有一定的截断，但是截断的位数不一定相等。 第一次坐标变换，将物理空间子结构的左边变化为模态坐标 T=T^{a}+T^{b}=\\frac{1}{2}\\left\\{\\dot{u}^{a}\\right\\}^{T}\\left[m^{a}\\right]\\left\\{\\dot{u}^{a}\\right\\}+\\frac{1}{2}\\left\\{\\dot{u}^{b}\\right\\}^{T}\\left[m^{b}\\right]\\left\\{\\dot{u}^{b}\\right\\}\\\\ V=V^{a}+V^{b}=\\frac{1}{2}\\left\\{u^{a}\\right\\}^{T}\\left[k^{a}\\right]\\left\\{u^{a}\\right\\}+\\frac{1}{2}\\left\\{u^{b}\\right\\}^{T}\\left[k^{b}\\right]\\left\\{u^{b}\\right\\}\\\\ T=\\frac{1}{2}\\left\\{\\dot{p}^{a}\\right\\}^{T}[M]^{a}\\left\\{\\dot{p}^{a}\\right\\}+\\frac{1}{2}\\left\\{\\dot{p}^{b}\\right\\}^{T}[M]^{b}\\left\\{\\dot{p}^{b}\\right\\}=\\frac{1}{2}\\{\\dot{p}\\}^{T}[M]\\{\\dot{p}\\}\\\\V=\\frac{1}{2}\\left\\{p^{a}\\right\\}^{T}[K]^{a}\\left\\{p^{a}\\right\\}+\\frac{1}{2}\\left\\{p^{b}\\right\\}^{T}[K]^{b}\\left\\{p^{b}\\right\\}=\\frac{1}{2}\\{p\\}^{T}[K]\\{p\\}\\\\{[M]^{a}=[\\Phi]^{a T}\\left[m^{a}\\right][\\Phi]^{a}, \\quad[M]^{b}=[\\Phi]^{b T}\\left[m^{b}\\right][\\Phi]^{b} }\\\\ [K]^{a}=[\\Phi]^{a T}\\left[k^{a}\\right][\\Phi]^{a}, \\quad[K]^{b}=[\\Phi]^{b T}\\left[k^{b}\\right][\\Phi]^{b}将a,b组集成为一个矩阵： [M]和[K]实际上是独立处理各子结构后得到的，而每个子结构的界面自由度${u_J^a}$、${u_J^b}$不是相互独立的，因此坐标${p}$中的元素不是相互独立的，不独立坐标的个数d恒等于界面自由度数，例如上图中d＝3 在界面上二者是重合的，因此需要将二者重叠的部分去掉一个，进而保证独立性成立。 新方程的阶数等于所选取的全部保留模态的总数减去对接自由度数。 系统的无阻尼自由振动的运动方程: \\begin{array}{c}{[M]^{*}\\{\\ddot{q}\\}+[K]^{*}\\{q\\}=\\{0\\}} \\\\{[M]^{*}=[S]^{T}[M][S], \\quad[K]^{*}=[S]^{T}[K][S]}\\end{array}对于一般的动力学分析问题, 也可以得到缩聚方程为: \\begin{array}{l}{[M]^{*}\\{\\ddot{q}\\}+[C]^{*}\\{\\dot{q}\\}+[K]^{*}\\{q\\}=\\{R\\}^{*}} \\\\{[C]^{*}=[S]^{T}[C][S], \\quad\\{R\\}^{*}=[S]^{T}\\{R\\}}\\end{array} 上述分析很容易推广到多个子结构组成的结构系统，困难的是如何构造和获取各子结构的保留模态来构成李兹基。各种不同的获取方法，便形成了不同的模态综合技术。 在模态综合法中，常常需取子结构低阶主振型来组成主振型矩阵[Фk]，以后称之为保留主振型，模态综合法正是因为舍去了子结构的高阶主振型而达到整体降阶的目的 于是解得解耦后的质量阵和刚度阵： 原结构第二阶频率具有中点不动的振型，它恰可由固定界面子结构的基本振型精确地组合，因此第二阶频率为精确解。 自由界面模态综合法 1968年SNHou和Goldman差不多同时提出的 将子结构从整体中分割出来后，解出全部约束，这与固定界面综合法完全不同 便于与试验相结合 自由界面模态综合法分析方法：将整体结构划分为自由界面的一系列子结构，然后完成每一子结构的模态析，以提出各子结构的主振型信息。用自由界面主振型中的保留主振型作为各子结构的假设振型参与综合，但是主振型数必须大于界面自由度数固定界面模态综合法很难与试验相结合，因为固定对界面在试验中很难实现 为与第二次变换中的变量顺序保持一致，修改第一次变换所得到了质量阵和刚度阵的顺序 第二次变换后的刚度阵和质量阵： 综合出的基频是精确解，原因：原结构的第一阶模态恰巧是由两个子结构的第一阶模态组成的，是反对称模态 典型例题 对于子结构3作同样处理，子结构3与子结构1、2对接，最终得到第二次变换： 得到第二次变换矩阵后，可以得到整体结构最终的质量阵和刚度阵，然后就可进行固有频率和振型分析。 长梁与每根短梁互相耦合4个自由度，所以减去8个自由度，最后建立的有限元模型为181阶方程。 采用自由界面模态综合法时，对每个子结构保留1/3左右正则主模态，最后得到利用自由界面模态综合法所建立的模型为63阶方程，可以看出经过局部降阶，即模态综合法建模，模型阶数已经较大程度的降低。","link":"/2022/03/19/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%BF%9E%E7%BB%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%AF%E5%8A%A8-%E8%BF%91%E4%BC%BC%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86/"},{"title":"第六章 连续系统的振动 近似求解方法 第二部分","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是连续系统振动的近似求解方法部分，主要以杆的纵向振动和梁的弯曲振动为例来介绍其中的模态综合法和有限元法。 模态综合法 对于由多个构件组成的复杂系统，很难找到适合于整个系统的假设模态。 对策：将复杂结构分解成若干个较简单的子结构，对每个子结构选定假设模态，然后根据对接面上的位移和力的协调条件，将各个子结构的假设振型综合成总体系统的振型函数。 实际工程问题中低阶模态占主要成分，因此对每个子结构只需要计算少量低阶模态，然后加以综合。 子结构的划分应使得子结构易于分析，并且对接面尽量缩小，以减少子结构之间的耦合。 模态综合法是计算机不发达时期主要产物，因为当是的算例和储存都达不到要求。 由于$o_3$为交界面，因此在$o_3$处的力、位移和转角都是一样的。 固定界面法：将两子结构的界面 $o_3 $加以固定，使两子结构成为两端固定的直梁。 自由界面法：将两个结构看做出成为两个悬臂梁。 此时满足几何边界条件的振型函数：$\\phi_1(x_i)=\\left(1-\\dfrac {x_i} l\\right)^2\\left(\\dfrac {x_i} l\\right)^{2}$ 满足在$x=0 /l$时的位移和转角都为0。 位移边界条件满足： \\phi_{1}\\left(x_{i}\\right)=\\left(\\frac{x_{i}}{l}\\right)^{2}\\left[1-\\left(\\frac{x_{i}}{l}\\right)\\right]^{2} \\\\x=0 \\text { 或 } l: \\quad \\phi_{1}\\left(x_{i}\\right)=0\\\\角位移边界条件满足： \\frac{\\partial \\phi_{1}\\left(x_{i}\\right)}{\\partial x_{i}}=\\left(\\frac{2 x_{i}}{l^{2}}\\right)\\left[1-\\left(\\frac{x_{i}}{l}\\right)\\right]^{2}+\\left(\\frac{x_{i}}{l}\\right)^{2}\\left[1-\\left(\\frac{x_{i}}{l}\\right)\\right]^{2}\\left(-\\frac{2}{l}\\right)\\\\ x=0\\ 或\\ l: \\frac{\\partial \\phi_{1}\\left(x_{i}\\right)}{\\partial x_{i}}=0当界面 $o_3 $产生角位移时，各子结构满足几何边界条件的模态称为约束振型，取为： \\phi_{2}\\left(x_{i}\\right)=\\left(\\frac{x_{i}}{l}\\right)^{2}\\left[1-\\left(\\frac{x_{i}}{l}\\right)\\right]根据相关的约束求导可得： \\frac{\\partial \\phi_{2}\\left(x_{i}\\right)}{\\partial x_{i}}=\\left(\\frac{2 x_{i}}{l^{2}}\\right)\\left[1-\\left(\\frac{x_{i}}{l}\\right)\\right]+\\left(\\frac{x_{i}}{l}\\right)^{2}\\left(-\\frac{1}{l}\\right)\\\\ x= l: \\frac{\\partial \\phi_{2}\\left(x_{i}\\right)}{\\partial x_{i}}=-\\frac 1 l于是我们把以上的两个模态作为基准： 于是计算系统的动能可得： T=\\frac{1}{2} \\int_{0}^{l} \\rho A\\left(\\left(\\frac{\\partial y_{1}\\left(x_{1}, t\\right)}{\\partial t}\\right)^{2}+\\left(\\frac{\\partial y_{2}\\left(x_{2}, t\\right)}{\\partial t}\\right)^{2}\\right) d x \\quad \\widetilde{\\boldsymbol{M}}=\\left[\\begin{array}{cc}\\widetilde{\\boldsymbol{M}}_{1} & \\\\ & \\widetilde{\\boldsymbol{M}}_{2}\\end{array}\\right] \\\\ =\\frac{1}{2} \\dot{\\boldsymbol{\\Psi}}_{1}^{T} \\widetilde{\\boldsymbol{M}}_{1} \\dot{\\boldsymbol{\\Psi}}_{1}+\\frac{1}{2} \\dot{\\boldsymbol{\\Psi}}_{2}^{T} \\widetilde{\\boldsymbol{M}}_{2} \\dot{\\boldsymbol{\\Psi}}_{2}=\\frac{1}{2} \\dot{\\boldsymbol{\\Psi}}^{T} \\widetilde{\\boldsymbol{M}} \\dot{\\boldsymbol{\\Psi}} \\quad \\boldsymbol{\\Psi}=\\left(\\begin{array}{llll}\\zeta_{1} & \\zeta_{2} & \\zeta_{3} & \\zeta_{4}\\end{array}\\right)^{T}计算系统的势能可得： V=\\frac{1}{2} \\int_{0}^{l} E I\\left(\\left(\\frac{\\partial^{2} y_{1}\\left(x_{1}, t\\right)}{\\partial x_{1}^{2}}\\right)^{2}+\\left(\\frac{\\partial^{2} y_{2}\\left(x_{2}, t\\right)}{\\partial x_{2}^{2}}\\right)^{2}\\right) d x =\\frac{1}{2} \\boldsymbol{\\Psi}^{T} \\widetilde{\\boldsymbol{K}} \\boldsymbol{\\Psi} \\\\ \\boldsymbol{\\Psi}=\\left(\\begin{array}{llll}\\zeta_{1} & \\zeta_{2} & \\zeta_{3} & \\zeta_{4}\\end{array}\\right)^{T} \\quad \\widetilde{\\boldsymbol{K}}=\\left[\\begin{array}{ll}\\widetilde{\\boldsymbol{K}}_{1} & \\\\ & \\widetilde{\\boldsymbol{K}}_{2}\\end{array}\\right] $\\Psi$ 只是单纯的组合排列，其中向量的独立性是未知的，其正交性或独立性需要进行分析和验证。 由上述的计算结果可知，向量的1、3不独立，2、4不独立，下面来分析以下1、2的独立性。 下面是$\\xi_1=\\xi_3,\\xi_2=\\xi_4$的独立性证明，使用了位移和弯矩的协调条件。 \\frac{\\partial y_{1}(l, t)}{\\partial x_{1}}=\\left.\\frac{\\partial \\phi_{1}\\left(x_{1}\\right)}{\\partial x_{1}}\\right|_{x=l} \\zeta_{1}(t)+\\left.\\frac{\\partial \\phi_{2}\\left(x_{1}\\right)}{\\partial x_{1}}\\right|_{x=l} \\zeta_{2}(t)=\\left(-\\frac{1}{l}\\right) \\zeta_{2}(t)\\\\ \\frac{\\partial y_{2}(l, t)}{\\partial x_{2}}=\\left.\\frac{\\partial \\phi_{1}\\left(x_{2}\\right)}{\\partial x_{2}}\\right|_{x=l} \\zeta_{3}(t)+\\left.\\frac{\\partial \\phi_{2}\\left(x_{2}\\right)}{\\partial x_{2}}\\right|_{x=l} \\zeta_{4}(t)=\\left(-\\frac{1}{l}\\right) \\zeta_{4}(t) 由弯矩的协调条件可得：$\\xi_1=\\xi_3$ 由位移的协调条件可得：$\\xi_2=\\xi_4$ 其中，$\\beta=\\left[\\begin{array}[c]\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 1 &amp; 0\\\\ 0 &amp; 1 \\end{array}\\right]$ ，通过上述转换我们可以的得出一个相互独立的模态坐标，这里我们可以看出$y_2(x,t)$ 的模态坐标和 $y_1$是一样的。 总结： 将原系统合理的差分为若干个独立的子结构； 根据子结构，列出合理的质量阵和刚度阵，并得到系统的动力学方程； 然后根据协调条件，对动力学方程进行修改，使得模态坐标彼此独立， 得出新的系统动力学方程。 有限元法 20世纪五六十年代发展起来的方法 吸取了集中质量法与假设振型法的优点 将复杂结构分割成有限个单元，单元端点称为节点，将节点的位移作为广义坐标，并将单元的质量和刚度集中到节点上； 每个单元作为弹性体，单元内各点的位移用节点位移的插值函数表示(单元的假设模态)； 由于是仅对单元、而非整个结构取假设振型，因此振型函数可取得十分简单，并且可令各个单元的模态相同 有限元法是目前工程中计算复杂结构广泛使用的方法 以杆的纵向振动和梁的弯曲振动为例进行介绍 杆的纵向振动单元质量矩阵和刚度矩阵的求解 以上其实就是一个杆的一个微小的单元体，其实是很小的。 对于x位置截面的位移$u(x,t)$的表达式是： u(x, t)=\\sum_{i=1}^{2} N_{i}(x) u_{i}(t) \\\\ N_{1}(x)=1-\\frac{x}{l} \\quad N_{2}(x)=\\frac{x}{l} \\\\ u(x, t)=\\boldsymbol{N}^{T} \\boldsymbol{u}_{e} \\\\ u_{e}=[u_1(t),u_2(t)]^T,\\ N=\\left(1-\\dfrac x l,\\dfrac x l \\right)^T $u_e=[u_1(t),u_2(t)]^T$ 为广义坐标。 计算单元体的动能可得： T_{e}=\\frac{1}{2} \\int_{0}^{l} \\rho A\\left(\\frac{\\partial u(x, t)}{\\partial t}\\right)^{2} d x=\\frac{1}{2} \\dot{\\boldsymbol{u}}_{e}^{T} \\boldsymbol{m}_{e} \\dot{\\boldsymbol{u}} \\\\ m_{e}=\\int_{0}^{l} \\rho A \\mathbf{N} \\mathbf{N}^{T} d x=\\frac{\\rho A l}{6}\\left(\\begin{array}{ll}2 & 1 \\\\ 1 & 2\\end{array}\\right) \\\\其中，$m_e$ 为单元质量阵。 单元体的势能为: V_{e}=\\frac{1}{2} \\int_{0}^{l} E A\\left(\\frac{\\partial u(x, t)}{\\partial x}\\right)^{2} d x =\\frac{1}{2} \\boldsymbol{u}_{e}^{T} \\boldsymbol{k}_{e} \\boldsymbol{u}_{e} \\quad E: 弹性模量 \\\\ \\boldsymbol{k}_{e}=\\int_{0}^{l} E A \\boldsymbol{N}^{\\prime} \\boldsymbol{N}^{\\prime T} d x=\\frac{E A}{l}\\left(\\begin{array}{cc}1 & -1 \\\\ -1 & 1\\end{array}\\right) \\quad \\quad \\boldsymbol{N}^{\\prime}=\\left(\\begin{array}{ll}-\\dfrac{1}{l} & \\dfrac{1}{l}\\end{array}\\right)^{T} $f(x, t)$对虚位移 $\\delta u(x, t) $ 的虚功，由$u(x, t)=\\boldsymbol{N}^{T}\\boldsymbol{u}_{e}$ 可得： \\delta W=\\frac{1}{2} \\int_{0}^{l} f(x, t) \\delta u(x, t) d x=\\boldsymbol{F}_{e}^{T} \\delta \\boldsymbol{u}_{e}其中，$\\boldsymbol{F}_{e} $与节点坐标 $\\boldsymbol{u}_{e} $ 对应的单元广义力列阵为：$\\boldsymbol{F}_{e}=\\int_{0}^{l} f(x, t) \\boldsymbol{N} d x$ 如果轴向力为常值力，可得：$\\boldsymbol{F}_{e} =\\dfrac {fl} 2 (1 \\quad 1)^T$ 全系统的动力学方程 单元质量矩阵： $\\boldsymbol{m}_{e 1}=\\boldsymbol{m}_{e 2}=\\frac{\\rho A l}{3}\\left(\\begin{array}{ll}2 &amp; 1 \\\\ 1 &amp; 2\\end{array}\\right) \\quad \\boldsymbol{m}_{e 3}=\\frac{\\rho A l}{6}\\left(\\begin{array}{ll}2 &amp; 1 \\\\ 1 &amp; 2\\end{array}\\right) $ 单元刚度矩阵：$\\boldsymbol{k}_{e 1}=\\boldsymbol{k}_{e 2}=\\frac{2 E A}{l}\\left(\\begin{array}{cc}1 &amp; -1 \\\\ -1 &amp; 1\\end{array}\\right) \\quad \\boldsymbol{k}_{e 3}=\\frac{E A}{l}\\left(\\begin{array}{cc}1 &amp; -1 \\\\ -1 &amp; 1\\end{array}\\right) $ 这里保证我们的$q_1,q_2,q_3$是相互独立。 全系统的动能： T=\\sum_{i=1}^{3} T_{e i}=\\frac{1}{2} \\sum_{i=1}^{3} \\dot{\\boldsymbol{u}}_{e i}^{T} \\boldsymbol{m}_{e i} \\dot{\\boldsymbol{u}}_{e i}=\\frac{1}{2} \\dot{\\boldsymbol{U}}^{T} \\widetilde{\\boldsymbol{M}} \\dot{\\boldsymbol{U}}=\\frac{1}{2} \\dot{\\boldsymbol{q}}^{T} \\boldsymbol{M} \\dot{\\boldsymbol{q}}\\\\ T_{e}=\\frac{1}{2} \\dot{\\boldsymbol{u}}_{e}^{T} \\boldsymbol{m}_{e} \\dot{\\boldsymbol{u}}_{e} \\quad \\boldsymbol{U}=\\boldsymbol{\\beta} \\boldsymbol{q}其中，$ \\tilde{\\boldsymbol{M}}=\\left(\\begin{array}{ccc}\\boldsymbol{m}_{e 1} &amp; \\boldsymbol{0} &amp; \\boldsymbol{0} \\\\ \\boldsymbol{0} &amp; \\boldsymbol{m}_{e 2} &amp; 0 \\\\ \\boldsymbol{0} &amp; \\boldsymbol{0} &amp; \\boldsymbol{m}_{e 3}\\end{array}\\right) \\in R^{6 \\times 6} $，$\\boldsymbol{M}=\\boldsymbol{\\beta}^{T} \\tilde{\\boldsymbol{M}} \\boldsymbol{\\beta} \\in R^{3 \\times 3} $ M为进行坐标独立正交化之后的形式。 质量矩阵 M 也可直接利用单元质量矩阵组集而成 (叠加法)：将单元质量矩阵 $m_{e1}$、$m_{e2}$ 和 $m_{e3}$ 的各个元素统一按 $q_i(i=1,2,3)$ 的下标重新编号，放入M 中与编号相对应的行和列中 $q_{1}=u_{2}=u_{3}, \\quad q_{2}=u_{4}=u_{5}, \\quad q_{3}=u_{6}$ 这里$u_1=0$，因此对应的都是0 这里其实就是将原来的单元体(i=1,2,3,4,5,6)的质量阵使用广义坐标(1,2,3)重新进行编号，相同的地方直接相加。 和广义坐标 $q=(q_1\\quad q_2 \\quad q_3)^T$ 相对应的质量矩阵： \\boldsymbol{M}=\\left(\\begin{array}{lll}m_{11} & m_{12} & m_{13} \\\\m_{21} & m_{22} & m_{23} \\\\m_{31} & m_{32} & m_{33}\\end{array}\\right)=\\frac{\\rho A l}{6}\\left(\\begin{array}{ccc}{4+4} & 2 & 0 \\\\2 & 4+2 & 1 \\\\ 0 & 1 & 2\\end{array}\\right)全系统的势能： V=\\sum_{i=1}^{3} V_{e i}=\\frac{1}{2} \\sum_{i=1}^{3} \\boldsymbol{u}_{e i}^{T} \\boldsymbol{k}_{e i} \\boldsymbol{u}_{e i}=\\frac{1}{2} \\dot{\\boldsymbol{U}}^{T} \\widetilde{\\boldsymbol{K}} \\dot{\\boldsymbol{U}}=\\frac{1}{2} \\boldsymbol{q}^{T} \\boldsymbol{K} \\boldsymbol{q}\\\\ V_{e}=\\frac{1}{2} \\boldsymbol{u}_{e}^{T} \\boldsymbol{k}_{e} \\boldsymbol{u}_{e} \\quad \\boldsymbol{U}=\\boldsymbol{\\beta} \\boldsymbol{q} \\quad \\boldsymbol{K}=\\boldsymbol{\\beta}^{T} \\widetilde{\\boldsymbol{K}} \\boldsymbol{\\beta} \\\\ \\widetilde{\\boldsymbol{K}}=\\left(\\begin{array}{ccc}\\boldsymbol{k}_{e 1} & \\boldsymbol{0} & 0 \\\\ \\boldsymbol{0} & \\boldsymbol{k}_{e 2} & \\boldsymbol{0} \\\\ 0 & \\boldsymbol{0} & \\boldsymbol{k}_{e 3}\\end{array}\\right) \\in R^{6 \\times 6}其中，$\\boldsymbol{\\beta}=\\left(\\begin{array}{cccccc}0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right)^{T} $ 也可以使用组集方法得：$\\boldsymbol{K}=\\frac{E A}{l}\\left(\\begin{array}{ccc}2+2 &amp; -2 &amp; 0 \\\\ -2 &amp; 2+1 &amp; -1 \\\\ 0 &amp; -1 &amp; 1\\end{array}\\right)$ 当杆上有常值轴向力作用时，三根杆的广义外力阵为： \\boldsymbol{F}_{e 1}=\\boldsymbol{F}_{e 2}=\\boldsymbol{F}_{e 3}=\\frac{f l}{2}\\left(\\begin{array}{ll}1 & 1\\end{array}\\right)^{T}系统的广义力阵：$\\boldsymbol{F}=\\left(\\begin{array}{lll}\\boldsymbol{F}_{e 1}^{T} \\quad \\boldsymbol{F}_{e 2}^{T} \\quad \\boldsymbol{F}_{e 3}^{T}\\end{array}\\right)^{T} \\in R^{6 \\times 1}$ 作用力的总虚功：$\\delta W=\\boldsymbol{F}^{T} \\delta \\boldsymbol{U}=\\boldsymbol{Q}^{T} \\delta \\boldsymbol{q} \\quad \\boldsymbol{U}=\\boldsymbol{\\beta} \\boldsymbol{q}$ 这里$Q=\\beta^TF$ 和广义的坐标$q$ 对应的广义力阵 (叠加法)也可将$F_{e1}$、$F_{e2}$和 $F_{e3}$ 的各个元素统一按 $q_i (i=1,2,3)$ 的下标重新编号，放入 Q 中与编号相对应的行和列中： \\boldsymbol{Q}=\\frac{f l}{2}(1+1 \\quad 1 \\quad 1)^{T}=\\frac{f l}{2}\\left(\\begin{array}{lll}2 & 1 & 1\\end{array}\\right)^{T}用广义坐标阵 q 表示的广义质量阵、广义刚度阵和广义外力阵： 用广义坐标阵 q 表示的全系统的动力学方程： \\boldsymbol{M} \\ddot{\\boldsymbol{q}}+\\boldsymbol{K} \\boldsymbol{q}=\\boldsymbol{Q}\\\\ \\frac{\\rho A l}{6}\\left(\\begin{array}{ccc}8 & 2 & 0 \\\\ 2 & 6 & 1 \\\\ 0 & 1 & 2\\end{array}\\right)\\left(\\begin{array}{l}\\ddot{q}_{1} \\\\ \\ddot{q}_{2} \\\\ \\ddot{q}_{3}\\end{array}\\right)+\\frac{E A}{l}\\left(\\begin{array}{ccc}4 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 1\\end{array}\\right)\\left(\\begin{array}{l}q_{1} \\\\ q_{2} \\\\ q_{3}\\end{array}\\right)=\\frac{f l}{2}\\left(\\begin{array}{l}2 \\\\ 1 \\\\ 1\\end{array}\\right) 其实有限元方法的基本步骤和模态叠加法基本类似： 首先，划分为有限个单元；之后，采用能量法来得到刚度阵和质量阵；最后，一节点的坐标为系统的广义坐标，来建立系统的动力学方程。 有限元方法建立的模型是有限自由度模型，广义坐标是节点的位移，是在物理空间上的，但是模态综合法建立的动力学方程的坐标是模态坐标，在模态空间上的。 梁的弯曲振动单元质量矩阵和刚度矩阵的求解 选为均质梁在端点常值位移作用下的静挠度曲线(多项式函数)： N_{i}(x)=a_{0}+a_{1} x+a_{2} x^{2}+a_{3} x^{3}, \\quad(i=1,2,3,4) $N_{3}(0)=N_{3}^{\\prime}(0)=0,N_{3}(l)=1,N_3^{\\prime}(l)=1$ $N_{4}(0)=N_{4}^{\\prime}(l)=N_{4}(0)=0, \\quad N_{4}^{\\prime}(l)=1$ 上述边界条件的意义就是跟$u_i(t)$是对应的，比如$N_1(0)=1$ 是由于$u_1(t)$ 对应的是x=0处的位移，$N_2(0)^{\\prime}=0$ 是由于$u_2(t)$ 对应的是x=0处的转角，依次类推。 形函数列阵：$\\boldsymbol{N}=\\left[\\begin{array}{llll}N_{1}(x) &amp; N_{2}(x) &amp; N_{3}(x) &amp; N_{4}(x)\\end{array}\\right]^{T}$ 代入$y(x, t)=\\boldsymbol{N}^{T} \\boldsymbol{u}_{e}$ 可得单元的动能为： T_{e}=\\dfrac{1}{2} \\int_{0}^{l} \\rho A\\left(\\dfrac{\\partial y(x, t)}{\\partial t}\\right)^{2} d x=\\dfrac{1}{2} \\dot{\\boldsymbol{u}}_{e}^{T} \\boldsymbol{m}_{e} \\dot{\\boldsymbol{u}}_{e}其中，$\\rho$代表材料密度，A代表截面积。 同理代入可得单元的势能： V_{e}=\\frac{1}{2} \\int_{0}^{l} E I\\left(\\frac{\\partial y^{2}(x, t)}{\\partial x^{2}}\\right)^{2} d x=\\frac{1}{2} \\boldsymbol{u}_{e}^{T} \\boldsymbol{k}_{e} \\boldsymbol{u}_{e} \\quad E I:抗弯刚度 假设梁上有分布外力，$f(x, t)$对虚位移 $\\delta u(x, t) $ 的虚功，由$y(x, t)=\\boldsymbol{N}^{T}\\boldsymbol{u}_{e}$ 可得： \\delta W=\\frac{1}{2} \\int_{0}^{l} f(x, t) \\delta y(x, t) d x=\\boldsymbol{F}_{e}^{T} \\delta \\boldsymbol{u}_{e}其中，$\\boldsymbol{F}_{e} $与节点坐标 $\\boldsymbol{u}_{e} $ 对应的单元广义力列阵为：$\\boldsymbol{F}_{e}=\\int_{0}^{l} f(x, t) \\boldsymbol{N} d x$ 对于均布载荷，$f$为常数，可得：$\\boldsymbol{F}_{e} =\\dfrac {fl} 2 (1 \\quad \\dfrac 1 6 \\quad 1 \\quad -\\dfrac 1 6)^T$ 全系统的动力学方程以上对单元所作的分析必须进行综合，以扩展到总体结构 T=\\frac{1}{2} \\dot{\\boldsymbol{q}}^{T} \\boldsymbol{M} \\dot{\\boldsymbol{q}} \\quad \\boldsymbol{M}=\\boldsymbol{\\beta}^{T} \\tilde{\\boldsymbol{M}} \\boldsymbol{\\beta} \\\\\\boldsymbol{\\beta}=\\left(\\begin{array}{llllllll}0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\end{array}\\right)^{T} \\quad \\widetilde{\\boldsymbol{M}}=\\left(\\begin{array}{cc}\\boldsymbol{m}_{e 1} & \\boldsymbol{0} \\\\ 0 & \\boldsymbol{m}_{e 2}\\end{array}\\right)因此, 有 假设梁上有简谐变化的均布载荷 $f(x,t)=f_0\\sin\\omega t$ \\boldsymbol{F}_{e 1}^{T}=\\boldsymbol{F}_{e 2}^{T}=(1 \\quad \\dfrac 1 6 \\quad 1 \\quad -\\dfrac 1 6)^T\\dfrac {f_0l} 2\\sin\\omega t系统的广义力阵：$\\boldsymbol{F}=\\left(\\begin{array}{lll}\\boldsymbol{F}_{e 1}^{T} \\quad \\boldsymbol{F}_{e 2}^{T} \\end{array}\\right)^{T} \\in R^{4 \\times 1}$ 与节点坐标U对应的广义力阵 $\\boldsymbol{U}=\\left(\\begin{array}{llll}u_{1} &amp; u_{2} &amp; \\cdots &amp; u_{8}\\end{array}\\right)^{T}_{8 \\times1}$ 作用力的总虚功：$\\delta W=\\boldsymbol{F}^{T} \\delta \\boldsymbol{U}=\\boldsymbol{Q}^{T} \\delta \\boldsymbol{q}$ 这里$Q=\\beta^TF$ 和广义的坐标$q$ 对应的广义力阵 (叠加法)也可将$F_{e1}$、$F_{e2}$和 $F_{e3}$ 的各个元素统一按 $q_i (i=1,2,3)$ 的下标重新编号，放入 Q 中与编号相对应的行和列中： \\boldsymbol{Q}=(1 \\quad 0 \\quad \\dfrac1 2\\quad -\\dfrac l {12})^{T}\\dfrac {f_0l} 2\\sin\\omega t用广义坐标阵 q 表示的广义质量阵、广义刚度阵和广义外力阵： 用广义坐标阵 q 表示的全系统的动力学方程：$\\boldsymbol{M} \\ddot{\\boldsymbol{q}}+\\boldsymbol{K} \\boldsymbol{q}=\\boldsymbol{Q}$ 有限元方法的主要思想：是将结构划分为非常多的有限单元，每个单元都是弹性体，将单元的质量和刚度都向节点进行集中，最后以所有节点的位移为广义坐标，来建立一个有限多自由度系统的动力学方程。","link":"/2022/03/19/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E5%85%AD%E7%AB%A0-%E8%BF%9E%E7%BB%AD%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%AF%E5%8A%A8-%E8%BF%91%E4%BC%BC%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"},{"title":"Ch1-Ch2 概述和预备知识","text":"hljs.initHighlightingOnLoad(); 本文主要讲解的是视觉SLAM的第一章和第二章的内容，主要介绍了SLAM是什么，有几部分组成，本书主要介绍的主要的内容等。对于Cmake和相关的作业后面还会有一些学习的补充。 课程内容与预备知识SLAM的应用实例SLAM是做什么的？ 室内室外的定位过程 对于视觉SLAM其实是通过使用相机本身的成像来进行位置的估计，通过提取相关的特征点，来反推相机的姿态的。本课程主要探索的是，这些点是如何提取的，以及如何根据点的信息来反推相机的姿态信息。 当然，可以使用GPS激光等其他的传感器来进行分析的，但是现阶段相机是完全可以实现上述定位的过程的。 稀疏-半稠密重建 如图就是一个典型的半稠密建图semi-dense，这样的工作主要是使用单目相机等廉价的相机来实现的。 稠密重建 使用的是比较昂贵的RGBD相机来实现的，既可以采集到图像也可以采集到深度信息。 总结：SLAM可以做的就是定位和环境重建。 SLAM可以用在什么地方？ 手持设备的定位； 自动驾驶汽车；SLAM的精度要比GPS搞，GPS一般可以达到10m左右。SLAM的定位精度在5cm，但是这个受到相机精度的影响。 AR增强现实。 课程内容 SLAM：Simultaneous Localization and Mapping，即同时定位和地图构建。 它是指搭载特定的传感器的主体，在没有环境先验信息的情况下，于运动的过程建立环境模型，同时估计自己的运动。对于以相机为主体传感器的系统，被称为视觉SLAM。 SLAM背后的数学知识，主要是前六章的在讲解，主要有三维空间的刚体运动，李群和李代数、非线性优化。 SLAM背后的计算机视觉，主要是低五章的相机模型，PCL和Opencv等。 工程实践主要是后七章的内容，主要有视觉里程计VIO、后端、回环检测和建图。 教材推荐： 视觉SLAM十四讲，比较浅显，容易入门。 多视图几何MVG，比较全面的计算机几何的一本书，太全面了，会淹没在一些细节中，导致比较难以理解。 机器人状态估计，但是比较偏robotic，偏估计，视觉的东西比较少，数学公式极其多，很深刻。 本门看强调在理解公式的基础上进行代码的实践。 其中，第九讲是个工程问题，自己照着敲代码就可以，第十四讲是展望。 预备知识与课程使用的环境 数学：高等数学、线性代数（矩阵论）、概率论 编程：C++、Linux，了解语法和基本命令即可 英语：文献、文档阅读能力 环境：Ubuntu 16.04 初识SLAM 一般情况下，这两个问题是同时解决的。 传感器主要分为内置的和外置的。 外置的传感器会对使用范围有限制，对使用的环境有多的依赖，而内置的相对的比较自由一些。 内置的传感器测量的数据比较简介，是将测量的数据通过传感器模型进行处理才可以得到想要的数据，而外置传感器如GPS，可以直接获取。 相机的特点是便宜，轻便，信息丰富，适合民用。 深度相机：会产生两个图，RGB和深度图，通过物理方法来获取深度信息，可以获取立体信息。 主要使用的是TOF和红外结构光的方法； 但是其测量的距离可能较短，其次就是噪声大容易受到日光的干扰，无法测量透视的物体； 所以其主要在室内使用，室外较难应用。 双目相机：会有两个图，通过视差来计算深度信息，可以获取立体的信息。 是通过两个相机之间的距离，基线来确定的； 其测量的范围也是受到基线和镜头分辨率的的影响； 对计算的要求较高，需要GPU加速。 单目相机：无深度信息，需要其他的手段来估计。 就是距离较近的事物移动速度快； 较远的事物移动速度慢，这样可以较好的来估计位置的相对关系； 但是，其存在一定的尺度不确定性，就是我们不知道其这是的尺寸，只能得到相对值。 根据近处的事物运动快，远处的事物运动慢来进行间接的测距，但在两张图上捕捉同一个特征点是有点难度的。单双目其实都是这个原理 解决短距离和小范围的情况时，可以使用视觉里程计VO，输入时一张一张的图片，输出为预测得到的距离定位的信息。 但如果时远距离长时间的定位和建图的，之前产生误差和噪声，随着世实践会逐渐的累积，产生漂移，需要后端来进行优化。其主要解决的是一个估计和优化问题，没有很多计算机视觉的内容。 最后又回到起始点，此时回来以后不一定会回到原点，需要进行检测，将偏移的轨迹归位。 有些场合还是需要建图算法。 前端，即视觉里程计，就是用来计算和估计图像的相对位置。 后端，滤波和图优化，就是进行噪声消除和优化误差的。 视觉SLAM流程包括以下步骤: 传感器信息读取: 在视觉SLAM中主要为相机图像信息的读取和预处理。如果是在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。 视觉里程计(Visual Odometry,VO): 视觉里程计的任务是估算相邻图像间相机的运动,以及局部地图的样子。VO又称为前端(Front End)。 视觉里程计不可避免地会出现累积漂移(Accumulating Drift)问题。 需要后端和回环检测来减少漂移现象。 前端的主要任务是图像的特征提取和匹配。 后端优化 (Optimization): 后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息,对它们进行优化,得到全局一致的轨迹和地图。由于接在VO之后，又称为后端(Back End)。 在视觉 SLAM中，后端则主要是滤波与非线性优化算法。 其实就是状态估计问题，机器人学中的状态估计和因子图主要解决的其实就是后端的问题。 回环检测 (Loop Closing): 回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。 回环检测的核心问题就是减少消除由于时间漂移造成的累积误差 其次就是如何识别出是否已经回到原点了。 建图 (Mapping): 它根据估计的轨迹,建立与任务要求对应的地图. 地图的形式包括： 度量地图：精确表示地图物体的位置关系分为稀疏和稠密，这里稀疏图仅仅包就特定的物体作为路标，而稠密图将三维空间网格化，每个网格仅仅有占用、未知、空闲三个状态。 拓扑地图：更强调地图元素之间的关系，有节点和边组成，并且仅仅考虑节点的连通性。 ​ 视觉SLAM的数学描述这里的核心方程就是一个运动方程，一个观测方程，主要解决的就是如何通过带噪声的数据，来估计内部的隐变量(即确定运动方程的参数) 这里，设$x_k$为k时刻的相机的状态位置，$y_i$ 代表路标； $z_{k,j}$ 代表k时刻第j个路标的测量值，其可以是像素在照片中的坐标（相机）或是一个距离和角度信息（激光），跟传感器的形式有关； $v_{k,j}$为测量噪声。 x_k \\rightarrow x_{k+1}\\\\ x_{k+1}=f(x_k,u_k) \\quad 运动方程\\quad u_k为输入\\\\ z_{k,j}=g(x_k,y_j)+v_{k,j} \\quad 测量方程 $(u_k,z_{k,j}) \\rightarrow x_k,y_j$ ：这里对于 $x_k$ 的确定称为定位，对于 $y_j$ 的确定称为建图。 以下是高博的手稿： 噪声$v_{k,j}$ 可以分为高斯系统和非高斯系统： 最简单的线性高斯系统，使用kalman filter 就可以很好的解决； 对于复杂非线性的高斯系统，可以使用Extended Kalman Filter来解决，在21世纪初期，EKF一直都是主流算法。 但是EKF有两个主要的缺点，主要是线性化之后的误差和噪声高斯化的假设，于是引入了粒子滤波等。 现如今，滤波算法都不如图优化的方法，如果计算条件允许，我们将优先考虑使用图优化的方法。 代码实战本章的代码实在太简单了，因此这里将一些基础知识： 之前我们编译可以直接使用g++ 或 clang (高博说clang的效果是比g++)要好一点的， 1234g++ helloSLAM.cpp# 使用g++直接来编译程序clang helloSLAM.cpp# 使用clang来编译程序 但是，对于大规模的项目，比如里面包含了`xxx.h 之类的库文件的直接编译就显得麻烦了，因此这里使用Cmake编译工具。 1234567891011121314151617181920212223# 声明要求的 cmake 最低版本cmake_minimum_required(VERSION 2.8)# 声明一个 cmake 工程project(HelloSLAM)# 设置编译模式set(CMAKE_BUILD_TYPE &quot;Debug&quot;)# 添加一个可执行程序# 语法：add_executable( 程序名 源代码文件 ）add_executable(helloSLAM helloSLAM.cpp)# 添加hello库add_library(hello libHelloSLAM.cpp)# 共享库add_library(hello_shared SHARED libHelloSLAM.cpp)# 添加可执行程序调用hello库中函数add_executable(useHello useHello.cpp)# 将库文件链接到可执行程序上target_link_libraries(useHello hello_shared) add_executable 函数是将含有main的文件编译称为可执行文件 add_library 函数将.h文件编成为库文件，方便调用。 target_link_libraries将库文件链接到可执行程序上，这样才能其真正的调用的.h文件。","link":"/2022/03/20/ch1-ch2-%E6%A6%82%E8%BF%B0%E5%92%8C%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"Typora","slug":"Typora","link":"/tags/Typora/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"机械振动","slug":"机械振动","link":"/tags/%E6%9C%BA%E6%A2%B0%E6%8C%AF%E5%8A%A8/"},{"name":"Latex","slug":"Latex","link":"/tags/Latex/"},{"name":"图像分类","slug":"图像分类","link":"/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"},{"name":"控制理论","slug":"控制理论","link":"/tags/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"control theory","slug":"control-theory","link":"/tags/control-theory/"},{"name":"auto-encoder","slug":"auto-encoder","link":"/tags/auto-encoder/"},{"name":"生成模型","slug":"生成模型","link":"/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"无监督学习","slug":"无监督学习","link":"/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"Generative Adversarial Networks","slug":"Generative-Adversarial-Networks","link":"/tags/Generative-Adversarial-Networks/"},{"name":"GAN实战","slug":"GAN实战","link":"/tags/GAN%E5%AE%9E%E6%88%98/"},{"name":"opencv","slug":"opencv","link":"/tags/opencv/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"object detection","slug":"object-detection","link":"/tags/object-detection/"},{"name":"latex","slug":"latex","link":"/tags/latex/"},{"name":"论文写作","slug":"论文写作","link":"/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"},{"name":"IEEE投稿","slug":"IEEE投稿","link":"/tags/IEEE%E6%8A%95%E7%A8%BF/"},{"name":"科研","slug":"科研","link":"/tags/%E7%A7%91%E7%A0%94/"},{"name":"SLAM","slug":"SLAM","link":"/tags/SLAM/"},{"name":"Computer Vision","slug":"Computer-Vision","link":"/tags/Computer-Vision/"},{"name":"c++","slug":"c","link":"/tags/c/"}],"categories":[{"name":"Typora","slug":"Typora","link":"/categories/Typora/"},{"name":"PyTorch教程与源码讲解","slug":"PyTorch教程与源码讲解","link":"/categories/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/"},{"name":"振动力学","slug":"振动力学","link":"/categories/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/"},{"name":"Latex","slug":"Latex","link":"/categories/Latex/"},{"name":"图像分类网络及Pytorch实现","slug":"图像分类网络及Pytorch实现","link":"/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/"},{"name":"现代控制理论","slug":"现代控制理论","link":"/categories/%E7%8E%B0%E4%BB%A3%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"auto-encoder","slug":"auto-encoder","link":"/categories/auto-encoder/"},{"name":"Generative Adversarial Networks","slug":"Generative-Adversarial-Networks","link":"/categories/Generative-Adversarial-Networks/"},{"name":"opencv","slug":"opencv","link":"/categories/opencv/"},{"name":"科研入门笔记","slug":"科研入门笔记","link":"/categories/%E7%A7%91%E7%A0%94%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"},{"name":"SLAM","slug":"SLAM","link":"/categories/SLAM/"}]}