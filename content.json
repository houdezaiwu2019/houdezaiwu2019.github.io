{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2022/01/18/hello-world/"},{"title":"Typora入门","text":"本文主要从目录设置、标题设置、特殊的标记符号、导入公式和列表以及图片图床的设置与排版来进行整理。 目录篇代码：[TOC] 标题篇对于Typora来说，主要分为快捷键方法和一般的直接写代码的方法，这里我比较推荐的还是直接写代码的方法，但是新手入门还是比较推荐先使用快捷键来进行入门。 标题的代码：==#+空格+内容== 这里有几个#就是几级标题 123# 一级标题## 二级标题### 三级标题 标题的快捷键：Ctrl+(数字) Ctrl+0 &lt;=&gt; 一般段落 Ctrl+1 &lt;=&gt; 一级标题 Ctrl+2 &lt;=&gt; 二级标题 连续两次Ctrl+同一个数字：取消标题操作 Ctrl+ +/-：代表标题的升级和降级 标记符号篇标记符号这里主要讲的是加粗、斜体、下滑线、高亮和内置公式与代码。 加粗：快捷键==Ctrl+B==，代码**text** 斜体：快捷键==Ctrl+I==，代码*text* 下滑线：快捷键==Ctrl+U==，代码**&lt;u&gt;text&lt;/u&gt;** 高亮：代码==text== 内置代码：快捷键==Ctrl+Shift+`==，代码`` 代码块：快捷键==Ctrl+Shift+K==， 代码: “```”+语言(c++/python/java) “```” 内置公式：代码\\aplha，这里使用的是latex代码进行标记的。 公式块：快捷键==Ctrl+Shift+M==，代码$$$ $$$ 1234$$ E_k = \\frac{1}{2}mv^2 \\tag{1.1}$$ E_k = \\frac{1}{2}mv^2 \\tag{1.1}引用：快捷键==Ctrl+Shift+Q，代码&gt;+空格+text 书籍是人类进步的阶梯。——高尔基 超级链接：代码[blibli](https://www.bilibili.com) [text](#name) eg1. blibli 的链接 eg2.目录 的链接 脚注：代码text[^name],之后在后面[^name]:text 1,这里添加成功后，后面会出现回车 ↩符号。 上标：代码X^2 下标：代码H~2~o 注意这里需要的一些额外的设置： 列表篇有序列表：快捷键==Ctrl+Shift+[==，代码1.+空格+text 无序列表：快捷键==Ctrl+Shift+]==，代码-+空格+text 注意：无序列表如果想切换到下一级时可以直接按==tab== 表格：快捷键==Ctrl+T==，之后弹出对话框，自己来选择需要的行数和列数。 今天 明天 早饭 午饭 晚饭 上述表格的源代码： 12345| | 今天 | 明天 || :--: | :--- | :--: || 早饭 | | || 午饭 | | || 晚饭 | | | 这里还是用快捷键吧，使用代码太麻烦了。 图片篇本地路径导入图片 一般来说，图片使用的是绝对路径，如下图所示为直接截图并粘贴等到的图片的路径。但这种语言不是md的语言。 &lt;img src=&quot;C:\\Users\\dell\\AppData\\Roaming\\Typora\\typora-user-images\\image-20220118215115730.png&quot; alt=&quot;image-20220118215115730&quot; style=&quot;zoom:50%;&quot; /&gt;。 这里的路径是Typora自行生成的，但实际输入的时候要输入代码![]()。 也可以用相对路径，代码是![](./+name.png/jpg) ./代表的是当前的文件夹。 ../代表的是上一级文件夹。 ![](./pic01.png) 对于.md来说，保存图片一般使用的是保存图片的链接，这样可以减小文件的空间（相比Word）。但这样会造成修改了文件或者图片的路径以及清除的C盘的缓存文件时会造成无法显示的问题。 利用图床导入图片这里有个比较矛盾的问题，使用GitHub的图床的话会造成国内没有VPN的用户无法访问，但是使用一般的七牛云的话要花钱，因此再三考虑我们使用码云gitee来作图床（弊端是图片的大小不能超过1M）。 将插入图片的无任何操作-&gt;上传图片 根据picgo的位置来配置路径。并且这里还要下载一个node.js，下载过程这里省略。 到picgo中下载gitee的插件，这里我们使用的是第三个，当然其他的两个也是可以的，但以下将围绕第三个gitee-uploader来配置。 在gitee里面创建仓库，注意创建时一定要点击初始化，只有这样才可以上传图片。如果没有点，那么创建好仓库时也可以点初始化。 要生成一个token，打开gitee的设置，点击生成令牌。点击设置-&gt;安全设置-&gt;私人令牌 设置权限，起个名字点击生成。注意令牌只弹出一次，最好找个记事本记录下来，防止自己忘记。 配置picgo，打开图床设置-&gt;gitee。 注意，这里自己仓库命名repo格式：用户名/仓库名。 但是之前遇到了问题，上传的图片在本地无法显示，其中的原因是gitee设置的是私有的，可以在仓库的管理界面更改。 Typora图片排版这里图片排版需要我们掌握一点点HTML(Hyper Text Markup Language)的语法知识。 图注 12345678# 这里为上面图片的HTML代码&lt;center&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;br&gt; // 这里&lt;br&gt;的意思是回车，如果没有br会和图片是并排的 图注&lt;/center&gt; 因此，要实现下面两个图片并排的效果，就可以直接将图片的放一起即可，如果实现连个图片的上下排列，这是需要在之间加一个&lt;br&gt;即可 这里的&lt;center&gt;是默认图片居中的意思。导入图片默认的情况就是都是居中的，如果要改成左对齐则将&lt;center&gt;--&gt;&gt;&lt;left&gt;即可实现，同理右对齐就用&lt;right&gt;。 图 晚安 123456789&lt;center&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;img src='https://gitee.com/houdezaiwu2022/image-bed/raw/master/Typora%E7%9A%84%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%92%8C%E4%BB%A3%E7%A0%81/202201211959259.png' width=25%&gt; &lt;br&gt; 图 晚安 &lt;img src=&quot;http://latex.codecogs.com/gif.latex? a^2&quot;&gt;&lt;/center&gt; 这里使用的是latex代码的形式来显示图注，虽然不是很好看，但是只是可以显示。","link":"/2022/01/21/Typora%E5%85%A5%E9%97%A8/"},{"title":"张量的创建与运算","text":"张量是一种特殊的数据结构，它于数组和矩阵很相似。我们用张量来标记模型的输入、输出和模型的参数。 张量（Tensor）和数组非常的相似，但是Tensor可以实现在硬件的加速。 tensors and NumPy 数组可以共享同一个内存。 hljs.initHighlightingOnLoad(); 初始化张量12import torchimport numpy as np 通过列表初始化 12345data = [ [1,2],[3,4] ]x_data = torch.tensor(data)type(data) # &lt;class 'list'&gt;type(x_data) # &lt;class 'torch.Tensor'&gt; 通过数组初始化 1234np_array = np.array(data)x_np = torch.from_numpy(np_array)# 默认的数据类型为 torch.float32 从另一个张量中初始化一个新的张量 1234567891011121314x_ones = torch.ones_like(x_data) # retains the properties of x_dataprint(f&quot;Ones Tensor: \\n {x_ones} \\n&quot;)x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_dataprint(f&quot;Random Tensor: \\n {x_rand} \\n&quot;)output:Ones Tensor: tensor([[1, 1], [1, 1]])Random Tensor: tensor([[0.4557, 0.7406], [0.5935, 0.1859]]) 用随机数或常值生成 12345678shape = ( 2,3,)rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)print(f&quot;Random Tensor: \\n {rand_tensor} \\n&quot;)print(f&quot;Ones Tensor: \\n {ones_tensor} \\n&quot;)print(f&quot;Zeros Tensor: \\n {zeros_tensor}&quot;) 这里的shape可以(2,3) or (2,3,) or [2,3] or [2,3,] Tensor的属性123456789101112tensor = torch.rand(3,4)tensor.shape # 形状 torch.Size([3, 4])tensor.dtype # 数据类型 torch.float32tensor.device # 使用的设备 device(type='cpu')print(f&quot;Shape of tensor: {tensor.shape}&quot;)print(f&quot;Datatype of tensor: {tensor.dtype}&quot;)print(f&quot;Device tensor is stored on: {tensor.device}&quot;)Shape of tensor: Datatype of tensor: torch.float32Device tensor is stored on: cpu Tensor的操作Tensor的主要操作包含100种包含矩阵运算、切片等。具体的操作API点击这里。 常用的操作如下： 关于Tensor的操作 is_tensor(x)判断是否为Tensor 123&gt;&gt;&gt; x=torch.tensor([1,2,3])&gt;&gt;&gt; torch.is_tensor(x)True is_nonzero(x)判断单一元素的Tensor是否为0 12345678910111213141516&gt;&gt;&gt; torch.is_nonzero(torch.tensor([0.]))False&gt;&gt;&gt; torch.is_nonzero(torch.tensor([1.5]))True&gt;&gt;&gt; torch.is_nonzero(torch.tensor([False]))False&gt;&gt;&gt; torch.is_nonzero(torch.tensor([3]))True&gt;&gt;&gt; torch.is_nonzero(torch.tensor([1, 3, 5]))Traceback (most recent call last):...RuntimeError: bool value of Tensor with more than one value is ambiguous&gt;&gt;&gt; torch.is_nonzero(torch.tensor([]))Traceback (most recent call last):...RuntimeError: bool value of Tensor with no values is ambiguous nueml(x)统计张量里的元素数目 123456&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5)&gt;&gt;&gt; torch.numel(a)120&gt;&gt;&gt; a = torch.zeros(4,4)&gt;&gt;&gt; torch.numel(a)16 创建Tensor操作 tensor(x)创建张量 from_numpy(a)也是创建张量，但是不同的是此方法的创建的tensor和array是共享内存的。 1234567&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.from_numpy(a)&gt;&gt;&gt; ttensor([ 1, 2, 3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1, 2, 3]) zeros() 创建全0矩阵 torch.zeros(size, , out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) 主要参数： *size：形状可以是元组、列表、切片等。 requires_grad=False：是否要求梯度。 dtype=None:默认是None,因为set_default_tensor_type()函数可以设置默认的dtype()类型。 arange()和range()函数都是用于一维计数，配合for来使用，基本的功能也是继承numpy和python的基本语法。 arange-&gt;[start, end)，size=$\\lceil \\frac{end-start}{step} \\rceil$，并且start、end、step —&gt;&gt; int range-&gt;[start, end]，size=$\\lfloor \\frac{end-start}{step} \\rfloor+1$，并且start、end、step —&gt;&gt; float eye(n,m=None) 单位矩阵 full(size,fill_value ) 就是用某个数值来填充某个size的张量。同样的还有full_like() 索引、切片、旋转以及聚合的操作 cat(tensors, dim=0) 1234567891011121314151617181920212223242526272829303132&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]])&gt;&gt;&gt; x1 = torch.randn(2, 3)&gt;&gt;&gt; x2 = torch.randn(2, 2)&gt;&gt;&gt; s = torch.cat((x1,x2),dim=1)&gt;&gt;&gt; stensor([[-0.5980, 0.2570, 1.1660, -0.0306, -1.0363], [ 0.2817, -0.1498, -0.6464, -0.3204, -1.6839]])# 这里dim=1上是不同的dim=0上是相同的，因此可以实现在dim=1上的切片，dim=0是用来对齐的。&gt;&gt;&gt; x3 = torch.randn(3, 2)&gt;&gt;&gt; s = torch.cat([x2,x3],dim=0)&gt;&gt;&gt; stensor([[-0.0306, -1.0363], [-0.3204, -1.6839], [-0.0858, 0.9804], [ 0.1920, -0.1728], [ 1.0318, -1.6507]])","link":"/2022/01/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/%E5%BC%A0%E9%87%8F%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E8%BF%90%E7%AE%97/"},{"title":"第一章绪论","text":"振动力学是力学专业的重要专业基础课，由此开始进入动力学的研究范畴，在航空航天、机械、土木等许多工程领域有着重要的应用背景。本文为第一章绪论，初步的介绍基本概念、学习目的、问题的提法、振动模型的建立以及系统的分类。 1.1 基本概念与学习目的 一般力学的对象主要是有限自由度系统的运动及其控制，有时它包含一个或多个无限自由度子系统。它包括运动稳定性理论、振动理论、动力系统理论、多体系统力学、机械动力学等。包括理论力学、振动力学、非线性力学。 固体力学是研究可变形固体在外界因素作用下所产生的应力、应变、位移和破坏等的力学分支。固体力学在力学中形成较早，应用也较广。包括材料力学、弹性力学、塑性力学、非线性介质力学。 流体力学是研究流体{液体和气体)的力学运动规律及其应用的学科。主要研究在各种力的作用下，流体本身的状态，以及流体和固体壁面,流体和流体间、流体与其他运动形态之间的相互作用的力学分支。包括流体力学、高等流体力学。 定义： 从广义上讲，如果表征一种运动的物理量作时而增大时而减小的反复变化，就可以称这种运动为振动 如果变化的物理量是一些机械量或力学量，例如物体的位移、速度、加速度、应力及应变等等，这种振动便称为机械振动 振动是自然界最普遍的现象之一 各种物理现象，诸如声、光、热等都包含振动 (1）心脏的搏动、耳膜和声带的振动，(2）桥梁和建筑物在风和地震作用下的振动,(3)飞机和轮船航行中的振动，（4）机床和刀具在加工时的振动 振动力学：借助数学、物理、实验和计算技术，探讨各种振动现象，阐明振动的基本规律，以便克服振动的消极因素，利用其积极因素，为合理解决各种振动问题提供理论依据。 学习的目的： 运用振动理论去创造和设计新型的振动设备、仪器及自动化装置 掌握振动的基本理论和分析方法，用以确定和限制振动对工程结构和机械产品的性能、寿命和安全的有害影响 1.2振动问题的提法 通常的研究对象被称作系统 它可以是一个零部件、一台机器、一个完整的工程结构 外部的激振力等因素被称作激励（输入） 系统发生的振动称为响应（输出） 振动的问题可以分为三类： 已知激励和系统，求响应（正问题） 动力响应分析，一般是建立微分方程，数值方法求解即可。 主要任务在于验算结构、产品等在工作时的动力响应（如变形、位移、应力等）是否满足预定的安全要求和其它要求 在产品设计阶段，对具体设计方案进行动力响应验算，若不符合要求再作修改，直到达到要求而最终确定设计方案，这一过程就是所谓的振动设计 这里比如响应中的反应设计的安全，加速度反应设计的舒适性特性。 高层建筑要有抗震要求，潜艇要有声学要求。 火箭-&gt;细长杆，尾端燃料推进器会给火箭本体造成振动（轴向、弯曲、扭振），从而影响卫星，因此火箭和推进器之间要加减震适配器。 第二类：已知和响应，求系统（第一个问题） 系统识别，系统辨识 求系统，主要是指获得对于系统的物理参数（如质量、刚数等）和系统关于振动的固有特性（如固有频率、主振型等）的认识 以估计物理参数为任务的叫做物理参数辨识，以估计系统振动固有特性为任务的叫做模态参数辨识或者试验模态分析 激励-&gt;卫星的姿态调整；响应-&gt;卫星的传感器（包括陀螺仪、端板上的加速度传感器） 机械臂本身的参数辨识 ; 空间机械臂抓取,输入(激励)为抓取前各个关节的力矩和加速度信息,输出(响应)为抓取后的力矩和加速度—&gt;&gt;得出被抓取的碎片的质量和惯量. 第三类：已知系统和响应，求激励（第二个逆问题） 环境预测 例如:为了避免产品在公路运输中的损坏，需要通过实地行车记录汽车振动和产品振动，以估计运输过程中是怎样的一种振动环境，运输过程对于产品是怎样的一种激励，这样才能有根据地为产品设计可靠的减震包装 对于火箭来说，系统的响应为传感器的输出，系统为火箭，反推激励-&gt;升空过程中的空气振动。 对于坦克来说，发射的炮弹为响应，坦克系统，反推地面环境对系统发射炮弹的激励。 相比之下，逆问题的求解要比正问题复杂，正问题建立ODE和PDE，再用数值方法求解即可。 1.3力学模型 振动系统的三要素：质量、刚度、阻尼 质量是感受惯性（包括转动惯量）的元件 刚度是感受弹性的元件 阻尼是耗能元件 描述振动系统的两类力学模型:（一般是建立数学模型） 连续系统模型（无限多自由度系统，分布参数系统） 结构参数（质量，刚度，阻尼等）在空间上连续分布 数学工具：偏微分方程 求解是将PDE离散化转化为ODE 离散系统模型（多自由度系统，单自由度系统） 结构参数为集中参量 数学工具：常微分方程 1.4振动及系统的分类按照微分方程的形式可以分为： 线性振动：描述其运动的方程为线性微分方程，相应的系统称为线性系统。线性系统的一个重要特性是线性叠加原理成立 非线性振动：描述其运动的方程为非线性微分方程，相应的系统称为非线性系统。对于非线性振动，线性叠加原理不成立 理论上，线性振动有统一的解决方法；而非线性振动就没有统一的方法。 按照激励的有无和性质可以分为： 固有振动：无激励时系统所有可能的运动集合（不是现实的振动，仅反映系统关于振动的固有属性) 自由振动：激励消失后系统所做的振动（现实的振动) 强迫振动：系统在外部激励作用下所做的振动 随机振动：系统在非确定性的随机激励下所做的振动，例如行驶在公路上的汽车的振动，激励无法使用函数来表示，只能用一个随机过程来表示 自激振动：系统受其自身运动诱发出来的激励作用而产生和维持的振动例如提琴发出的乐声，切削加工的高频振动，机翼的颤振等 参数振动：激励以系统本身的参数随时间变化的形式出现的振动，例如秋千被越荡越高。秋千受到的激励以摆长随时间变化的形式出现，而摆长的变化由人体的下蹲及站立造成。","link":"/2022/01/23/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%BB%AA%E8%AE%BA/"},{"title":"Latex公式排版快速教程","text":"src=\"//cdn.bootcss.com/highlight.js/9.2.0/highlight.min.js\"> hljs.initHighlightingOnLoad(); 本文主要关注Latex在公式排版的技巧，并且配合Markdown更好的编辑公式。主要包含希腊字母、上下标、分式与根式、运算符、标注、箭头、括号与定界符、多行公式、大括号、矩阵以及实战书写几个公式。 typora有些latex字符是不识别的，因此可能出现一些乱码问题。 行内公式创建行内的公式的方法有三种： 美元号：a+b=b+a 小括号：\\( a+b=b+a\\) math环境：\\begin{math} a+b=b+a \\end{math} 行间公式创建行间公式的主要方法有： 双美元号：$$ $$ 中括号：\\[a+b=b+a\\] displaymath环境：\\begin{displaymath} a+b=b+a \\end{displaymath} equation环境(自动生成编号)：\\begin{equation} a+b=b+a \\end{equation}，同时可以对生成的公式用\\label进行交叉引用，如下图所示。 equation*环境(不自动生成编号)：\\begin{equation*} a+b=b+a \\end{equation*}，同时可以对生成的公式用\\label进行交叉引用，如下图所示。 此时交叉引用的编号为小结的编号。 带星号的equation环境需要使用amsmath宏包。 1\\usepackage{amsmath} 希腊字母输入方法：\\+字母的拼写 常用的希腊字母表 对于有大写的字母来说，直接将首字母大写即可得到大写的希腊字母，对于没有大写的字母如$\\alpha$和$\\beta$，如果首字母大写对应的是$\\Alpha$和$\\Beta$即AB。 对于变体字的输入，直接在前面加var即可。 上下标 一般来说，上下标直接使用^和_即可。 对于斜体的要加大括号，否则就会只对第一个有效。 1234$ a^2 , a_2 x^{y+z},p_{ij},p_ij$ a^2 , a_2, \\, x^{y+z},p_{ij},p_ij 英文字母只有在表示变量(或单一字符的函数名称，如f(r))时才可使用斜体，其余情况都应使用罗马体(直立体)。 1234567$ x_i 此时为斜体 x_{\\rm i} 此时为直立体roman x_{\\mathrm i} \\rm is short for \\mathrm x_{\\text i} \\text{e} ,\\text{i} 自然对数和虚数单位也要为直立体$ x_i, i=1,2...n\\\\ x_{\\rm i} ,i是input\\\\ x_{\\mathrm i},x_{\\text i}\\\\ \\text{A B},\\rm{AB}\\\\ \\text{e} 自然对数,\\text{i} 虚数单位 \\text{}和\\rm{}的区别是前者支持空格，后者不支持空格。 分式于根式\\frac(fraction,分数) 12345$ \\frac{1}{2}, \\frac 1 2 \\frac{1}{x+y} \\frac{\\frac 1 x +1}{y+1},\\frac{\\dfrac 1 x +1}{y+1}$ 单个字符可以不用大括号，原理与上面相似。 如果感觉嵌套时的字符比较小的话，可以用\\dfrac{}{}. \\frac{1}{2}, \\frac 1 2 \\frac{1}{x+y}\\\\ \\frac{\\frac 1 x +1}{y+1},\\frac{\\dfrac 1 x +1}{y+1}\\sqrt[]{}(square root,平方根) 123$ \\sqrt 2, \\sqrt{x+y}, \\sqrt[3]{x+z}$ \\sqrt 2, \\sqrt{x+y}, \\sqrt[3]{x+z} 对于非平方根的数在\\sqrt后加[]来自定义。 普通运算符12345678910$ +- \\times,\\cdot,\\div \\pm,\\mp &gt;&lt;,\\ge,\\le,\\gg,\\ll,\\ne,\\approx,\\equiv \\cap,\\cup,\\in,\\notin,\\subseteq,\\subsetneqq,\\subnsetneqq \\varnothing,\\forall,\\exists,\\nexists\\because,\\therefore \\mathbb R,\\R,\\Q,\\N,\\Z_+ \\mathcal F,\\mathscr F$ $\\pm$ ：\\pm(plus-mius 正负号) $\\mp$：\\mp(mius-plus 负正号) $\\ge$：\\ge(greater than or equal 大于等于) $\\le$：\\le(less than or equal 大于等于) $\\gg,\\ll$：\\gg,\\ll（远大于,远小于） $\\ne$：\\ne（not equal, 不等于） $\\approx$：\\approx(approximate, 约等于) $\\equiv$：\\equiv(equivalent, 恒等的) $\\cap,\\cup$：\\cap，\\cup（交集，并集） $\\in,\\notin$：\\in,\\notin（属于，不属于） $\\subseteq,\\subsetneqq,\\subsetneq$：\\subseteq,\\subsetneqq,\\subsetneq（子集，真子集） $\\supseteq,\\supsetneqq,\\supsetneq$：\\suqseteq,\\suqsetneqq,\\suqsetneq（与上面相反） $\\varnothing,\\forall$：\\varnothing（空集）,\\forall(与任意的) $\\exists,\\nexists$：\\exists,\\nexists（存在，不存在） $\\because,\\therefore$：\\because,\\therefore（因为，所以） $\\R,\\Q,\\N,\\Z$：\\R,\\Q,\\N,\\Z对于数集来说特有的字符，可以直接使用\\转义即可，但是除了数集以外的其他字符就无法使用了。完整写法为：\\mathbb R $\\mathcal F,\\mathscr F$：\\mathcal F,\\mathscr F（花体字符） 以下为上面字符的运行结果。 +-\\\\ \\times,\\cdot,\\div\\\\ \\pm,\\mp\\\\ >","link":"/2022/01/24/Latex/Latex%E5%85%AC%E5%BC%8F%E6%8E%92%E7%89%88%E5%BF%AB%E9%80%9F%E6%95%99%E7%A8%8B/"},{"title":"第二章单自由度系统振动_第一部分","text":"单自由度系统振动的第一部分，主要讲的是无阻尼自由振动、能量法、瑞利法以及等效刚度与质量。 无阻尼的自由振动令$x$为位移，以质量块静平衡位置为原点，$\\lambda$为静变形。受到初始扰动时，由牛顿第二定律，得： m\\ddot x(t) = mg - k(\\lambda+x(t))在静平衡时的位置：$mg = k\\lambda$ 则物体的振动或自由振动的微分方程为： m\\ddot x(t)+kx(t)=0令：$\\omega_0=\\sqrt \\frac{k}{m}$ 固有频率，单位：弧度/秒（rad/s) 则有微分方程为：$\\ddot x(t) + \\omega_0^2x(t)=0 \\Rightarrow$ 对应得特征方程为：$ms^2+k=0 \\Rightarrow$ 特征根：$s_{1,2}=\\pm i\\omega_0$ $s_1、s_2$都满足特征方程，因此通解可以写为 \\begin{align} x(t) &= A_1e^{s_1t}+A_2 e^{s_2t}\\\\ &=A_1e^{i\\omega t}+A_2 e^{-i\\omega t}\\\\ &= c_1 \\cos (\\omega_0t) +c_2 \\sin (\\omega_0t)\\\\ &= A \\sin (\\omega_0t+\\varphi) \\end{align}$c_1和c_2$为新常数，主要由初始条件来决定 振幅：$A=\\sqrt {(c_1^2+c_2^2)}$ 初相位：$\\varphi=tg^{-1} \\left (\\dfrac {c1}{c2} \\right )$ $\\omega_0$ ：系统固有的数值特征，与系统是否振动以及如何振动无关。 $A,\\varphi$：非系统固有数字特征，与系统所受激励和初始状态有关。 考虑系统在初始扰动下的自由振动，设$t=\\tau$的初始位移和初始速度为：$x(\\tau)=x_\\tau \\qquad \\dot x(\\tau)=\\dot x_\\tau$ 则令： c_1=b_1 \\cos(\\omega_0 \\tau)-b_2\\sin (\\omega_0\\tau)\\\\ c_2=b_1 \\sin(\\omega_0 \\tau)-b_2\\cos (\\omega_0\\tau)带入得：$b_1=x_\\tau \\quad b_2=\\dfrac{\\dot x_\\tau}{\\omega_0}$ 并且$\\tau$时刻后得自由振动的解为： x(t) = x_\\tau \\cos \\omega_0(t-\\tau) +\\frac{\\dot x_\\tau}{\\omega_0} \\sin \\omega_0(t-\\tau)\\\\带入零时刻的初始条件$x(0)=x_0 \\qquad \\dot x(0)=\\dot x_0$可得： x(t) = x_0 \\cos \\omega_0(t) +\\frac{\\dot x_0}{\\omega_0} \\sin \\omega_0(t)=A\\sin(\\omega_0t+\\varphi)此时的振幅为$A=\\sqrt{x_0^2+\\left(\\frac{\\dot x_0}{\\omega_0}\\right)^2}$，相位为$\\varphi=tg^{-1} \\dfrac{x_0\\omega_0}{\\dot x_0}$ 无阻尼的质量弹簧系统受到初始扰动后，其自由振动是以$\\omega_0$为振动顿率的简谐振动,并且永无休止。 初始条件的说明：初始条件是外界能量转入的一种方式， 有初始位移即转入了弹性势能。 有初始速度即转入了动能，用锤子敲击的脉冲就是输入了动能。 如图所示：可以得出， 弹簧的刚度为蓝色最大、黑色最小。 刚度越大、圆频率$\\omega_0=\\sqrt{\\dfrac{k}{m}}$ 越大，周期（$T=\\dfrac{2\\pi}{\\omega_0}$）越小。 由此得出圆频率的另一种计算方法： 由于在静止情况下$mg=k\\lambda$ 可得：$\\omega_0=\\sqrt{\\dfrac{k}{m}}=\\sqrt{\\dfrac{g}{\\lambda}}$ 对于不易得到m和k 的系统，若能测出静变形$\\lambda$，则用该式计算较为方便。 无阻尼振动的计算实例 首先，我们规定的原点一般为静平衡位置，这样可以时建模得到了微分方程为齐次的，在原长处建模时，微分方程为非齐次的。 m\\ddot x+kx=0\\\\ 如果以原长的位置进行建模：\\\\ m\\ddot x+k x=mg 由题可得振动的频率为：$\\omega_0=\\sqrt{\\dfrac{gk}{W}}=19.6\\,rad/s$ 重物匀速下降时处于静平衡位置，若将坐标原点取在绳被卡住瞬时重物所在位置，则t=0时有，$x_0=0\\quad \\dot x_0=v$ 于是带入振动方程： \\begin{align} x(t) &= x_0 \\cos \\omega_0(t) +\\frac{\\dot x_0}{\\omega_0} \\sin \\omega_0(t)\\\\ &=\\dfrac{v}{\\omega_0} \\sin(\\omega_0t)=12.8\\sin(19.6t) (cm) \\end{align} 求绳子的最大张力，一定时是绳子在最底部时的。绳中最大张力等于静张力与因为振动引起动的张力之和： T_{max}=T_s+kA=W+kA\\\\ =1.47\\times10^5+0.74\\times10^5=2.21\\times10^5 动张力几乎是静张力的一半 由于$kA=k\\dfrac{v}{\\omega_0}=v\\sqrt{km}$ 为了减少振动引起的动张力，应当降低升降系统的刚度。尽管这样可能会造成A过大舒适性下降，但是由于加速度减小，安全性得到保证。 首先，取平衡位置，以梁承受重物静平衡位置为坐标原点（就像相当于将重物平稳的放在上面产生的静变形为$\\lambda$） 由材料力学得：$\\lambda=\\dfrac{mgl^3}{48EJ}$ 自由振动频率：$\\omega_0=\\sqrt{\\dfrac{g}{\\lambda}}=\\sqrt {\\dfrac{48EJ}{ml^3}}$ 撞击时刻为零时刻，则t=0 时，有：$x_0=-\\lambda\\quad\\dot x_0=\\sqrt{2gh}$ 自由振动振幅：$A=\\sqrt{x_0^2+\\left(\\frac{\\dot x_0}{\\omega_0}\\right)^2}=\\sqrt{\\lambda^2+2h\\lambda}$ 梁的最大扰度：$\\lambda_{max}=A+\\lambda$ 由牛顿第二定律： I\\ddot \\theta(t)+k_\\theta \\theta(t)=0\\\\ \\ddot \\theta(t)+\\omega_0^2\\theta(t)=0 则扭振的固有频率为$\\omega_0=\\sqrt{\\dfrac{k_\\theta}{I}}$ 总结：由上例可看出，除坐标不同外，角振动与直线振动的数学描述完全相同。如果在弹簧质量系统中将m、k 称为广义质量及广义刚度，则弹簧质量系统的有关结论完全适用于角振动。以后不加特别声明时，弹簧质量系统是广义的。 从上两例还可看出，单自由度无阻尼系统总包含着惯性元件和弹性元件两种基本元件。 惯性元件是感受加速度的元件，它表现为系统的质量或转动惯量； 弹性元件是产生使系统恢复原来状态的恢复力的元件，它表现为具有刚度或扭转刚度的弹性体。 同一个系统中，若惯性增加，则使固有频率降低，而若刚度增加，则固有频率增大。 求复摆在平衡位置附近做微振动时的微分方程和固有频率。这里我们考虑的振动都是微振动（$\\sin \\theta \\approx \\theta$ )。 首先，由牛顿定律：$I_0\\ddot \\theta(t)+mga\\sin \\theta(t)=0$ 得出固有频率$\\omega_0=\\sqrt{\\dfrac{mga}{I_0}}$ 若已测出物体的固有频率，则可求出$\\omega_0$，再由移轴定理，可得物质绕质心的转动惯量：$I_c=I_0-ma^2$ 实验确定复杂形状物体的转动惯量的一个方法。 以静平衡位置为坐标原点建立坐标系，振动固有频率：$\\omega_0=\\sqrt{\\dfrac{K}{m}}=70rad/s$ 振动初始条件：$kx_0=mg\\times\\sin30° \\ \\Rightarrow x_0=-0.1cm$ 由题初始速度为$\\dot x_0=0$，则运动方程为$x(t)=-0.1\\cos(70t) cm$ 如果系统竖直放置，振动频率是否改变？不改变，因为质量是不变的，只有原长会改变。 能量法 对于不计阻尼即认为没有能量损失的单自由度系统，可利用能量守恒原理建立自由振动微分方程，或直接求出固有频率。 无阻尼系统为保守系统，其机械能守恒，即动能T 和势能V 之和保持不变，即：$T+V=const$ \\dfrac{d}{dt} (T+V)=0 当原点在静平衡位置的情况： 则总的势能为： V=-mgx+\\int_0^xk(\\lambda+x)\\,dx\\\\=-mgx+k\\lambda x+\\frac 1 2 k x^2 =\\frac 1 2 kx^2 则总的能量的为：$T+V=\\dfrac 1 2 k x^2+\\dfrac 1 2m \\dot x^2$ $\\dfrac{d}{dt} (T+V)=(m\\ddot x +kx)\\dot x=0 \\,\\Rightarrow \\, m\\ddot x(t)+kx(t)=0$ 能量法的步骤：(1)列方程，(2)求固有频率 如果将坐标原点不是取在系统的静平衡位置，而是取在弹簧为自由长时的位置，此时由能量之和的导数为零可得： $m\\ddot x \\dot x-mg\\dot x +kx \\dot x=0 \\, \\Rightarrow \\, m\\ddot x(t)+kx(t)=mg$ 设新的坐标$y=x-\\dfrac {mg} k =x- \\lambda \\ \\Rightarrow \\ m\\ddot y(t)+ky(t)=0$ 如果重力的影响仅是改变了惯性元件的静平衡位置，那么将坐标原点取在静平衡位置上，方程中就不会出现重力项。 由于$T+V=const$，则有$T_{max}= V_{max}\\Rightarrow\\frac 1 2m \\dot x_{max}^2=\\frac 1 2 k x_{max}^2$ 即有$\\dot x_{max}=\\omega_0x_{max}$，带入$x(t)= A \\sin (\\omega_0t+\\varphi) $，可得$\\omega_0=\\sqrt{\\dfrac{k}{m}}$ 对于转动：$\\dot \\theta_{max}=\\omega_0\\theta_{max}$ (1) 倒摆作微幅振动时的固有频率(2) 摆球$m=0.9kg$时，测得频率f为1.5HZ，$m=1.8kg$时，测得频率f为0.75HZ，问摆球质量为多少千克时恰使系统处于不稳定平衡状态？ 解法一：静平衡位置如左图，广义坐标为$\\theta$ 势能：由于是微振动，正弦遵循小角近似，但是余弦不遵循 \\begin{align} V&=2 \\times \\frac{1}{2}\\left(\\frac{1}{2} k\\right)(\\theta a)^{2}-m g l(1-\\cos \\theta)\\\\ &=\\frac{1}{2} k a^{2} \\theta^{2}-m g l\\left(1-\\left(1-2 \\sin ^{2} \\frac{\\theta}{2}\\right)\\right)\\\\ &=\\frac{1}{2}\\left(k a^{2} \\theta^{2}-m g l \\theta^{2}\\right)=\\frac{1}{2}\\left(k a^{2}-m g l\\right) \\theta^{2} \\end{align} 动能：$T=\\dfrac{1}{2} I \\dot{\\theta}^{2}=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2}$ 微分方程为：$T_{max}=U_{max} \\Rightarrow \\frac{1}{2} m l^{2} \\dot{\\theta}_{\\max }^{2}=\\dfrac{1}{2}\\left(k a^{2}-m g l\\right) \\theta_{\\max }^{2}$ 求固有频率：$\\dot \\theta_{max}=\\omega_0\\theta_{max} \\Rightarrow \\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 这里我们将$ m l^{2}$视为等效质量，$\\left(k a^{2}-m g l\\right) $视为等效刚度。 解法二：静平衡位置如右图，广义坐标为$\\theta$ 势能：由于是微振动，这里用小角近似 \\begin{align} V&=2 \\times \\frac{1}{2}\\left(\\frac{1}{2} k\\right)(\\theta a)^{2}+m g l\\cos \\theta\\\\ &=\\frac{1}{2} k a^{2} \\theta^{2}+m g l\\left(1-2 \\sin ^{2} \\frac{\\theta}{2}\\right)\\\\ &=\\frac{1}{2}\\left(k a^{2} -m g l \\right)\\theta^{2}+mgl \\end{align} 动能：$T=\\dfrac{1}{2} I \\dot{\\theta}^{2}=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2}$ 由于$\\frac{d}{d t}(T+U)=0 \\Rightarrow\\ 2 m l^{2} \\dot{\\theta} \\ddot{\\theta}+2 \\theta\\left(k a^{2}-m g l\\right) \\dot{\\theta}=0$ 则有：$2 m l^{2} \\ddot{\\theta}+2\\left(k a^{2}-m g l\\right) \\theta=0 \\Rightarrow \\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 确定系统微振动的固有频率。 广义坐标：圆柱微转角$\\theta$，圆柱做一般运动， 由柯希尼定理，动能：$T=\\frac{1}{2}\\left(\\frac{3}{2} m R^{2}\\right) \\dot{\\theta}^{2}$ C点为瞬心 A点速度：$v_{A}=(R+a) \\dot{\\theta} \\ \\Rightarrow \\ x_{A}=(R+a) \\theta$ B点的速度：$v_{B}=(R-b) \\dot{\\theta}\\ \\Rightarrow \\ x_{B}=(R-b) \\theta$ 则势能为：$U=\\dfrac{1}{2}\\left(2 k_{1}\\right)(R+a)^{2} \\theta^{2}+\\dfrac{1}{2}\\left(2 k_{2}\\right)(R-b)^{2} \\theta^{2}$ 由于$T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{\\theta}_{\\max }=\\omega_{0} \\theta_{\\max }$ \\begin{align} \\omega_{0}^{2}&=\\dfrac{2\\left[k_{1}(R+a)^{2}+k_{2}(R-b)^{2}\\right]}{3 m R^{2} / 2}\\\\ &=\\dfrac{4}{3 m}\\left[k_{1}\\left(1+\\dfrac{a}{R}\\right)^{2}+k_{2}\\left(1-\\dfrac{b}{R}\\right)^{2}\\right] \\end{align}解得固有频率为：$\\omega_{0}=\\sqrt{\\dfrac{4}{3 m}\\left[k_{1}\\left(1+\\dfrac{a}{R}\\right)^{2}+k_{2}\\left(1-\\dfrac{b}{R}\\right)^{2}\\right]}$ 坐标原点设在静平衡位置，因为计算势能的时候预先压缩造成的重力势能和弹性势能一定会被约掉，得出其次动力学方程。 因此，广义的坐标：质量块的垂直位移x 动能： \\begin{align} T&=\\frac{1}{2} m \\dot{x}^{2}+\\dfrac{1}{2} M\\left(\\dfrac{1}{2} \\dot{x}\\right)^{2}+\\frac{1}{2}\\left(\\dfrac{1}{2} M R^{2}\\right)\\left(\\dfrac{\\dot{x}}{2 R}\\right)^{2}\\\\ &=\\frac{1}{2}\\left(m+\\frac{1}{4} M+\\frac{1}{8} M\\right) \\dot{x}^{2}\\\\ &=\\frac{1}{2}\\left(m+\\frac{3}{8} M\\right) \\dot{x}^{2} \\end{align} 势能：$U=\\dfrac{1}{2} k_{2} x^{2}+\\dfrac{1}{2} k_{1}\\left(\\dfrac{1}{2} x\\right)^{2}=\\dfrac{1}{2}\\left(k_{2}+\\dfrac{1}{4} k_{1}\\right) x^{2}$ 由能量守恒可知： $T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{x}_{\\max }=\\omega_{0} x_{\\max }$ 解得：$\\omega_{0}=\\sqrt{\\dfrac{2 k_{1}+8 k_{2}}{3 M+8 m}}$ 瑞利法利用能量法求解固有频率时，对于系统的动能的计算只考虑了惯性元件的动能，而忽略不计弹性元件的质量所具有的动能，因此算出的固有频率是实际值的上限。 这种简化方法在许多场合中都能满足要求，但有些工程问题中，弹性元件本身的质量因占系统总质量相当大的比例而不能忽略，否则算出的固有频率明显偏高。 当弹簧的质量的质量不是远小于小车的质量时，就无法忽略弹簧的质量，因此这里用等效的质量来分析。 系统最大的势能：$V_{\\max }=\\frac{1}{2} k x_{\\max }^{2}$ $T_{\\max }=U_{\\max } \\quad \\Rightarrow \\quad\\dot{x}_{\\max }=\\omega_{0} x_{\\max } \\quad \\Rightarrow \\quad \\omega_{0}=\\sqrt{\\dfrac{k}{m+m_{t}}}$ 若忽略 $m_t$ ，则$\\omega_0$增大，因此忽略弹簧动能所算出的固有频率是实际值的上限。 瑞利法求解等效的质量是通过积分来求解的。 等效质量与等效阻尼方法一：能量法求解 选定广义位移坐标后，将系统得动能、势能写成如下形式：$T=\\dfrac{1}{2} M_{e} \\dot{x}^{2} ,\\, V=\\dfrac{1}{2} K_{e} x^{2}$ 当 $\\dot x,x$分别取最大值时：$T \\rightarrow T_{\\max }， \\ V \\rightarrow V_{\\max }$ ，可以得出：$\\omega_{0}=\\sqrt{\\dfrac{K_{e}} {M_e}}$ $K_e$：简化系统的等效刚度；$M_e$：简化系统的等效质量。 等效的含义是指简化前后的系统的动能和势能分别相等。 主要步骤： 选定广义坐标 将动能和势能写成标准形式 根据系数写出等效刚度和等效阻尼 动能：$T=\\dfrac{1}{2} m l^{2} \\dot{\\theta}^{2} \\Rightarrow M_{e}=m l^{2}$ 势能：$V=\\frac{1}{2}\\left(k a^{2}-m g l\\right) \\theta^{2} \\Rightarrow K_{e}=k a^{2}-m g l$ 固有频率：$\\omega_{0}=\\sqrt{\\dfrac{k a^{2}-m g l}{m l^{2}}}$ 势能：$U=\\frac{1}{2}\\left[\\left(2 k_{1}\\right)(R+a)^{2}+\\left(2 k_{2}\\right)(R-b)^{2}\\right] \\theta^{2}$ 等效刚度：$K_{e}=\\left(2 k_{1}\\right)(R+a)^{2}+\\left(2 k_{2}\\right)(R-b)^{2}$ 则固有频率为：$\\omega_{0}^{2}=\\dfrac{2\\left[k_{1}(R+a)^{2}+k_{2}(R-b)^{2}\\right]}{3 m R^{2} / 2}$ 方法二：定义法 等效刚度：使系统在选定的坐标上产生单位位移而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效刚度。 等效质量：使系统在选定的坐标上产生单位加速度而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效质量。 使系统在选定的坐标上产生单位位移而需要在此坐标方向上施加的力，叫做系统在这个坐标上的等效刚度。 串联弹簧的刚度的倒数是原来各个弹簧刚度倒数的总和 并联弹簧的刚度是原来各个弹簧刚度的总和。 求：系统对于坐标x 的等效质量和等效刚度。 方法1：能量法 动能：$T=\\dfrac{1}{2} m_{1} \\dot{x}^{2}+\\dfrac{1}{2} m_{2}\\left(\\dfrac{l_{2}}{l_{1}} \\dot{x}\\right)^{2}=\\dfrac{1}{2}\\left(m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}\\right) \\dot{x}^{2}$ 等效质量：$M_{e}=m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}$ 势能：$V=\\dfrac{1}{2} k_{1} x^{2}+\\dfrac{1}{2} k_{2}\\left(\\dfrac{l_{3}}{l_{1}} x\\right)^{2}=\\dfrac{1}{2}\\left(k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}\\right) x^{2}$ 等效刚度：$K_{e}=k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}$ 方法二：定义法 设使系统在x方向产生单位加速度需要施加力P，则在m1、m2上产生惯性力，对支座取矩： $P l_{1}=\\left(m_{1} \\cdot 1\\right) l_{1}+\\left(m_{2} \\cdot \\dfrac{l_{2}}{l_{1}}\\right) l_{2}$ 等效质量：$M_{e}=m_{1}+\\dfrac{l_{2}^{2}}{l_{1}^{2}} m_{2}$ 画虚线是夸张化的m，其实其位移非常的为微小，在零时刻产生了单位加速度$\\ddot x=1$，$m_1,m_2$在平衡位置静平衡位置：重力和弹簧预变形相互抵消，因此弹力可以忽略。 设使系统在x坐标上产生单位位移需要施加力P，则在k1、k2处将产生弹性恢复力，对支点取矩： $P l_{1}=\\left(k_{1} \\cdot 1\\right) l_{1}+\\left(k_{2} \\cdot \\dfrac{l_{3}}{l_{1}}\\right) l_{3}$ 等效刚度：$K_{e}=k_{1}+\\dfrac{l_{3}^{2}}{l_{1}^{2}} k_{2}$ 等效质量(惯性力)：动态求解(达朗贝尔原理)； 等效刚度(静力平衡：静态求解(列静力学方程)。","link":"/2022/01/27/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E7%AC%AC%E4%BA%8C%E7%AB%A0%E5%8D%95%E8%87%AA%E7%94%B1%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%8C%AF%E5%8A%A8-%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86/"},{"title":"Latex期刊论文的排版","text":"本文主要针对与IEEE的一个Latex模板，主要从模板下载、语句解释、插入图片、插入公式、插入算法图、插入引用等方面来讲解如何使用模板。该教程属于简易教程，深入学习请看后续的详细教程。视频参考 Latex模板下载 模板下载:IEEE模板:http://www.ieee.org/publications_standards/publications/authors/author_templates.html 通用模板:https://www.overleaf.com/ 其他方法:百度,csdn等网页找 Latex的核心思想就是内容与格式分离 模板的基本语句解释 \\begin{document}是文件的开始，之前为导言区，可以加入各种各样的宏包。 \\begin{document}之后到\\begin{abstract}之前用来写作者信息，联系方式以及致谢等信息。 \\begin{IEEEkeywords} 后面写关键词。 \\section{Introduction} 之后为介绍综述部分。 \\subsection{name} 为子标题部分。 \\section{References Section} 参考文献部分。 \\begin{IEEEbiography} 开始作者生平部分。 插入图片 首先，导入宏包 \\usepackage{graphicx}。 其次，导入代码块，其中的后缀名可以去查看详细的教程。 最后，编辑图片的大小样式。 插入公式公式部分主要的内容参考之前的专门写公式那篇博客](https://houdezaiwu2019.github.io/2022/01/24/Latex/Latex公式排版快速教程/))。 首先，导入宏包 \\usepackage{amsmath,amsfonts} 其次，行间公式如图，文章终会自行标号，也可以自行标号\\tag{1.1}。 \\begin{equation} \\label{deqn_ex1} x = \\sum_{i=0}^{n} 2{i} Q. \\tag{1.1} \\end{equation} 最后，行内的公式的写法是在$ $之间直接写即可，如$x^2+y^2=0$ 插入参考文献实用办法插入参考文献 首先，引入宏包 \\usepackage[colorlinks,linkeolor-black,anchorcolor-black,citecolor-black]{hyperref }%用于调整多个参考文献的间距。直接用就好。 模板的插入方法，直接插入。 也可以自己使用BibTex格式的文件来插入，但是在实际编译的过程中要记得将BibTex文件也进行编译。 到百度上下载一个BibTex的文件。 引用BIb文件。 以下为BibTex文件的内容。 12345678@article{2021Exploiting,title={Exploiting Vector Attention and Contextual Prior for Ultrasound Image Segmentation},author={ Xu, L. and Gao, S. and Shi, L. and Wei, B. and He, Y. },journal={Neurocomputing},volume={454},number={1},year={2021},} 引用，BibTex也要编译，否则出现的是？，但是目前TexStduio我还没有找到在哪里编译BibTex文件。 插入算法图 首先，导入宏包 \\usepackage[ruled,linesnumbered]{algorithm2e}。 之后，导入算法块即可。 工具补充 补充好用网页:手写公式转latex https://editor.codecogs.com/ 手写符号，转latex表达 https://detexify.kirelabs.org/classify/html 截图看公式 https://mathpix.com/ 手写表格，转latex表达 https://www.tablesgenerator.com/","link":"/2022/01/25/Latex/Latex%E6%9C%9F%E5%88%8A%E8%AE%BA%E6%96%87%E7%9A%84%E6%8E%92%E7%89%88/"},{"title":"AlexNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); 本文首先详解AlexNet网络，计算每一层网络的维度并且整理了花数据集。之后，通过Pytorch对网络进行复现，主要从模型搭建、设置GPU、数据预处理、加载数据、配置损失函数和优化器、模型保存、模型训练验证与测试等方面进行实现。 AlexNet的详解AlexNet是2012年ISLVRC 2012 (lmageNet Large Scale Visual RecognitionChallenge）竞赛的冠军网络，分类准确率由传统的70%+提升到80%+。它是由Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，深度学习开始迅速发展。 AlexNet网络简介ISLVRC 2012训练集:1,281,167张已标注图片；验证集:50,000张已标注图片；测试集:100,000张未标注图片。 AlexNet的结构图 该网络的亮点在于: 首次利用GPU进行网络加速训练。 使用了ReLU激活函数，而不是传统的Sigmoid激活函数以及Tanh激活函数。 因为Sigmoid函数会出现梯度消失，梯度爆炸。 使用了LRN局部响应归一化。 在全连接层的前两层中使用了Dropout随机失活神经元操作，以减少过拟合。 过拟合：根本原因是特征维度过多，模型假设过于复杂，参数过多，训练数据过少，噪声过多，导致拟合的函数完美的预测训练集，但对新数据的测试集预测结果差。过度的拟合了训练数据，而没有考虑到泛化能力。 使用Dropout的方式在网络正向传播过程中随机失活一部分神经元。 回顾：经过卷积后的矩阵尺寸大小计算公式为： 其中输入图片的大小为$W\\times W$； Filter的大小为$F\\times F$，即kernel_size为F； 步长Stirde=S padding的像素数为P N=\\dfrac{(W-F+2P)}{S}+1网络分层计算 第一层（Conv1）： 这里是运用了GPU的并行计算的功能，将96层分成了两部分进行计算。 通道数：input_channels = 3；output_channels = 48$\\times$2=96 卷积核参数：kernel_size = 11; stride = 4; padding = [1,2] 输入输出的尺寸数：input_size=[224,224,3]; output_size=[55,55,96] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(224-11+(1+2))}{4}+1=55 第二层（Maxpool1）：只是改变高度宽度，不改变深度。 通道数：input_channels = 96；output_channels = 48$\\times$2=96 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[55,55,96]; output_size=[27,27,96] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(55-3+0)}{2}+1=27 第三层（Conv2）： 通道数：input_channels = 96；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 5; stride = 1; padding = 2 输入输出的尺寸数：input_size=[27,27,96]; output_size=[27,27,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(27-5+4)}{1}+1=27 第四层（Maxpool2）： 通道数：input_channels = 256；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[27,27,256]; output_size=[13,13,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(27-3+0)}{2}+1=13 第五层（Conv3）： 通道数：input_channels = 256；output_channels = 192$\\times$2=384 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,256]; output_size=[13,13,384] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第六层（Conv4）： 通道数：input_channels = 384；output_channels = 192$\\times$2=384 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,256]; output_size=[13,13,384] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第六层（Conv4）： 通道数：input_channels = 384；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 1; padding = 1 输入输出的尺寸数：input_size=[13,13,384]; output_size=[13,13,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+2)}{1}+1=13 第六层（Conv4）： 通道数：input_channels = 256；output_channels = 128$\\times$2=256 卷积核参数：kernel_size = 3; stride = 2; padding = 0 输入输出的尺寸数：input_size=[13,13,256]; output_size=[6,6,256] N=\\dfrac{(W-F+2P)}{S}+1=\\dfrac{(13-3+0)}{2}+1=6 后三层接了三个全连接层，注意最后数据集有1000类，因此最后一个全连接层有1000个节点，如果我们的数据集有10类，最后的全连接层就10个节点。 layer_name kernel_size output_channels padding stride Conv1 11 96 [1,2] 4 Maxpool1 3 None 0 2 Conv2 5 256 [2,2] 1 Maxpool2 3 None 0 2 Conv3 3 384 [1,1] 1 Conv4 3 384 [1,1] 1 Conv5 3 256 [1,1] 1 Maxpool3 3 None 0 2 FC1 2048 None None None FC2 2048 None None None FC3 1000 None None None 数据集(flowers) （1）在data_set文件夹下创建新文件夹”flower_data” （2）点击链接下载花分类数据集 （3）解压数据集到flower_data文件夹下 （4）执行”split_data.py”脚本自动将数据集划分成训练集train和验证集val 1234├── flower_data ├── flower_photos（解压的数据集文件夹，3670个样本） ├── train（生成的训练集，3306个样本） └── val（生成的验证集，364个样本） AlexNet网络的pytorch实现模型的搭建 对于网络层比较多的结构来说，一般用在网络比较多的时候，且网络结构比较简单，没有跳跃连接等复杂的结构。 padding=tuple/int：如果tuple(1,2),代表上下方各补一行0，左右补两列0。但是，如果出现上述的计算出来的下一层的[H,W]不为0时，自动舍弃一行或一列零来满足最后结果为整数。 1nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2) 如果要左侧补一列，右侧补两列，上侧补一行，右侧补两行。要使用nn.ZeroPad((1，2，1，2)) nn.ReLU(inplace=True): 减小使用内存，提高性能的参数。 self.modules()：返回一个迭代器，遍历网络中所有的模块； 关于初始化，目前版本的pytorch其实是可以自动进行初始化的。初始化时，对于卷积层使用恺明初始化，对于一般的全连接层使用正态分布初始化，偏差全部初始化为0。 1234567891011121314def _initialize_weights(self): for m in self.modules(): # module()返回一个迭代器 遍历一个类中所有的模块类 if isinstance(m, nn.Conv2d): # 参数1为对象(迭代器) 参数2为要作比较的对象 # 如果参数1对应的对象和参数2对应的类相同，返回True nn.init.kaiming_normal_(m.weight, mode='fan_out') # 对于卷积的参数使用恺明初始化 if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): # 对于全连接层，使用的正态分布初始化权重 nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.constant_(m.bias, 0) 对于展开成为全连接层来说，保证第0位batch不变, start_dim=1$[batch\\ , channel\\ , weight\\ , height] \\rightarrow [batch\\ , num]$ 1x = torch.flatten(x, start_dim=1) # 也可以用view 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class AlexNet(nn.Module): def __init__(self, num_classes=1000, init_weight=False): super(AlexNet, self).__init__() # 卷积层合集 self.feature = nn.Sequential( # input [3,224,224] nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2), # output [48,55,55] # 如果出现上述的计算出来的下一层的[H,W]不为0时， # 自动舍弃一行零来满足最后结果为整数 # 或者使用nn.ZeroPad((1，2，1，2)) nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [48,27,27] nn.Conv2d(48, 128, kernel_size=5, padding=2), # output [128,27,27] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [128,13,13] nn.Conv2d(128, 192, kernel_size=3, padding=1), # output [192,13,13] nn.ReLU(inplace=True), nn.Conv2d(192, 192, kernel_size=3, padding=1), # output [192,13,13] nn.ReLU(inplace=True), nn.Conv2d(192, 128, kernel_size=3, padding=1), # output [128,13,13] nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2), # output [128,6,6] ) # 全连接层合集 self.classifier = nn.Sequential( nn.Dropout(p=0.5), nn.Linear(128*6*6, 2048), nn.ReLU(inplace=True), nn.Dropout(p=0.5), nn.Linear(2048, 2048), nn.ReLU(inplace=True), nn.Linear(2048, num_classes), ) if init_weight: self._initialize_weights() # 一般情况下时自动进行初始化的 def _initialize_weights(self): for m in self.modules(): # module()返回一个迭代器 遍历一个类中所有的模块类 if isinstance(m, nn.Conv2d): # 参数1为对象(迭代器) 参数2为要作比较的对象 # 如果参数1对应的对象和参数2对应的类相同，返回True nn.init.kaiming_normal_(m.weight, mode='fan_out') # 对于卷积的参数使用恺明初始化 if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): # 对于全连接层，使用的正态分布初始化权重 nn.init.normal_(m.weight, mean=0, std=0.01) nn.init.constant_(m.bias, 0) def forward(self, x): x = self.feature(x) x = torch.flatten(x, start_dim=1) # [batch channel weight height] start_dim=1 # 就是保证第0位batch不变-&gt;[batch num] # 也可以用view x = self.classifier(x) return x 设置GPU设备1device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) 数据预处理12345678910111213data_transform = { &quot;train&quot;: transforms.Compose([ transforms.RandomResizedCrop(224), # 随机截取大小为224的图片 transforms.RandomHorizontalFlip(), # 随机翻转 transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]), &quot;val&quot;: transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])} 这里我们将预处理部分分成“train”和”val”两部分，将其封装称为一个字典。 transforms.RandomResizedCrop(224)：随机截取一定大小的图片。 transforms.RandomHorizontalFlip()：对图片数据进行随机反转。 transforms.ToTensor()：将数据转化为Tensor，维度按照torch要求的排列$[batch\\ , channel\\ , weight\\ , height]$。 transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))：将图片数据标准化。 加载数据12345678910# 获得绝对路径data_root = os.path.abspath(os.path.join(os.getcwd(), &quot;../..&quot;))# os.getcwd()获得当前位置的目录# ./.代表返回上一层目录 ../..代表返回上两层目录# 获得花的数据data_root = os.path.abspath(os.getcwd())image_path = data_root + &quot;/data_set/flower_data/&quot;train_dataset = datasets.ImageFolder(root=image_path + &quot;/train&quot;, transform=data_transform[&quot;train&quot;])# 求数据即得数据个数train_num = len(train_dataset) os.getcwd()：获得当前位置的目录。 ./.代表返回上一层目录； ../..代表返回上两层目录 os.path.join(os.getcwd(), “../..”)：代表返回上两层目录，其中join()的作用就是将内部路径进行合并，也可以进入当前目录的下的某个文件，join(os.getcwd(), “/xxx/xxx”) ImageFolder：用来以一定方式整理图片的类。 123456789# flower_list得到字典，但是得出的结果是反的flower_list = train_dataset.class_to_idx# 是的字典的val和key交换为位置的一种方法cla_dict = dict((val, key) for key, val in flower_list.items())# 将字典写入到Json文件中json_str = json.dumps(cla_dict, indent=4)with open('class_indices.json', 'w') as json_file: json_file.write(json_str) train_dataset.class_to_idx：得到类别和索引值的字典，此处预测得到的是索引值，想要得到花的名字就需要将key（名称）和value（索引值）交换位置。 json.dumps(cla_dict, indent=4)：将字典值写入到json文件中。 1234567{ &quot;0&quot;: &quot;daisy&quot;, &quot;1&quot;: &quot;dandelion&quot;, &quot;2&quot;: &quot;roses&quot;, &quot;3&quot;: &quot;sunflowers&quot;, &quot;4&quot;: &quot;tulips&quot;} 数据的导入和加载12345678910batch_size = 32train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)val_dataset = datasets.ImageFolder(root=image_path + &quot;val&quot;, transform=data_transform[&quot;val&quot;])val_num = len(val_dataset)val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0) 这边和上一章相同，此处不在赘述。 损失函数的计算与优化器配置12345678net = AlexNet(num_classes=5, init_weight=True)net.to(device) # 启动GPUloss_function = torch.nn.CrossEntropyLoss()optimizer = optim.Adam(net.parameters(), lr=0.0002)save_path = '.AlexNet.pth'best_acc = 0.0 # 设置保存最高准确率所对应的参数 模型的训练与验证 net.train(),net.eval()：主要的作用是用来管理Dropout层和BN层，在训练的过程启用上述层，但是在预测的时候还是正常计算，不进行随机失活和BN操作。 训练过程 t1 = time.perf_counter()：用来记录训练的时间。 用来打印进度条的程序。其中，end=’’空字符。 123456# 打印训练的过程进度条 rate = (step + 1) / len(train_loader) a = &quot;*&quot; * int(rate * 50) b = &quot;.&quot; * int((1-rate) * 50) print(&quot;\\rtrain loss: {:^3.0f}%[{}-&gt;{}]{:.3f}&quot;.format(int(rate*100), a, b, loss), end='') # 这里end=''相当于是一个换行符 以下是训练过程的代码实现。 123456789101112131415161718192021222324for epoch in range(15): net.train() # 主要针对的Dropout方法 running_loss = 0.0 t1 = time.perf_counter() for step, data in enumerate(train_loader, start=0): images, labels = data images, labels = images.to(device), labels.to(device) # 将梯度初始化为0 optimizer.zero_grad() outputs = net(images) loss = loss_function(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() # 打印训练的过程进度条 rate = (step + 1) / len(train_loader) a = &quot;*&quot; * int(rate * 50) b = &quot;.&quot; * int((1-rate) * 50) print(&quot;\\rtrain loss: {:^3.0f}%[{}-&gt;{}]{:.3f}&quot;.format(int(rate*100), a, b, loss), end='') # 这里end=''相当于是一个换行符 print() print(time.perf_counter()-t1) 验证过程1234567891011121314151617# 直接接在上一个for循环中net.eval()acc = 0.0with torch.no_grad(): for data_test in val_loader: test_images, test_labels = data_test test_images, test_labels = test_images.to(device), test_labels.to(device) outputs = net(test_images) predict_y = torch.max(outputs, dim=1)[1] print(predict_y == test_labels) acc += (predict_y == test_labels).sum().item() accurate_test = acc / val_num # 只保存最好的结果 if accurate_test &gt; best_acc: best_acc = accurate_test torch.save(net.state_dict(), save_path) print('[epoch %d] train_loss: %.3f test_accuracy: %3f' % (epoch + 1, running_loss/step, acc / val_num)) 模型测试导入数据1234567891011121314151617181920data_transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# load imageimg = Image.open(&quot;4.jpg&quot;)plt.imshow(img)# [N C H w]img = data_transform(img)# 增加一个维度img = torch.unsqueeze(img, dim=0)# 读取字典try: json_file = open('./class_indices.json', 'r') class_indict = json.load(json_file)except Exception as e: print(e) exit(-1) 导入模型12345678910111213141516# 建立模型model = AlexNet(num_classes=5)# load 权重model_weigh_path = &quot;.AlexNet.pth&quot;model.load_state_dict(torch.load(model_weigh_path))model.eval()with torch.no_grad(): # predict class output = torch.squeeze(model(img)) # 降低维度，将batch一维去掉 predict = torch.softmax(output, dim=0) predict_cla = torch.argmax(predict).numpy()print(class_indict[str(predict_cla)], predict[predict_cla].item())plt.show()","link":"/2022/01/28/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/AlexNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"Pytorch入门教程及LeNet网络","text":"hljs.initHighlightingOnLoad(); 本篇为Pytorch入门教程，使用的网络为LeNet(1998)网络。主要从网络搭建、导入数据、定义损失函数与优化器、训练与验证模型、模型测试与保存方面进行展示。 建立LeNet网络模型123456789101112131415161718192021222324252627import torchimport torch.nn as nnimport torch.nn.functional as Fclass LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5) self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5) self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(32 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) # pytorch [batch, channel, height, width] def forward(self, x): #input(3,32,32) N=(32-5+0)/1+1=28 x = F.relu(self.conv1(x)) #output(16,28,28) N=28/2=14 x = self.pool1(x) #output(16,14,14) N=(14-5+0)/1+1=10 x = F.relu(self.conv2(x)) #output(32,10,10) N=10/2=5 x = self.pool2(x) #output(32,5,5) x = x.view(-1, 32*5*5) #output=32*5*5=800 x = F.relu(self.fc1(x)) #output=120 x = F.relu(self.fc2(x)) #output=84 out = F.relu((self.fc3(x)))#output=10 return out 经过卷积后的矩阵尺寸大小计算公式为： 其中输入图片的大小为$W\\times W$； Filter的大小为$F\\times F$，即kernel_size为F； 步长Stirde=S padding的像素数为P N=\\dfrac{(W-F+2P)}{S}+1以下我们进行模型的测试和调试： 12345678910111213141516# 模型的测试import torchX = torch.rand(size=(32, 3, 32, 32), dtype=torch.float32)model = LeNet()print(model)# 打印的结果如下：LeNet( (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1)) (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1)) (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (fc1): Linear(in_features=800, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 导入数据 这里我们使用的CIFAR-10数据集，一共由10类。 首先，下载数据集。 torchvision.datasets.xxx() 里面有我们需要很多数据集。 123456# 下载训练数据trainset = torchvision.datasets.CIFAR10( root=&quot;./data&quot;, #数据集下载的位置 train=True, #是否为训练集 transform=transforms,#图像处理函数 download=True) #是否下载 图像的处理函数集 1234transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) Compose([ ])函数的作用是将所有的处理函数合并、打包。 ToTensor()函数的作用是将图片数据转换为张量，维度转化为pytorch标准的维度，同时数据的范围由原来的$[0,255]\\rightarrow[0.0,1.0]$。 Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))的作用是将图片的张量数据进行标准化。 之后，导入数据集。 123456# 导入训练数据trainloader = utils.data.DataLoader( trainset, # 总的训练数据集 batch_size=50, # 一个batch里的样本数 shuffle=True, # 是否打乱顺序 num_workers=0) # 线程数 由于一次训练不可能将训练集所有的参数都选进来进行反向传播，因此我们把一次喂入数据的个数称为batch_size。 数据的测试，使用迭代器获得数据集中的图片和标签。 1234567891011121314151617181920test_data_iter = iter(test_loader)# 迭代器，用来获取test_image, test_label = test_data_iter.next()# 用来获取下一个参数# 显示函数def imshow(img): img = img / 2 + 0.5 # unnormalize 反标准化 npimg = img.numpy() # 转化为numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) # 由于ToTensor()改变了数据的维度排列，转化为原来的维度排列 plt.show()# # get some random training imagesdataiter = iter(trainloader)images, labels = dataiter.next()# show imagesimshow(torchvision.utils.make_grid(images))# print labelsprint(' '.join('%5s' % classes[labels[j]] for j in range(4))) 定义损失函数12# 定义损失函数为交叉熵损失函数loss_function = nn.CrossEntropyLoss() 如图，在CrossEntropyLoss()里面已经包含了交叉熵的计算公式，因此就不在需要在网络里定义softmax()层了。 定义优化器这里使用的Adam() 优化器。 1234# 定义优化器（训练参数，学习率）optimizer = optim.Adam(net.parameters(), lr=0.01)# 可变学习率scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1) 训练模型1234567891011121314151617181920212223242526272829303132for epoch in range(5): # 一个epoch即对整个训练集进行一次训练,这里对训练集循5轮 running_loss = 0.0 # 训练误差初始化为0 for step, train_data in enumerate(trainloader, start=0): inputs, labels = train_data inputs, labels = inputs.to(device), labels.to(device) # 清除历史梯度,每一轮训练都要 optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) # 正向传播 loss = loss_function(outputs, labels) # 计算损失 loss.backward() # 反向传播 optimizer.step() # 优化器更新参数 # 打印训练结果 running_loss += loss.item() accuray = 0 test_image, test_label = test_image.to(device), test_label.to(device) if step % 500 == 499: print('******************') with torch.no_grad(): # 代表以下的代码都是在torch.no_grad()下进行运算的 outputs = net(test_image) predit_y = torch.max(outputs, dim=1)[1] accuray = (predit_y == test_label).sum().item() / test_label.shape[0] print('[%d, %5d] loss: %.3f test_accuray: %.3f' % (epoch + 1, step + 1, running_loss / 500, accuray)) running_loss = 0.0 enumerate(trainloader, start=0)：返回的是数据集和对应的步数。 关于optimizer.zero_grad() 进行梯度的清零。 12345678910# 传统的方法for i, (image, label) in enumerate(train_loader): # 1. input output pred = model(image) loss = criterion(pred, label) # 2. backward optimizer.zero_grad() # reset gradient loss.backward() optimizer.step() 获取 loss：输入图像和标签，通过infer计算得到预测值，计算损失函数； optimizer.zero_grad() 清空过往梯度； loss.backward() 反向传播，计算当前梯度； optimizer.step() 根据梯度更新网络参数 1234567891011121314151617# 梯度累加方法for i,(image, label) in enumerate(train_loader): # 1. input output pred = model(image) loss = criterion(pred, label) # 2.1 loss regularization loss = loss / accumulation_steps # 2.2 back propagation loss.backward() # 3. update parameters of net if (i+1) % accumulation_steps == 0: # optimizer the net optimizer.step() # update parameters of net optimizer.zero_grad() # reset gradient 获取 loss：输入图像和标签，通过infer计算得到预测值，计算损失函数； loss.backward() 反向传播，计算当前梯度； 多次循环步骤 1-2，不清空梯度，使梯度累加在已有梯度上； 梯度累加了一定次数后，先optimizer.step() 根据累计的梯度更新网络参数，然后optimizer.zero_grad() 清空过往梯度，为下一波梯度累加做准备； 总结来说：梯度累加就是，每次获取1个batch的数据，计算1次梯度，梯度不清空，不断累加，累加一定次数后，根据累加的梯度更新网络参数，然后清空梯度，进行下一次循环。对于一些GPU内存不足的实验室，这是一个可以扩大batch_size的trick。 模型测试12345678910111213141516# 打印训练结果running_loss += loss.item()accuray = 0# 使用GPUtest_image, test_label = test_image.to(device), test_label.to(device)if step % 500 == 499: print('******************') with torch.no_grad(): # 代表以下的代码都是在torch.no_grad()下进行运算的 outputs = net(test_image) predit_y = torch.max(outputs, dim=1)[1] # _,predit_y = torch.max(outputs, dim=1) # max()返回的是两个维度，首先的最大值，之后是index accuray = (predit_y == test_label).sum().item() / test_label.shape[0] print('[%d, %5d] loss: %.3f test_accuray: %.3f' % (epoch + 1, step + 1, running_loss / 500, accuray)) running_loss = 0.0 with torch.no_grad():这里的with是一个上下文管理器，其中的作用是下面的代码都遵循torch.no_grad()。 其次，在预测过程中，不用进行梯度计算，用了反而内存不够，因此使用torch.no_grad()。 tensor.item()：用来获取tensor的数值。 保存模型12save_path = './LeNet.pth'torch.save(net.state_dict(), save_path) 模型测试 首先，对图片进行处理。 123456789101112# 数据预处理transform = transforms.Compose( [transforms.Resize((32, 32)), # 首先需resize成跟训练集图像一样的大小 transforms.ToTensor(), # 转换为torch的对的张量维度 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])# 导入要测试的图像（自己找的，不在数据集中），放在源文件目录下im = Image.open('4.jpg')im = transform(im) # 转换成[C, H, W]im = torch.unsqueeze(im, dim=0) # 对数据增加一个新维度，# 因为tensor的维度是[batch, channel, height, width] torch.unsqueeze(im, dim=0)：对数据增加一个新维度，即加一个batch的维度。 之后，实例化网络 123# 实例化网络，加载训练好的模型参数net = LeNet()net.load_state_dict(torch.load('Lenet.pth')) 最后，预测类别 1234567# 预测classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')with torch.no_grad(): outputs = net(im) predict = torch.max(outputs, dim=1)[1].data.numpy()print(classes[int(predict)]) 这里，我们将outputs丢进torch.softmax() ，可得到属于每个类别的概率。 1234567predict = torch.softmax(outputs, dim=1)# dim=1的原因，是因为outputs为两维的，dim=0只有一个# outputs = tensor([[0.0000, 0.0000, 5.4191, 0.4988, 0.0000, 1.3599, 0.0000, 0.2233, 0.0000, 0.0000]])print(predict)# 输出:tensor([[0.0042, 0.0042, 0.9464, 0.0069, 0.0042, 0.0163, 0.0042, 0.0052, 0.0042, 0.0042]])","link":"/2022/01/28/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/Pytorch%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%E5%8F%8ALeNet%E7%BD%91%E7%BB%9C/"},{"title":"GoogLeNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); GoogLeNet详解简介GoogLeNet在2014年由Google团队提出，斩获当年ImageNet竞赛中Classification Task (分类任务)第一名，本文主要从模型的搭建以及pytorch实现上进行介绍。 网络中的亮点： 引入了Inception结构（融合不同尺度的特征信息) 使用1x1的卷积核进行降维以及映射处理 添加两个辅助分类器帮助训练 AlexNet和VGG都只有一个输出层，GoogLeNet有三个输出层(其中两个辅助分类层) 丢弃全连接层，使用平均池化层（大大减少模型参数) Inception结构 如图为原始的一种Inception结构； 主要的区别是之前的VGG主要采取串联的结构，增加模型的深度， Inception主要采取并行的结构，增加的是模型的宽度。 注意：每个分支所得的特征矩阵的高和宽必须是相同的。 如图为改进之后的网络图，这里1$\\times$1的卷积核的主要作用是用来降维的。 这里降维就是将深度降低，即channels数，通过减少深度，进而减少参数量，减少计算量。 辅助分类器(Auxiliary Classifier)The exact structure of the extra network on the side, including the auxiliary classifier, is as follows: An average pooling layer with 5x5 filter size and stride 3, resulting in an 4x4×512 outputfor the (4a), and 4×4×528 for the (4d) stage. $input_{size}=14,\\ output_{size}=\\dfrac{14-5+0}{3}+1=4$ A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.· A fully connected layer（全连接层） with 1024 units and rectified linear activation（RELU). A dropout layer with 70% ratio of dropped outputs. A linear layer with softmax loss as the classifier (predicting the same 1000 classes as themain classifier, but removed at inference time). 在3$\\times$3卷积之前要进行一个1$\\times$1卷积的降维处理。 上图为下面表格中对于的结构在网络中的位置。 如图，在实际的操作过程中，LocalRespNorm(LPN)层的作用不大，因此可以去掉。 如图相比之下发现，VGG的参数参数过多，是GoogLeNet的20倍。 但VGG的模型搭建简单，并且GoogLeNet辅助分类器的使用和修改比较麻烦，不容易调试，因此一般来说VGG的使用较多。 GoogLeNet的pytorch实现模型搭建因为GoogLeNet的一些结构块的代码复用比较多，因此这里首先创建几个模板文件，以方便之后模型的搭建。 基本卷积块12345678910class BasicCon2vd(nn.Module): def __init__(self, in_channels, out_channels, **kwargs): super(BasicCon2vd, self).__init__() self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, **kwargs) self.relu = nn.ReLU(inplace=True) def forward(self, x): x = self.conv(x) x = self.relu(x) return x 由于卷积层都是伴随着ReLU激活函数来使用的，因此首先建立将二者合并的模板。 通过使用模板搭建的方式，使得网络得结构一目了然，很清晰。 Inception模板的搭建 1234567891011121314151617181920212223242526272829class Inception(nn.Module): def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj): super(Inception, self).__init__() self.branch1 = BasicCon2vd(in_channels, ch1x1, kernel_size=1) self.branch2 = nn.Sequential( BasicCon2vd(in_channels, ch3x3red, kernel_size=1), BasicCon2vd(ch3x3red, ch3x3, kernel_size=3, padding=1) # 将padding=1 使得branch的输入和输出是相同的 ) self.branch3 = nn.Sequential( BasicCon2vd(in_channels, ch5x5red, kernel_size=1), BasicCon2vd(ch5x5red, ch5x5, kernel_size=5, padding=2) # 将padding=2 使得branch的输入和输出是相同的 ) self.branch4 = nn.Sequential( nn.MaxPool2d(kernel_size=3, stride=1, padding=1), BasicCon2vd(in_channels, pool_proj, kernel_size=5, padding=2) # 将padding=2 使得branch的输入和输出是相同的 ) def forward(self, x): branch1 = self.branch1(x) branch2 = self.branch2(x) branch3 = self.branch3(x) branch4 = self.branch4(x) output = [branch1, branch2, branch3, branch4] return torch.cat(output, 1) # 在维度1上面进行叠加和整合, 即在channel上进行拼接 通过不同的padding参数，保证每个branch的输入和输出总是相同的。 最后，在维度dim=1上面进行叠加和整合, 即在channel上进行拼接。 辅助分类器的搭建123456789101112131415161718192021222324252627class InceptionAux(nn.Module): def __init__(self, in_channels, num_classes): super(InceptionAux, self).__init__() self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3) self.conv = BasicConv2d(in_channels, 128, kernel_size=1) # output[batch, 128, 4, 4] self.fc1 = nn.Linear(2048, 1024) self.fc2 = nn.Linear(1024, num_classes) def forward(self, x): # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14 x = self.averagePool(x) # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4 x = self.conv(x) # N x 128 x 4 x 4 x = torch.flatten(x, 1) # [batch channel width height] 1 代表在1的维度下进行展平 x = F.dropout(x, 0.5, training=self.training) # self.training model.train()模式下为true 在model.eval()下为False # N x 2048 x = F.relu(self.fc1(x), inplace=True) x = F.dropout(x, 0.5, training=self.training) # N x 1024 x = self.fc2(x) # N x num_classes return x 开始时，在平均池化下采样和卷积之前，一个辅助分类器与第二个辅助分类器的深度是不同的。 dropout层的随机概率p=0.7，但是实际的效果不一定好，因此我们将调整为p=0.5。 当我们实例化个模型model后，可以通过model.train()和model.eval()来控制模型的状态。在model.train()模式下self.training=True，在model.eval()模式下self.training=False GoogLeNet搭建123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101class GoogLeNet(nn.Module): def __init__(self, num_classes=1000, aux_logits=True, init_weight = False): super(GoogLeNet, self).__init__() self.aux_logits = aux_logits self.conv1 = BasicCon2vd(3, 64, kernel_size=7, stride=7, padding=3) # 计算默认为下取整 该层的作用是将H,W缩小为原来的一半 self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True) # ceil_mode=True时 取整方式变为了上取整 self.conv2 = BasicCon2vd(64, 64, kernel_size=1) self.conv3 = BasicCon2vd(64, 192, kernel_size=3, padding=1) self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32) self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64) self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64) self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64) self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64) self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64) self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128) self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True) self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128) self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128) if self.aux_logits: self.aux1 = InceptionAux(512, num_classes) self.aux2 = InceptionAux(528, num_classes) self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # (1, 1) 这里的(1, 1) &lt;=&gt; (width, Height) 无论输入是多少 输出都为1*1 self.dropout = nn.Dropout(p=0.4) self.fc = nn.Linear(1024, num_classes) if init_weight: self._initialize_weights() def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) def forward(self, x): # N x 3 x 224 x 224 x = self.conv1(x) # N x 64 x 112 x 112 x = self.maxpool1(x) # N x 64 x 56 x 56 x = self.conv2(x) # N x 64 x 56 x 56 x = self.conv3(x) # N x 192 x 56 x 56 x = self.maxpool2(x) # N x 192 x 28 x 28 x = self.inception3a(x) # N x 256 x 28 x 28 x = self.inception3b(x) # N x 480 x 28 x 28 x = self.maxpool3(x) # N x 480 x 14 x 14 x = self.inception4a(x) # N x 512 x 14 x 14 if self.training and self.aux_logits: # eval model lose this layer aux1 = self.aux1(x) x = self.inception4b(x) # N x 512 x 14 x 14 x = self.inception4c(x) # N x 512 x 14 x 14 x = self.inception4d(x) # N x 528 x 14 x 14 if self.training and self.aux_logits: # eval model lose this layer aux2 = self.aux2(x) x = self.inception4e(x) # N x 832 x 14 x 14 x = self.maxpool4(x) # N x 832 x 7 x 7 x = self.inception5a(x) # N x 832 x 7 x 7 x = self.inception5b(x) # N x 1024 x 7 x 7 x = self.avgpool(x) # N x 1024 x 1 x 1 x = torch.flatten(x, 1) # N x 1024 x = self.dropout(x) x = self.fc(x) # N x 1000 (num_classes) if self.training and self.aux_logits: # eval model lose this layer return x, aux2, aux1 return x self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)：ceil_mode=True时 取整方式变为了上取整。 nn.AdaptiveAvgPool2d((1, 1)) ：自适应的池化下采样操作，其中输入参数为一个元组，大小为目标输出的[H,w]值。 模型的训练但部分还是和之前的模型相似的，以下为不同的部分。 1234net = GoogLeNet(num_classes=5, aux_logits=True, init_weights=True) net.to(device) loss_function = nn.CrossEntropyLoss() optimizer = optim.Adam(net.parameters(), lr=0.0003) 首先是定义的模型为GoogLeNet与原来不同。 123456789101112131415for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits, aux_logits2, aux_logits1 = net(images.to(device)) loss0 = loss_function(logits, labels.to(device)) loss1 = loss_function(aux_logits1, labels.to(device)) loss2 = loss_function(aux_logits2, labels.to(device)) loss = loss0 + loss1 * 0.3 + loss2 * 0.3 loss.backward() optimizer.step() # print statistics running_loss += loss.item() train_bar.desc = &quot;train epoch[{}/{}] loss:{:.3f}&quot;.format(epoch + 1, epochs, loss) 其次是治理的损失函数用三个，训练时分别辅助分类器进行计算，之后再将计算得到的辅助分类器的值进行加权处理。 在训练是，会去计算各种辅助分类器的输出，计算损失函数，之后在验证和测试过程中，就不会再去计算辅助分类器了。 模型测试1234567# create modelmodel = GoogLeNet(num_classes=5, aux_logits=False).to(device)# load model weightsweights_path = &quot;./googleNet.pth&quot;assert os.path.exists(weights_path), &quot;file: '{}' dose not exist.&quot;.format(weights_path)missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device), strict=False) 这里，我们在训练过程中是保存了辅助分类器的参数的，但是在测试时，没有定义辅助分类器，因此将strict=False，默认为True，此时model.load_state_dict() 会返回两个结果，unexpected_keys保留的是之前辅助分类器的层。 最好的结果达到了86%左右。","link":"/2022/01/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/GoogLeNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"VGG网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); VGG网络详解简述VGG在2014年由牛津大学著名研究组VGG (Visual GeometryGroup)提出，斩获该年ImageNet竞赛中Localization Task (定位任务)第一名和Classification Task (分类任务)第二名。本文从模型的搭建与pytorch的实现进行具体的介绍。 VGG网络的几种配置 一般使用的时候，多用D网络。 网络中的亮点: 通过堆叠多个3x3的卷积核来替代大尺度卷积核——(减少所需参数) 论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核，代替后拥有相同的感受野。 CNN的感受野在卷积神经网络中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野(receptive field)。通俗的解释是，输出feature map上的一个单元对应输入层上的区域大小。 感受野的计算公式： F(i)=(F(i+1)-1)\\times Stride+K_{size} $F(i)$为第i层感受野，$Stride$为第i层的步距，$K_{size}$为卷积核或采样核尺寸。 Pool1：Size=2$\\times$2，Stride=2；Conv1：size=3$\\times$3，Stride=2 Feature map：$F=1$ Pool1：$F=(1-1)\\times2+2=2$ Conv1：$F=(2-1)\\times2+3=5$ 验证两个3$\\times$3的卷积核可以代替一个7$\\times$7(Stride=1): Feature map：$F=1$ Conv3$\\times$3(3)：$F=(1-1)\\times1+3=3$ Conv3$\\times$3(2)：$F=(3-1)\\times1+3=5$ Conv3$\\times$3(1)：$F=(5-1)\\times1+3=7$ 论文中提到，可以通过堆叠两个3x3的卷积核替代5x5的卷积核，堆叠三个3x3的卷积核替代7x7的卷积核。使用7x7卷积核所需参数，与堆叠三个3x3卷积核所需参数(假设输入输出channel为C) $7\\times7\\times C \\times C = 49C^2$ $3\\times3\\times C \\times C+3\\times3\\times C \\times C+3\\times3\\times C \\times C=27C^2$ 卷积层的作用是用来提取特征，全连接层的作用的用来分类。 VGG的pytorch实现模型搭建123456cfgs = { 'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'], 'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'], 'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],} 配置VGG的模型list，里面的数字代表Conv层的channel数，“M”代表Maxpool层。 其中cfg为一个dict的数据，Key为不同VGG的类型，Value为list即不同的VGG网络的配置参数。 卷积层的实现12345678910111213def make_features(cfg: list): layers = [] in_channels = 3 for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) layers += [conv2d, nn.ReLU(True)] # 每个卷积函数跟ReLU激活函数都是绑定在一起的。 in_channels = v # 下一层的输入为本层的输出 return nn.Sequential(*layers) 该代码是用来读取配置文件的，由于输入为一张图片，因此输入的in_channels=3。 nn.Sequential(layers)*：实例化非关键字参数实现的。 全连接层的实现12345678910111213self.classifier = nn.Sequential( # 第一个全连接层 nn.Dropout(p=0.5), nn.Linear(512*7*7, 2048), nn.ReLU(True), # 第二个全连接层 nn.Dropout(p=0.5), nn.Linear(2048, 2048), # 原论文为4096，为了加快速度我们设置为2048 nn.ReLU(True), # 第三个全连接层 nn.Linear(2048, class_num) ) 全部的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738class VGG(nn.Module): def __init__(self, features, class_num=1000, init_weights=False): super(VGG, self).__init__() self.features = features self.classifier = nn.Sequential( nn.Dropout(p=0.5), nn.Linear(512*7*7, 2048), nn.ReLU(True), nn.Dropout(p=0.5), nn.Linear(2048, 2048), # 原论文为4096，为了加快速度我们设置为2048 nn.ReLU(True), nn.Linear(2048, class_num) ) if init_weights: self._initialize_weights() def forward(self, x): # N x 3 x 224 x 224 x = self.features(x) # N x 512 x 7 x 7 # 展平操作，和之前相似 x = torch.flatten(x, start_dim=1) # N x 512*7*7 x = self.classifier(x) return x def _initialize_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') nn.init.xavier_uniform_(m.weight) if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): nn.init.xavier_uniform_(m.weight) # nn.init.normal_(m.weight, 0, 0.01) nn.init.constant_(m.bias, 0) 模型的实例化123456789def vgg(model_name=&quot;vgg16&quot;, **kwargs): try: cfg = cfgs[model_name] # 得出配置列表参数 except: print(&quot;Warning: model number {} not in cfgs dict!&quot;.format(model_name)) exit(-1) model = VGG(make_features(cfg), **kwargs) return model \\*kwargs*：代表可变字典长度的参数。 模型的训练训练的过程，基本和ALexNet相同。这里由于VGG网络的参数多，训练所需要的数据大，而现实过程中我们的参数不足，因此需要迁移学习的方法来解决比较好。","link":"/2022/01/30/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/VGG%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"ResNeXt网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); ResNeXt网络详解论文题目为Aggregated Residual Transformations for Deep Neural Networks，主要的创新点在于更新了block. 在宽度和深度上性能有了一定的提升。 ResNeXt网络在相同的计算量的情况下相比ResNet来说，错误率更小。 组卷积 本质上是将输入$C_{in}$划分为g个组，每个组$C_{in}/g$层，每个组通过卷积层之后将产生$n/g$层的一个卷积，再将g个组的卷积再拼接以下，就得出了n层的卷积网络。 极端情况$g=C_{in},n=C_{in}$，此时就是DW Conv. (c)首先$1 \\times 1$对进行降维处理，256-&gt;128；之后用组卷积group=32进行卷积，最后$1 \\times 1$进行升维，再和输入进行相加。 (b)先分组，分成32个组，每组channel=4，进行$1 \\times 1$卷积之后，再将每个组进行$3 \\times 3$的组卷积，归并之后其他跟(c)相同。 (a)前面和(b)相同，后面用一个相加来代替归+卷积操作。 理论上三者是等价的，(b)和(c)的等价比较显然，与(a)的等价不好理解。 相加的规则如下图所示，kernel=[1,2]卷积的运算结果如绿线所示。 分组卷积再相加的结果和直接卷积的结果相同。 利用组卷积代替他一般的卷积，得出ResNeXt网络的基本框架。下图的(32$\\times$4d)代表成32个组，每个组conv2_channel=4 如下图所示，作者通过实验来验证为什么要选分组数为32. 这里尽量保持计算量相同的情况下，通过改变不同的分组数和conv_channel，右图设计了几个不同的分组组合。 左面的图代表50和101层网络中，不同分组组合对应得错误率。 对于浅层得block来讲，效果没有很大的提升。 ResNeXt网络的pytorch实现网络的搭建基本和ResNet相同，基本是在ResNet上面的改进。 如右图所示，网络的框架的是相同的，改进主要发生在基本的block上面： 第二个卷积成换成了group=32的组卷积。 第一个卷积层的输入channel是原来ResNet的2倍。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Bottleneck(nn.Module): &quot;&quot;&quot; 注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2， 这么做的好处是能够在top1上提升大概0.5%的准确率。 可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch &quot;&quot;&quot; expansion = 4 def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64): super(Bottleneck, self).__init__() width = int(out_channel * (width_per_group / 64.)) * groups self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width, kernel_size=1, stride=1, bias=False) # squeeze channels self.bn1 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups, kernel_size=3, stride=stride, bias=False, padding=1) self.bn2 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion, kernel_size=1, stride=1, bias=False) # unsqueeze channels self.bn3 = nn.BatchNorm2d(out_channel*self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out += identity out = self.relu(out) return out width = int(out_channel \\ (width_per_group / 64.)) * groups*计算的是翻倍以后的ResNeXt网络的out_channel数，如图所示，ResNeXt网络的第一层out_channel=128是ResNet的2倍，根据计算公式 width=(64*(32/64))*32=128 out_channel代表的是残差块的out_channel数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374class ResNet(nn.Module): def __init__(self, block, blocks_num, num_classes=1000, include_top=True, groups=1, width_per_group=64): super(ResNet, self).__init__() self.include_top = include_top self.in_channel = 64 self.groups = groups self.width_per_group = width_per_group self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.in_channel) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) if self.include_top: self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # output size = (1, 1) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') def _make_layer(self, block, channel, block_num, stride=1): downsample = None if stride != 1 or self.in_channel != channel * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channel * block.expansion)) layers = [] layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride, groups=self.groups, width_per_group=self.width_per_group)) self.in_channel = channel * block.expansion for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x 相比之前的ResNet增加了groups=1, width_per_group=64 参数，在_make_layer()函数以及初始化的部分都增加上述参数，其余不变。 123456789def resnext50_32x4d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth groups = 32 width_per_group = 4 return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group) 这里我们采用第三种方法，仅仅训练最后一层全连接层的权重。 param.requires_grad = False 这样将前面的参数视之为不可训练的，这样就可以仅仅之训练最后一层的参数了，因为最后一层是重新定义的。 12345678net.load_state_dict(torch.load(model_weight_path, map_location=device))for param in net.parameters(): param.requires_grad = False # change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 数据预测数据的打包过程，这里的我们的首先将图片名称放到一个list中，之后使用batch_img = torch.stack(img_list, dim=0) 将list里面的图片打包成一个batch中。","link":"/2022/02/03/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/ResNeXt%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"ResNet网络详解与Pytorch实现","text":"hljs.initHighlightingOnLoad(); ResNet详解ResNet在2015年由微软实验室提出，斩获当年lmageNet竞赛中分类任务第一名，目标检测第一名。获得coco数据集中目标检测第一名，图像分割第一名。 网络中的亮点: 超深的网络结构(突破1000层) 提出residual模块 使用Batch Normalization加速训练(丢弃dropout) 随着网络的不断加深：梯度消失和梯度爆炸、退化问题。 通过残差结构可以很好的解决上述问题。 residual结构 如图，1x1的卷积层的主要作用是改变channel个数，即进行升维和降维； 注意，求和是时主分支和shoutcut的特征矩阵的形状[H,W]和深度channel必须相同。 左边这副图原本是以输入channel为64，3x3卷积层卷积核数也是64为例的，这里为了方便对比都改成了256。 表格中的乘积代表堆叠几层 下图所示，残差块的结构中残差有实线也有虚线。 实线连接和虚线连接的区别 其中的实线代表相加的二者形状和维度相同，即残差块的输入channel和输出的channel相同； 虚线表示残差块的输入channel和输出channel不同。因为虚线块首先要进行一个维度变化，导致input和shoutcut的维度不同，因此需要一个$kernel\\ size=1\\times1 \\, ,stride = 2$的卷积层来缩减矩阵形状，并且改变通道数来保证形状和维度相同。 一般在迭代的时候，将虚线层放在每次迭代的第一层来改变宽度和维度，第二层以后都为实线层，主要是用来将迭代，不改变任何参跨度和维度。 同理，另一个卷积的结构也适用。 注意原论文中，右侧虚线残差结构的主分支上，第一个1x1卷积层的步距是$stride=2$，第二个3x3卷积层步距是$stride=1$。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是$stride=1$，第二个3x3卷积层步距是$stride=2$，这样能够在ImageNet的top1上提升大概0.5%的准确率。详细可参考Resnet v1.5 综上，我们的残差块的主要的功能是降维缩减为原来的一半，之后将通道数变为原来的两倍。 Batch Normalization详解 Batch Normalization是google团队在2015年论文《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》提出的。通过该方法能够加速网络的收敛并提升准确率。 Batch Normalization 的目的是使得我们一批(Batch)的feature map满足均值为0，方差为1的规律。 Batch Normalization原理 我们在图像预处理过程中通常会对图像进行标准化处理，这样能够加速网络的收敛，如下图所示。 对于Conv1来说输入的就是满足某一分布的特征矩阵，但对于Conv2而言输入的feature map就不一定满足某一分布规律了（注意这里所说满足某一分布规律并不是指某一个feature map的数据要满足分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律）。 也就是说，卷积Conv1输入以前的满足归一化，输出后不一定还满足之前输入的归一化。同时数据的Normalization是针对feature map的每一个维度在整个训练集上的数据进行的。 Batch Normalization的目的就是使我们的feature map满足均值为0，方差为1的分布规律。 “对于一个拥有d维的输入x，我们将对它的每一个维度进行标准化处理。” 假设我们输入的x是RGB三通道的彩色图像，那么这里的d就是输入图像的channels即d=3，$x^{(1)}$代表的是图片R通道的特征矩阵，依次类推分别是G通道和B通道的。 x = (x^{(1)}, x^{(2)}, x^{(3)}) 标准化处理也就是分别对我们的R通道，G通道，B通道进行处理。上面的公式不用看，原文提供了更加详细的计算公式如下： 由于Normalization后归一化成[0,1]并非是最好的结果，因此这里选择了引入$\\gamma;\\beta$两个超参数一个是 让feature map满足某一分布规律，理论上是指整个训练样本集所对应feature map的数据要满足分布规律，也就是说要计算出整个训练集的feature map然后在进行标准化处理，对于一个大型的数据集明显是不可能的，所以论文中说的是Batch Normalization，也就是我们计算一个Batch数据的feature map然后在进行标准化（batch越大越接近整个数据集的分布，效果越好）。 使用BN时需要注意的问题（1）训练时要将traning参数设置为True，在验证时将trainning参数设置为False。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。 （2）batch size尽可能设置大点，设置小后表现可能很糟糕，设置的越大求的均值和方差越接近整个训练集的均值和方差。 （3）建议将BN层放在卷积层（Conv）和激活层（例如Relu）之间，且卷积层不要使用偏置bias，因为没有用，参考下图推理，即使使用了偏置bias求出的结果也是一样的. 总结如下 迁移学习迁移学习的优势 能够快速训练出一个理想的结果。 当数据集较少时也能训练出理想的效果。传统情况下，网络越深，数据越少，越容易造成过拟合。 迁移学习可以将别人预训练好的模型拿来使用，大大的加速训练的过程。 注意：在使用别人的预训练模型参数时，要注意别人的预处理方式。自己的预处理的方式要与预训练模型的相同。 浅层网络的学习到的信息具有通用性，可以直接迁移到比较复杂的网络中使用，大大减小的训练的时间。 迁移学习的主要目的是，将预训练好的浅层网络的通用的识别信息迁移到我们的要训练的模型里去，使得新的网络也拥有识别底层通用特征得能力。 常见的迁移学习方式：1．载入权重后训练所有参数；2．载入权重后只训练最后几层参数；3．载入权重后在原网络基础上再添加一层全连接层，仅训练最后一个全连接层(由于我们最后的一层输出的结点数和类别数可能不等于，因此最后加一层)。 一般情况下，第一种方法的效果是最好的，但相比第二和第三种方法，需要的运算资源较长、花费的时间也较长。 Resnet的pytorch实现模型搭建基本骨架的搭建18和34层结构上的主分支操残差结构的两层卷积网络是一模一样的，50、101、152层的残差结构中的三个卷积网络都不同。 这里现从基本的18层和34层的网络开始搭建。 1234567891011121314151617181920212223242526272829class BasicBlock(nn.Module): expansion = 1 # 通道数的变化，输出是输入的几倍 def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs): super(BasicBlock, self).__init__() self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=3, stride=stride, padding=1, bias=False) self.bn1 = nn.BatchNorm2d(out_channel) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel, kernel_size=3, stride=1, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(out_channel) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out += identity out = self.relu(out) return out downsample的作用是区分结果是否为虚线上的的残差结构。 out_channel为主分支上的通道数。 bias=False使用BN层时，偏置是不需要的，设为False. expansion输出是输入的几倍的参数。 以下是50层、101层和152层网络的搭建： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Bottleneck(nn.Module): &quot;&quot;&quot; 注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。 但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2， 这么做的好处是能够在top1上提升大概0.5%的准确率。 可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch &quot;&quot;&quot; expansion = 4 def __init__(self, in_channel, out_channel, stride=1, downsample=None, groups=1, width_per_group=64): super(Bottleneck, self).__init__() width = int(out_channel * (width_per_group / 64.)) * groups self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width, kernel_size=1, stride=1, bias=False) # squeeze channels self.bn1 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups, kernel_size=3, stride=stride, bias=False, padding=1) self.bn2 = nn.BatchNorm2d(width) # ----------------------------------------- self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel * self.expansion, kernel_size=1, stride=1, bias=False) # unsqueeze channels self.bn3 = nn.BatchNorm2d(out_channel * self.expansion) self.relu = nn.ReLU(inplace=True) self.downsample = downsample def forward(self, x): identity = x if self.downsample is not None: identity = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) out += identity out = self.relu(out) return out ResNet基本框架1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class ResNet(nn.Module): def __init__(self, block, blocks_num, groups=1, num_classes=1000, include_top=True, width_per_group=64): super(ResNet, self).__init__() self.include_top = include_top self.in_channel = 64 self.groups = groups self.width_per_group = width_per_group self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(self.in_channel) self.relu = nn.ReLU(inplace=True) self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) self.layer1 = self._make_layer(block, 64, blocks_num[0]) self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) if self.include_top: self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # output size = (1, 1) self.fc = nn.Linear(512 * block.expansion, num_classes) for m in self.modules(): if isinstance(m, nn.Conv2d): nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') def _make_layer(self, block, channel, block_num, stride=1): downsample = None if stride != 1 or self.in_channel != channel * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(channel * block.expansion)) layers = [] layers.append(block(self.in_channel, channel, downsample=downsample, stride=stride, groups=self.groups, width_per_group=self.width_per_group)) self.in_channel = channel * block.expansion for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) return nn.Sequential(*layers) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) x = self.layer4(x) if self.include_top: # include_top的主要的作用是对于需要在ResNet基础上做扩展的网络，则要将 # include_top设置为False x = self.avgpool(x) x = torch.flatten(x, 1) x = self.fc(x) return x _make_layer(self, block, channel, block_num, stride=1): 输入为block(基本骨架)，channel(输入通道数)，block_num(基本骨架循环的层数) if stride != 1 or self.in_channel != channel $\\times$ block.expansion: 此处的判断逻辑是stride!=1(有H,W的改变)或者输入维度与输出维度不相等(不满足in_channel = channel $\\times$ block.expansion就代表不是第一层)，对于每一个残差结构的第一个来说，其改变的宽度和深度，因此要用考虑虚线连接。之后，几层为一般的实线的结构，不需要if分支设计虚线层。 设计堆叠的实线的残差层。第一层为实线，后几层都为实线。 12345for _ in range(1, block_num): layers.append(block(self.in_channel, channel, groups=self.groups, width_per_group=self.width_per_group)) nn.Sequential(\\layers)* 返回值为非关键字参数。 第一个残差层不改变[H,W]，因此stride设计为1. 12345self.layer1 = self._make_layer(block, 64, blocks_num[0]) # 第一个残差层不改变[H,W]，因此stride设计为1 self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2) self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2) self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2) 1234567891011121314151617181920212223242526272829303132333435def resnet34(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet34-333f7ec4.pth return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)def resnet50(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet50-19c8e357.pth return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)def resnet101(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)def resnext50_32x4d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth groups = 32 width_per_group = 4 return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group)def resnext101_32x8d(num_classes=1000, include_top=True): # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth groups = 32 width_per_group = 8 return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top, groups=groups, width_per_group=width_per_group) 定义不同类型的网络。 模型训练迁移学习1import torchvision.models.resnet ctrl+左键进入后可以直接看源码。可以下载预训练参数进行的前迁移学习，网址如下： 1234567891011model_urls = { 'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth', 'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth', 'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth', 'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth', 'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth', 'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth', 'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth', 'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth', 'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',} 数据增强处理123456789data_transform = { &quot;train&quot;: transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), &quot;val&quot;: transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])} 载入预训练模型的方法12345678910111213net = resnet34()# load pretrain weights# download url: https://download.pytorch.org/models/resnet34-333f7ec4.pthmodel_weight_path = &quot;./resnet34-pre.pth&quot;assert os.path.exists(model_weight_path), &quot;file {} does not exist.&quot;.format(model_weight_path)net.load_state_dict(torch.load(model_weight_path, map_location=device))# for param in net.parameters():# param.requires_grad = False# change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 首先，定义网络net = resnet34(); 之后，导入预训练的数据集 12model_weight_path = &quot;./resnet34-pre.pth&quot;net.load_state_dict(torch.load(model_weight_path, map_location=device)) 最后，改变模型最后一层结构。 1234# change fc layer structurein_channel = net.fc.in_featuresnet.fc = nn.Linear(in_channel, 5)net.to(device) 也可以先载入到内存里面，之后将全连接层删去，将其载入到模型中去。 12torch.load(model_weight_path, map_location=device)# 载入到内存 其余的部分跟之前一样 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import osimport jsonimport torchimport torch.nn as nnimport torch.optim as optimfrom torchvision import transforms, datasetsfrom tqdm import tqdmfrom model import resnet34def main(): device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;) print(&quot;using {} device.&quot;.format(device)) data_transform = { &quot;train&quot;: transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]), &quot;val&quot;: transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])} data_root = os.path.abspath(os.path.join(os.getcwd(), &quot;../..&quot;)) # get data root path image_path = os.path.join(data_root, &quot;data_set&quot;, &quot;flower_data&quot;) # flower data set path assert os.path.exists(image_path), &quot;{} path does not exist.&quot;.format(image_path) train_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;train&quot;), transform=data_transform[&quot;train&quot;]) train_num = len(train_dataset) # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4} flower_list = train_dataset.class_to_idx cla_dict = dict((val, key) for key, val in flower_list.items()) # write dict into json file json_str = json.dumps(cla_dict, indent=4) with open('class_indices.json', 'w') as json_file: json_file.write(json_str) batch_size = 16 nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8]) # number of workers print('Using {} dataloader workers every process'.format(nw)) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw) validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, &quot;val&quot;), transform=data_transform[&quot;val&quot;]) val_num = len(validate_dataset) validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size, shuffle=False, num_workers=nw) print(&quot;using {} images for training, {} images for validation.&quot;.format(train_num, val_num)) net = resnet34() # load pretrain weights # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth model_weight_path = &quot;./resnet34-pre.pth&quot; assert os.path.exists(model_weight_path), &quot;file {} does not exist.&quot;.format(model_weight_path) net.load_state_dict(torch.load(model_weight_path, map_location=device)) # for param in net.parameters(): # param.requires_grad = False # change fc layer structure in_channel = net.fc.in_features net.fc = nn.Linear(in_channel, 5) net.to(device) # define loss function loss_function = nn.CrossEntropyLoss() # construct an optimizer params = [p for p in net.parameters() if p.requires_grad] optimizer = optim.Adam(params, lr=0.0001) epochs = 3 best_acc = 0.0 save_path = './resNet34.pth' train_steps = len(train_loader) for epoch in range(epochs): # train net.train() running_loss = 0.0 train_bar = tqdm(train_loader) for step, data in enumerate(train_bar): images, labels = data optimizer.zero_grad() logits = net(images.to(device)) loss = loss_function(logits, labels.to(device)) loss.backward() optimizer.step() # print statistics running_loss += loss.item() train_bar.desc = &quot;train epoch[{}/{}] loss:{:.3f}&quot;.format(epoch + 1, epochs, loss) # validate net.eval() acc = 0.0 # accumulate accurate number / epoch with torch.no_grad(): val_bar = tqdm(validate_loader) for val_data in val_bar: val_images, val_labels = val_data outputs = net(val_images.to(device)) # loss = loss_function(outputs, test_labels) predict_y = torch.max(outputs, dim=1)[1] acc += torch.eq(predict_y, val_labels.to(device)).sum().item() val_bar.desc = &quot;valid epoch[{}/{}]&quot;.format(epoch + 1, epochs) val_accurate = acc / val_num print('[epoch %d] train_loss: %.3f val_accuracy: %.3f' % (epoch + 1, running_loss / train_steps, val_accurate)) if val_accurate &gt; best_acc: best_acc = val_accurate torch.save(net.state_dict(), save_path) print('Finished Training')if __name__ == '__main__': main() 视频参考","link":"/2022/02/03/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/ResNet%E7%BD%91%E7%BB%9C%E8%AF%A6%E8%A7%A3%E4%B8%8EPytorch%E5%AE%9E%E7%8E%B0/"},{"title":"Latex基础教程（二）","text":"hljs.initHighlightingOnLoad(); 本文主主要包含Latex的表格操、作浮动体的设置、参考文献以及自定义命令的设置，主要参考了B站的latex中文教程-15集从入门到精通包含各种latex操作视频的后9讲。 Latex中的表格123\\begin{tabular}[&lt;垂直对齐方式&gt;]{&lt;列格式说明&gt;} &lt;表项&gt;&amp;&lt;表项&gt;&amp; ...&lt;表项入&gt; \\\\\\end{tabular} 用\\\\\\表示换行 用&amp;表示不同的列 对于格式说明参数主要有： l-本列左对齐 c-本列居中对齐 r-本列右对齐 p{&lt;宽&gt;}-本列宽度固定,能够自动换行 如图，一共五列，因此就有五个格式说明符。 其次，两个列之间加一个竖线加一个”|“，如果加一个双竖线就加一个”||“。 如果要加一个横线了，就在行前或行后加\\hline。 帮助文档的打开 123texdoc booktab # 三线表texdoc longtab # 长表格宏包texdoc tabu # 综合表格宏包 Latex中的浮动体浮动体的创建操作LATEX 预定义了两类浮动体环境figure 和table。习惯上figure 里放图片，table 里放表格，但并没有严格限制，可以在任何一个浮动体里放置文字、公式、表格、图片等等任意内容。 对于一般的图像，有figure浮动体环境。 对于表格，有table浮动体环境。 结果可以看出图像和表格的位置都发生了浮动效果。 给浮动体加属性以table 环境的用法举例： 123\\begin{table}[⟨placement⟩]...\\end{table} ⟨placement⟩ 参数提供了一些符号用来表示浮动体允许排版的位置，如hbp 允许浮动体排版在当前位置、底部或者单独成页。table 和figure 浮动体的默认设置为tbp。 \\caption{...}为设置浮动体的标题。 \\centering为设置图片的居中和偏左偏右。 \\label{...}为设置标签操作，方便后面的应用。 \\ref{...}为引用设置，和标签在一起来实现交叉引用。 这里浮动体的交叉引用和标号是自动完成的。 总结： 浮动体可以实现灵活的分页（避免无法分割的内容产生的页面六百留白） 可以给图表添加标题。 交叉引用。 123456789figure环境(table环境与之类似)\\begin{figure}[&lt;允许位置&gt;]&lt;任意内容&gt;...\\end{figure}&lt;允许位置&gt;参数(默认tbp)% h，此处( here)-代码所在的上下文位置% t，页顶(top)-代码所在页面或之后页面的顶部% b，页底( bottom)一代码所在页面或之后页面的底部% p，独立一页(page)-浮动页面 标题控制( caption、bicaption等宏包) 并排与子图表( subcaption、subfig、floatrow等宏包) 绕排(picinpar、 wrapfig等宏包) Latex中的参考文献thebibliography环境的使用 一次管理，一次使用参考文献格式: 12345\\begin{thebibliography}{编号样本}\\bibitem[记号]{引用标志}文献条目1\\bibitem[记号]{引用标志}文献条目2end{thebibliography}% 其中文献条目包括:作者，题目，出版社，年代，版本，页码等。 如下为一个具体的案例： 123456789101112引用文章\\cite{article1}，引用一本书\\cite{book1}\\begin{thebibliography}{99}\\bibitem{article1}陈立辉,苏伟,蔡川,陈晓云，\\emph{基于LaTex的Meb数堂公式提取方法研究}[7].计算机科学．2014(86)\\bibitem{book1}William H. Press,saul A. Ieukolsky,william T. Vetterling,Brian P. Elanneny.,\\emph{Numerical Recipes 3rd Edition:The Art of scientific Computing}Cambridge University Press,New York ,2007.\\bibitem{latexGuide} Kopka Helmut,w.Daly Patrick,\\emph{Guide to \\LaTeX}, $4^{th}$ Edition.Available at \\texttt{http://www.amazon.com}.\\bibitem{latexMath} Graetzer. George，\\emph{Math Into \\LaTeX},BirkhAtuser Boston;3 edition (June 22，2000).\\end{thebibliography} \\emph{}表示要强调的具体内容。 引用的时候可以采用：\\cite{引用标志1,引用标志2,...}。因此，这样管理参考文献需要给每个参考文献进行详细排版，比较的麻烦，同时也方便文献的跨文献的引用。 因此需要一个可以一次管理多次使用的包。 使用BibTex文件进行参考文献管理 bibtex文件的格式如下： 12345@⟨type⟩{⟨citation⟩, ⟨key1⟩ = {⟨value1⟩}, ⟨key2⟩ = {⟨value2⟩},...} 其中⟨type⟩ 为文献的类别，如article 为学术论文，book 为书籍，incollection 为论文集中的某一篇，等等。 ⟨citation⟩ 为\\cite 命令使用的文献标签。在⟨citation⟩ 之后为条目里的各个字段，以⟨key⟩ = {⟨value⟩} 的形式组织。 我们在此简单列举学术论文里使用较多的BIBTEX 文献条目类别： article 学术论文，必需字段有author, title, journal, year; 可选字段包括volume, number,pages, doi 等； book 书籍，必需字段有author/editor, title, publisher, year; 可选字段包括volume/number,series, address 等； incollection 论文集中的一篇，必需字段有author, title,booktitle, publisher, year; 可选字段包括editor, volume/number, chapter, pages, address 等； inbook 书中的一章，必需字段有author/editor, title,chapter/pages, publisher, year; 可选字段包括volume/number, series, address 等。 具体的使用： 在导言区设置参考文献的格式 1234\\bibliographystyle{plain}%plain unsrt alpha abbrv\\bibliographystyle[round]{plain}% 引用的括号变为圆括号 在正文区设置参考文献 1234567%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\bibliography{test} % 可以不写扩展名\\end{document} 然后bibtex.exe会编译.aux的辅助文件，根据\\citation在.bib文件中选择指定的参考文献，并指定的参考文献按照指定的样式进行排版，生成另一个.bbl辅助文件。 \\cite {xxx}，引用的标志为下图所示。 使用google的相应功能来实现数据库的维护。 在搜索结果中打开引用链接。 打开bibtex可以得到该文件对于的bibtex数据。 将生成的bibtex数据copy到文献数据库中即可。 使用知网导入数据，需要导入zotero的FireFox浏览器。 1.打开浏览器的关联插件； 2.选择需要的文献。 3.在Zotero里选择导出的文献，来执行导出条目的操作。 4.打开test.bib文件 5.引用之后的排版如下： 1234567%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\bibliography{test} % 可以不写扩展名\\end{document} 默认只能显示导入数据库内已经引用的文献。 对于还未引用的文献，可以使用\\nocite{}来显示。 12\\nocite{*} % 表示所有未引用的参考文献\\notice{xxx} % 特定的未引用的参考文献 特别注意，重新编译的时候要手动删除之前的参考文献，否则编译完之后没有变化。 12345678%正文区(文稿区)\\begin{document} 引用一个无人车的文献\\cite{__2021} \\nocite{*} \\bibliography{test} % 可以不写扩展名\\end{document} 输出的结果如下： 使用BibLaTex文件进行参考文献管理biblatex 宏包是一套基于LATEX 宏命令的参考文献解决方案，提供了便捷的格式控制和强大的排序、分类、筛选、多文献表等功能。biblatex 宏包也因其对UTF-8 和中文参考文献的良好支持，被国内较多LATEX 模板采用。 新的TEX参考文献排版引擎biblatex/ biber样式文件(参考文献样式文件—bbx文件，引用样式文件—cbx文件)使用LATEx编写支持根据本地化排版，如: 12biber -l zh__pinyin texfile %用于指定按拼音排序biber -l zh__stroke texfile %用于按笔画排序 配置编译器，将默认的文献工具设置为Biber。 首先是在导言区调用biblatex 宏包。宏包支持以⟨key⟩=⟨value⟩ 形式指定选项，包括参考文献样式style、参考文献著录排序的规则sorting 等。 1\\usepackage[style=numeric,backend=biber]{biblatex} 接着在导言区使用\\addbibresource 命令为biblatex 引入参考文献数据库。与基于BIBTEX的传统方式不同的是，这里需要写完整的文件名。 1\\addbibresource{test.bib} 在正文中使用\\cite 命令引用参考文献。除此之外还可以使用丰富的命令达到不同的引用效果， 如\\citeauthor 和\\citeyear 分别单独引用作者和年份， \\textcite 和\\parencite分别类似natbib 宏包提供的\\citet 和\\citep 命令，以及脚注式引用\\footcite 等。 最后在需要排版参考文献的位置使用命令\\printbibliography 默认的title是英文的Reference，可以通过增加可选参数来修改。 ```latex\\printbibliography[title={参考文献}] 123456789101112 &gt; 重新编译时，要清除上一次生成的辅助文件。- **开源的样式文件**https://gitlab.com/CasperVector/biblatex-caspervector - 先将格式文件拷贝到当前工作目录中。 ![image-20220207232045374](https://gitee.com/houdezaiwu2022/image-bed/raw/master/latex/202202072320093.png) ```latex \\usepackage[style=caspervector,backend=biber,utf8]{biblatex} % 修改导言区的文件，使得引用caspervector样式 但是文章是中英文混排的，需要修改biber.exe的相关配置。 并且修改导言区的宏包配置。 12345678\\usepackage[style=caspervector,backend=biber,utf8,sorting=centy]{biblatex}% sorting=centy 就是按先英文、再中文、再姓名、再标题、再出版年份的顺序进行排序% c——chinese 中文% e——English 英文% n——name 作者姓名% t——title 文献标题% y——year 出版年份 这里需要连续编译两次，来产生正确的编号。 使用.bat进行批处理编译流程。 LaTeX自定义命令和环境\\newcommand一定义命令命令只能由字母组成,不能以 \\end开头 1\\newcommand&lt;命令&gt;[&lt;参数个数&gt;][&lt;首参数默认值&gt;]{&lt;具体定义&gt;} \\newcommand可以是简单字符串替换12%例如:使用\\PRC相当于 People 's Republic of \\emph{China}这一串内容\\newcommand \\PRC{People's Republic of \\emph{china}} \\newcommand也可以使用参数1234567891011%参数个数可以从1到9,使用时用#1,#2,..... .,#9表示\\newcommand \\ loves[2]{#1 喜欢#2}\\newcommand \\hatedby[2]{#2不受#1喜欢}\\begin{document}\\loves{猫儿}{鱼}\\hatedby{猫儿}{萝卜}\\end{document}% 猫儿喜欢鱼% 萝卜不受猫儿喜欢 \\newcommand的参数也可以有默认值123456789%指定参数个数的同时指定了首个参数的默认值，那么这个命令的%第一个参数就成为可选的参数(要使用中括号指定)\\newcommand \\love[3][喜欢]{#2#1#3}\\begin{document}\\love[最爱]{猫儿}{鱼}\\end{document}% 猫儿最爱鱼 \\renewcommand-重定义命令1234%与 \\newcommand 命令作用和用法相同,但只能用于已有命令% \\renewcommand&lt;命令&gt;[&lt;参数个数&gt;][&lt;首参数默认值&gt;]{&lt;具体定义&gt;}\\renewcommand\\abstractname{内容简介}% 将摘要的名称改为内容简介 注意，该命令只能修改已经定义的命令，不能修改还没有的命令，其他地方的使用方法相似。 定义和重定义环境12345% 基本用法和重定义命令类似\\newenvironment{&lt;环境名称&gt;}[&lt;参数个数&gt;][&lt;首参数默认值&gt;] {&lt;环境前定义&gt;}{&lt;环境后定义&gt;}\\renewenvironment{&lt;环境名称&gt;}[&lt;参数个数&gt;][&lt;首参数默认值&gt;]% {&lt;环境前定义&gt;}{&lt;环境后定义&gt;} 为book类中定义摘要( abstract)环境 12345678910111213% 导言\\newenvironment{myabstract}[1][摘要]%{\\small \\begin{center}\\bfseries #1 \\end{center}% \\begin{quotation}}%{\\end{quotation}}% 正文\\begin{document}\\begin{myabstract}[我的摘要]% 我的摘要 -&gt; #1参数 这是一段自定义格式的摘要...\\end{document} 重定义参数的嵌套使用： 123456789101112131415% 环境参数只有&lt;环境前定义&gt;中可以使用参数，% &lt;环境后定义&gt;中不能再使用环境参数。% 如果需要，可以先把前面得到的参数保存在一个命令中，在后面使用:\\newenvironment{Quotation}[1]%{\\newcommand \\quotesource{#1}% \\begin{quotation}}% {\\par \\hfill--- 《textit{\\quotesource}》% \\end{quotation}} % 正文\\begin{document} \\begin{Quotation}{易$\\cdot$乾} 初九,潜龙勿用。 \\end{Quotation}.\\end{document} 总结： 定义命令和环境是进行LaTeX格式定制、达成内容与格式分离且标的利器。 使用自定义的命令和环境把字体、字号、缩进、对齐、间距等各种琐细的内容包装起来，赋以一个有意义的名字,可以使文挡结构清晰、代码整洁、易于维护。 在使用宏定义的功能时，要综合利用各种已有的命令、环境、变量等功能，事实上，前面所介绍的长度变量与盒子、字体字号等内容，大多并丕直接出现在文档正文史，而主要都是用在实现各种结构化的宏定义里。 问题汇总 中文乱码问题。在网上下载了一个中文模板，跑起来没问题，语句测试也没有问题。但是在我的文件里却一再出现乱码。 原因：编码问题。.tex文件要以utf-8格式存储。然后导言区加入： 1\\usepackage[UTF8,noindent]{ctex} 编译器的配置。右侧的Default compiler默认值是pdflatex。这个并不支持中文的。因此将其设置为xelatex。 更多教程请见https://www.bilibili.com/video/BV1Zh411y7ps?p=2","link":"/2022/02/08/Latex/Latex%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B(%E4%BA%8C)/"},{"title":"Latex基础教程(一)","text":"hljs.initHighlightingOnLoad(); 本文主主要包含Latex文件基本结构、如何处理中文、字体字号的设置、文档篇章的设置、特殊字符的输入以及图像的插入和排版，主要参考了B站的latex中文教程-15集从入门到精通包含各种latex操作视频的前8讲。 Latex文件的基本结构导言区 在latex中，我们使用% 来进行注释。 导言区主要进行全局设置 \\title{}设置文章的标题 \\author{}设置文章的的作者 \\date{}设置文章的时间 但是设置的全局的属性是看不到的，要在正文区加\\maketitle才可以显示出来 如果使用的是book类，则输出title是单独在一页的。 1234567% 导言区\\documentclass{book}% report book letter article\\title{My First Document}\\author{Zhengtong Cao}\\date{\\today} 正文区 使用\\begin{}和\\end{}来创建一个环境。 每个文章里最多有一个document环境。 12345678% 正文区\\begin{document} \\maketitle hello, \\LaTeX Let $f(x)$ be define by the formula $f(x)=3x^2+x-1$ \\end{document} 如果两句话之间加一个空行，则相当于一个换行操作。 两段话之间可以加入多个空行，但是现实的时候就只有一个空行了。 这里的空行是实实在在的空行，不可以是注释行。之前错误的以为注释行可以作为一个空行，但是编译出来发现之前的换行消失了。 单 是行内公式，双$$$ $$$的作用是行间公式， \\begin{equation},\\end{equation}环境可以产生带编号的行间公式。 123\\begin{equation} AB^2+BC^2=AC^2\\end{equation} Latex中的中文处理texstdio配置设置编译器为XeLateX 设置默认编码为UTF-8 在导言区导入ctex宏包 未定义符号的定义1234$\\angle$A=90$\\degree$% latex内没有对应的命令\\degree的命令，编译时会显示错误，要在导言区重新定义\\newcommand{\\degree}{^\\circ} latex内没有对应的命令\\degree的命令，要在导言区重新定义，\\newcommand{\\degree}{^\\circ} 由于latex的编译器不同造成的，在typora里就定义了\\degree命令，但在latex里就没有，要重新定义。 中文的输入123456\\usepackage{ctex}\\title{\\heiti \\LaTeX 学习笔记}%\\author{Zhengtong Cao}\\author{\\kaishu 曹政通}\\date{\\today} 字体的设置：\\heiti 黑体、kaishu 楷体。 导入宏包后直接输入即可，无视下面的红色下滑线。 在cmd里输入以下命令，可以弹出ctex的宏包手册。这里也可以直接导入\\ctexart,\\ctexbook等。 1texdoc ctex 可以使用texdoc命令来查看各种文档 12texdoc lshort-zh# 一个latex的简易学习文件，112分钟入门 Latex字体字号的设置 字体族设置字体族主要分为罗马字体\\textrm{Roman Family}；无衬线字体\\textsf{San serif Family}；打字机字体\\texttt{Typewriter} 如图，\\rmfamily 代表以后的字体都是罗马体，如果之后又其他的字体的设置如\\ttfamily 则上个作用自动结束，开始新的作用。 一般使用时会加上{\\rmfamily xxx}来限定字体族的作用范围。 \\rmfamily罗马体，\\sffamily 无衬线字体，\\ttfamily打字机字体。 字体系列设置字体系列主要分为粗细、宽度设置如\\textmd{ Medium Series } 和 \\textbf{ Boldface Series }（字体设置命令） { \\mdseries xxx }和{ \\bfseries xxx } (字体设置声明) 字体形状命令字体形状直立\\textup{upright Shape}、斜体\\textit{Italic Shape}、伪斜体\\textsl{s1anted Shape}、小型大写\\textsc{Small caps Shape}（字体设置命令） { \\upshape Upright shape} {\\itshape Italic shape} {\\slshape Slanted Shape} {\\scshape Small caps Shape}(字体设置声明) 中文字体{\\songti 宋体} {\\heiti 黑体} {\\fangsong 仿宋} {\\kaishu 楷书}中文字体的\\textbf{粗体}（黑体）与\\textit{斜体}（楷体） 字体大小的设置 设置全文的默认字号大小，一般只有种10pt、11pt、12pt 对于\\ctex宏包来说又一些设置自好操作命令。 自定义字体操作 latex的思想时将内容和排版分里，因此经常在导言区设置\\newcommand{}{}来设计字体样式。 Latex文档的篇章结构提纲构建的几个命令： 1234\\chapter{⟨title⟩} %章\\section{⟨title⟩} %节\\subsection{⟨title⟩} %小节\\subsubsection{⟨title⟩} %子小节 其中\\chapter 只在book 和report 文档类有定义。这些命令生成章节标题，并能够自动编号。 article 文档类带编号的层级为\\section / \\subsection / \\subsubsection 三级； report/book 文档类带编号的层级为\\chapter / \\section / \\subsection 三级 如下，对于节操作的一些参数设计： 带可选参数的变体：\\section[⟨short title⟩]{⟨title⟩}标题使用⟨title⟩ 参数，在目录和页眉页脚中使用⟨short title⟩ 参数 带星号的变体：\\section*{⟨title⟩}标题不带编号，也不生成目录项和页眉页脚 分段操作 分段操作一般是插入一个空行，也可以使用\\par,但是多用插入空行的命令。\\\\的作用是换行而不是分段。 \\ctexart 命令 通过\\ctexart 设置布局时section是居中排版的. 使用\\ctexset命令可以自定义ctexart格式。 排版以后的格式如图所示。 目录的生成 在LATEX 中生成目录非常容易，只需在合适的地方使用命令：\\tableofcontents 这个命令会生成单独的一章（book / report）或一节（article），标题默认为“Contents“ \\tableofcontents生成的章节默认不写入目录（\\section*或\\chapter*） Latex中的特殊字符空白符号 空行分段，多个空行等同1个； 自动缩进，绝对不能使用空格代替； 英文中多个空格处理为1个空格，中文中空格将被忽略； 汉字与其它字符的间距会自动由XeLaTeX处理； 禁止使用中文全角空格。 通过命令来生成空格字符： 12345678910111213141516171819202122% 1em(当前字体中M的宽度)产生一个空格的宽度a\\quad b% 2em 产生的两个空格的宽度a\\qquad b% 约为1/6个空格的宽度a\\,b a\\thinspace b% 0.5个空格命令a\\enspace b% 产生一个空格a\\ b% 硬空格a~b% 1pc=12pt=4.218mm % 产生固定宽度值的空白 a\\kern 1pc ba\\kern -1em ba\\hskip 1em ba\\hspace{35pt}b% 占位宽度(根据大括号内的占位符的宽度产生空白)a\\hphantom{xyz}b%弹性长度(即占满整个的空间)a\\hfill b Latex的控制符由于有些符号在Latex中具有特殊的含义，因此输入的时候前面要加\\ (注意，\\\\代表换行操作，因此这里用\\textbackslash来代替) 1\\# \\$ \\% \\{ \\} \\~{} \\_{} \\&amp; \\textbackslash 输出结果如下： Latex排版符号排版中的特殊符号如下： 1\\S \\P \\dag \\ddag \\copyright \\pounds TEX标志符号123456789101112131415% 基本符号\\Tex{} \\LaTeX{} \\LaTeXe{}\\usepackage{xltxtra}% 提供了针对XeTeX的改进并且加入了XeTeX的LOGO% xltxra宏包\\XeLaTeX\\usepackage{texnames}% texnames宏包提供\\AmSTeX{} \\AmS-\\LaTeX{}\\BibTeX{} \\LuaTeX{}\\usepackage{mflogo}% mflogo宏包\\METAFONT{} \\MF{} \\MP{} 输出结果： 引号1`(左引号) '(右引号) ``(左双引号) ''(右双引号) 连字符使用几个减号的累加来代表不同长度的连字符。 1- -- --- 非英文字符12% 非英文字符\\oe \\OE \\ae \\AE \\aa \\AA \\o \\O \\l \\L \\ss \\SS !`?` 重音符号(以o为例)12%重音符号(以o为例)\\`o \\'o \\^o \\''o \\~o \\=o \\.o \\u{o} \\v{o}\\H{o} \\r{o} \\t{o} \\b{o} \\c{o} \\d{o} Latex的图像插入123456%导言区: \\usepackage{graphicx}%语法:\\includegraphics[&lt;选项&gt;]{&lt;文件名&gt;}%格式: EPS,PDF,PNG,JPEG,BMP\\usepackage{graphicx}\\graphicspath{{figures/},{pic/}} %图片在当前目录下的 figures目录 \\includegraphics[&lt;选项&gt;]{&lt;文件名&gt;} 的必选参数为文件名，可选参数为图片的缩放和旋转。 文件名，可以加类型后缀.jpg .png \\graphicspath{ {figures/},{pic/} } 指定图片的搜索路径，多个路径使用，隔开。 hexo 排版中连续两个大括号之间要加空格。 123456789101112% 指定缩放比例\\includegraphics[scale=0.3]{lion}\\includegraphics[scale=0.03]{mountain}\\includegraphics[scale=0.3]{oscilloscope}% 指定固定宽度和高度\\includegraphics[height=2cm]{lion}\\includegraphics[width=2cm]{lion.jpg}% 指定相对的高度和宽度\\includegraphics[width=0.2 \\textwidth]{lion}\\includegraphics[width=0.2 \\textheight]{mountain}% 指定多个参数\\includegraphics[angle=-45,width=0.2\\textwidth]{lion} 细节查看: 1texdoc graphic","link":"/2022/02/08/Latex/Latex%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B(%E4%B8%80)/"},{"title":"第0章绪论","text":"hljs.initHighlightingOnLoad(); 本章主要介绍控制理论的性质、发展、应用以及控制动态系统的几个基本步骤，为以后现代控制理论的学习做铺垫。 0.1 控制理论的性质控制理论的两个目标： 了解基本控制原理； 以数学表达它们，使它们最终能用以计算进入系统的输入，或用以设计自动控制系统。 两个主题：自动控制领域中有两个不同的但又相互联系的主题。 反馈的概念。 最优控制的概念。 0.2 控制理论的发展 20世纪20年代到40年代，马克斯威尔对装有调速器的蒸汽机系统动态特性的分析、马诺斯基对船舶驾驶控制的研究都是控制理论的开拓性工作。 20世纪40年代至50年代，维纳对控制理论作出了创造性的贡献。 20世纪50年代后期到60年代初期是控制理论发展的转折时期。 苏联学者在20世纪50年代对包含非线性特性、饱和作用和受到限制的控制等因素的系统的最优瞬态的研究表现出很大的兴趣。这些学者的研究讨论导致了庞特里亚金的“极大值原理”。 显示控制理论转折时期的另一个里程碑是20世纪50年代后期卡尔曼(卡尔曼——布西)滤波器的发现。 最近25年线性系统理论的研究非常活跃。 20世纪60年代后期和70年代早期，将线性二次型理论推广到无穷维系统(即以偏微分方程、泛函微分方程、积分微分方程和在巴拿赫空间的一般微分方程描述的系统)的工作得到很大进展。 目前研究的是以线性偏微分方程或相对简单的迟延方程描述的只能在空间的边界上加以观察和控制的系统。 20世纪70年代末80年代初，反馈控制的设计问题经历了一个重新修正的过程。随着人工智能的发展和引入了新的计算机结构，控制理和计算机科学的联系愈来愈密切。 0.3控制理论的应用控制系统之所以能得到如此普遍的应用，要归功于 现代仪表化(完备的传感器和执行机构) 便宜的电子硬件 控制理论有处理其模型和输出信号所具有的不确定性动态系统的能力 在控制理论中已完善的各种方法愈来愈得到普遍应用的同时，先进的理论概念的应用却仍集中在像空间工程那样的高技术方面。当然，由于计算机技术的飞速发展和世界性的激烈的工业竞争，这种情况将会改变。 控制概念得到主要应用的一个领域是石油化工生产过程。钢铁行业中热轧厂是最早成功地采用计算机控制的工厂。 0.4 控制一个动态系统的几个基本步骤简单地说，控制一个动态系统有下列四个基本步骤： 建模：为一个系统选择一个数学模型是控制工程中最重要的工作。 系统辨识： 定义为用在一个动态系统上观察到的输入与输出数据来确定它的模型的过程。 当前系统辨识方面的研究集中在下列诸基本问题上：辨识问题的可解性和问题提出的恰当性、对各类模型的参数估计方法。 信号处理：控制理论外面的独立的一门学科，但这两学科之问有许多重叠之处，而控制界曾对信号处理作出了重要贡献，特别是在滤波和平滑的领域。 控制的综合：为控制系统生成控制规律。这些过程的复杂性导致了各种控制研究课题，主要有：","link":"/2022/02/08/%E7%8E%B0%E4%BB%A3%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E7%AC%AC0%E7%AB%A0%E7%BB%AA%E8%AE%BA/"},{"title":"Auto-Encoder的补充内容","text":"hljs.initHighlightingOnLoad(); 本文主要介绍的是auto-encoder的补充内容，包括除了重构误差以外评估编码器效果的判别器模型，以及离散化表示学习的方法来进行嵌入。 More than minimizing reconstruction error 什么是好的embedding，好的embedding可以很好的表示其编码的对象的主要特征（如五等分花嫁中三玖的特点是带耳机）。 除了Reconstruction，如何评估encoder的效果？(下图为 一个编码的例子) 使用一个判别器discriminator——binary classifer 如果编码正确输出1，错误输出0。此时分类器可以很容易判断输出编码的争取与否，即编码具有很强的代表性。 如果编码不具有代表性，分类器很难判断正确和错误，此时编码器的代表很差。 于是，训练的目标函数变成最小化$\\theta$ 使得损失函数$L_D^*$最小。 \\begin{align} \\theta^*&=arg\\ \\min_\\limits{\\theta} \\ L_D^*\\\\ &=arg\\ \\min_\\limits{\\theta}\\ \\min_\\limits{\\phi}L_D^*\\\\ \\end{align}Train the encoder 𝜃and discriminator 𝜙 to minimize $L_D^*$ $\\Rightarrow$ (c.f. training encoder and decodert o minimize reconstruction error) 经典的auto-encoder形式其实是一个特例。 一般的设计如下图所示： 训练的时候给定图片和编码向量，输出的为重构的误差，这里没有了positive example和negative example的影响，只是positive example的score(重构的错误率)越小越好。 More interpretable embeddingDiscrete Representation一般方法 要得到的编码是一个一维向量，每个向量都有很好的可解释性。 non differentiable : https://arxiv.org/pdf/1611.01144.pdf 处理不能微分的论文。 Binary vector相比one hot 更好，因为其参数相对更少，而且可以产生训练参数里没有的东西（泛化性能更强）。 Vector Quantized Variational Auto encoder (VQVAE) Codebook里面是一系列的向量，Codebook里的系列也是NN学习出来的。 得出的code vector 和Codebook里面的所有向量进行相似度的计算，取相似度最高的向量，进行decoder。(通过这样来实现离散化) 离散化的作用是保留声音里文字的部分，过滤掉背景噪声的因素。","link":"/2022/02/08/Auto-encoder/Auto-encoder%E7%9A%84%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9/"},{"title":"Deep Auto-Encoder基本理论","text":"hljs.initHighlightingOnLoad(); 本文主要从PCA进行进行类比得出auto-encoder，通过增加深度来提高auto-encoder的特性，以及其在文字处理、预训练参数和图片相似度比较上的应用。 Auto-encoder 编码器：将图片的信息压缩称为一个code，代表原图的一个精简的表示。 解码器：根据code来将原来的图片进行重构。 单个encoder，只有input没有output，是没有办法学习的。 因此实际了一个解码器来得到输出。 然后将encoder和decoder连接到一起，一起来学习。 Starting from PCA $W$为变换矩阵，PCA一种Linear method。 目标是使得输出的重构图片和输入的原图片之间的差距越小越好。 可以将$W$看成是一个encoder，将$W^T$看成是一个decoder，中间的code相当于是一个hidden layer。 encoder可以是一个NN，因此可以增加中间的层数。 Deep Auto-encoder 之前，Deep Auto-encoder很难训练，需要RBM进行layer的初始化才能训练的好一点。 Symmetric is not necessary，就是可以将encoder和decoder对应起来训练，但是这样其实是不必要的，直接训练两个NN即可。 相比PCA来说，通过增加深度可以很好的提高结果的清晰度。 下图为一个效果上的对比，将数据降维成2维 PCA得出的code很难讲这个数据的特征提取出来。 Deep Auto-encoder则可以明显的将这几个特征分开。 Text Retrieval(文字处理) Vector Space Model：将文章编码成高维的向量，对比cos上的相似度。 Bag-of-word：通过0和1的形式，来完成句子的取词和构建。还有一种是用[0,1]来表示重要性。但是无法学习到单词的语义。 通过deep auto-encoder学到的文档的信息有清晰的分类。 LSA则很难将不同的文档中的特征进行区分。 Auto-encoder-similar Image Search 将图片之间直接计算相似度，得出的结果和预期相差较大。 因此使用自编码器将每张图片进行编码，直接对编码计算相似度如图所示。 Auto-encoder - Pre-training DNN使用auto-encoder进行神经网络的初始化操作。 先用auto-encoder进行预训练，这里如左图所示，先将784维转换1000维再784维，看训练的重构的情况。(将低位转换成为高维需要很强的正则化[如L1使得但部分为0只有个别几维不为0]，否则会出现一个分块的单位矩阵那样) W= \\begin{pmatrix} 1 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 & 0 & \\cdots & 0 \\\\ 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 0 & 0 & \\cdots & 0 \\\\ \\end{pmatrix} 之后就将第1层进行固定，再次进行auto-encoder操作。 然后，重复上面的操作如图，再向下学习得到下一层。 最后一层直接随机初始化即可。 在大量数据进行训练的时候，上述的技术还是十分的重要的。 其他类型的auto-encoder 加入噪声后的auto-encoder，decode是跟原来没有噪声的数据进行对比的，不是最原始的结果。 Auto-encoder for CNN 目标是相同的，减小解码前和重构以后的差别。 相比于编码器的卷积和池化操作，解码器的操作为反卷积和反池化。 CNN-Unpooling Maxpooling是将一个kernel里去最大的一个。 Unpooling是保留原来卷积的位置信息，其他地方用0来补齐。但是keras是直接将值复制四份。 CNN-Deconvolution其实，deconvolution也是一种卷积。 一般的卷积是将多个值变为一个值，反卷积是将一个值通过乘以不同的权重变成多个值。 反卷积也可以等价为多个值转换为一个值，其实是等价的。 NEXT 将编码值进行解码，即可以得出如图二维code的分布，没有分布点的的地方解码以后的结果不是很好，有点分布的地方可以看到大概的数字。 通过二维的分布来判断那边是有值存在的，那边取样才可能出现结果。 在code上面加L2的正则化，保证在0的附近都是有值的。","link":"/2022/02/08/Auto-encoder/Deep-Auto-encoder%E5%9F%BA%E6%9C%AC%E7%90%86%E8%AE%BA/"},{"title":"Auto-Encoder (2021)","text":"hljs.initHighlightingOnLoad(); 本文主要从auto-encoder的Basic Idea of Auto-encoder、Feature Disentanglement、Discrete Latent Representation以及More Applications四个方面进行叙述。这里主要参考的时李宏毅2021年的新课。这里auto-encoder和 CycleGAN 和transformers机制有一定的相似之处，后续还要深入了解。 Self supervised Learning Framework 自监督学习：使用大量的资料，进行不用标注的学习任务，主要有填空题和预测下一句话。 将自监督模型做一点调整，用在下游的任务里。 BERT和GPT是主流的自监督学习的框架，但是之前的Auto-Encoder其实也可以算作是自监督学习的一种(因为其可以不要label)。 Basic Idea of Auto-encoder Auto-encoder的思想和Cycle GAN相当的类似，就是将输入的图片进行两次转换得到解码后的图片和原来的图片之间的差距越小越好。 中间编码的向量可以见Embedding、也可以叫Representation或者叫Code。 下游任务：将图片压缩后进行相关的处理。如More Dimension Reduction 技术上可以使用Auto-encoder。 为什么使用Auto-encoder这里举了神雕侠侣的例子，就是想说明3x3的图片其实受到图片自身的限制，内部是由一定规律的，可以将其进行压缩。抑或是武器是受到手臂活动范围的约束，不可能变化很多。 由于图片的自身的约束，可以简化为低维的状态。 auto-encoder is not a new ideas.assets/image-20220209095443614.png) 之前训练是使用受限玻尔兹曼机RBM来训练深度模型。 主要是利用RBM将每一层都训练好，之后拼接起来进行微调，继而完成深度模型的预训练。 但是2012年，Hinton发表文章，说明其实RBM技术没有太大的必要，于是进年来没有人再去使用RBM了。 De-noising Auto encoder 通过encoder和decoder来去掉噪声。 BERT模型其实思想上和De-noising Auto encoder相似。 Feature DisentanglementRepresentation includes information of different aspects 但是，这些信息是纠缠在一个向量里面的，我们不知道那些维里包含有上述信息的主要内容。 Feature Disentanglement：训练出来的auto-encoder,可以将信息按照维度进行分类，不同的维度代表则不同的信息。 主要的应用是 Application: Voice Conversion 之前是需要成对的声音讯号，就是让两个人读相同的句子(监督学习)。 现在A和B不需要将同样的句子和语言，就可以实现声音的转换。 提取出我自己的内容讯号和新垣结衣的声音讯号，结合起来完成Voice Conversion。 Discrete Latent RepresentationDiscrete Representation one-hot可以实现手写数据集Mnist 的分类，一共10维，每一维代表的是一维数据的。 Codebook里面是一系列的向量，Codebook里的系列也是NN从字典（大量训练集）中学到的。 得出的code vector 和Codebook里面的所有向量进行相似度的计算，取相似度最高的向量，进行decoder。 decoder的输入必须是Codebook里面的向量，这样来来实现离散化。 这里相似度的计算和attention机制相似。 在语音上面，codebook可以学习到最基本的发音。 Text as Representation将Embedding变成一段文字，这样一段文字有可能就成为文章摘要。 encoder和decoder要是transformer机制。 但是，实际train以后，encoder生成的code是人看不懂的，但是decoder就可以看懂。 因此，要增加一个判别器，来判别是不是人话。 主要的思想几就是CycleGAN的思想。 Tree as Embedding More ApplicationsGenerator With some modification, we have variational auto encoder (VAE). 这里在得到encoder的同时，也收获到一个生成器模型(decoder)。 Compression 做图片压缩，但是会有一定得失真。 Anomaly Detection(异常检测机制) 就是给了一些列的训练资料 ${x^1, x^2,x^3,\\cdots,x^N}$。 检测输入的$x$，是否和训练数据相类似。 相似的内涵是训练数据来定的。 信用卡消费检测 Fraud DetectionTraining data: credit card transactions, 𝑥: fraud or notRef: https://www.kaggle.com/ntnu-testimon/paysim1/homeRef: https://www.kaggle.com/mlg-ulb/creditcardfraud/home 网络的侵入检测 Network Intrusion DetectionTraining data: connection, 𝑥: attack or notRef: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html 癌症检测 Cancer DetectionTraining data: normal cells, 𝑥: cancer or notRef: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/home 异常检测的难点在于异常的资料相当少，因此不能看作二元分类器，相当于使用一元数据进行分类。 对于相似的数据，decoder可以顺利的还原图片数据。 对于异常的图片，decoder就很难还原，因为训练的时候根本没见过这样的情况，通过这种机制来实现检测。 根据重构的误差的损失来看是否异常。 More about Anomaly DetectionPart 1: https://youtu.be/gDp2LXGnVLQPart 2: https://youtu.be/cYrNjLxkoXsPart 3: https://youtu.be/ueDlm2FkCnwPart 4: https://youtu.be/XwkHOUPbc0QPart 5: https://youtu.be/Fh1xFBktRLQPart 6: https://youtu.be/LmFWzmn2rFYPart 7: https://youtu.be/6W8FqUGYyDo","link":"/2022/02/09/Auto-encoder/Auto-encoder-2021/"},{"title":"Pytorch配置和初识篇","text":"hljs.initHighlightingOnLoad(); 本文主要从Pytorch的环境配置和安装、编辑器的配置教程、python学习中法宝以及Pycharm 和 Jupyter 的对比四个方面进行介绍。主要的视频教程 。 Pytorch的环境配置和安装主要内容可以参考视频教程 这里主要记录如何查看GPU配置。 首先，可以打开任务管理器直接查看显卡的配置。 打开在cmd里输入nvidia-smi 来查看当前显卡的CUDA版本。 如果显示没有该命令，则可以参考以下教程。 总结，在选择显卡pytorch对应的显卡的配置时 如果无英伟达显卡:CUDA选择None； 如果有英伟达显卡:CUDA选择9.2 编辑器的配置教程这里主要以jupyter notebook为例，详细教程请参考以下视频。 首先，打开Anaconda Prompt，激活需要的环境。 之后，conda list来查看当前已经安装好的包。 然后，安装conda install nb_conda 最后，启动即可，cmd中输入jupyter notebook 测试安装。 12import torchtorch.cuda.is_available() 但是，这个操作我这边一直行不通，我选择直接修改kernel的.json文件。 python学习中法宝 dir()函数，能让我们知道工具箱以及工具箱中的分隔区有什么东西。 help()函数，能让我们知道每个工具是如何使用的，工具的使用方法。 实战演练： 以torch为例，我们来查找torch这个包下面包含的分区。 1234dir(torch.cuda.is_available)# 返回该函数下面的属性dir(torch.cuda.is_available())# 返回的是该函数返回值下面的属性，这里返回的是bool变量的属性 help的使用也是同理的。 返回是一个bool值，表示cuda是否可用。 如果加上括号就得出bool值得帮助。 Pycharm 和 Jupyter 的对比 python的文件是整体是一个块，如果一步出错编译以后再次运行是从头开始运行的。效率低下，每次都要重新来一次 优点：通用，传播方便，适用于大型项目 缺点：需要从头运行 python控制台，是一句是一个块，修改之后就是从出错的地方开始往下运行。代码的阅读性不好 其实可以多行输入，在输入完一句之后可以Ctrl+Enter，出现三个点，继续输入下一句。 优点：显示每个变量属性 缺点：不利于代码阅读及修改 jupyter可以自己定义代码块的大小，可以是一句，也可以是多句。功能介于二者之间 优点：利于代码的阅读和修改 缺点：环境需要配置","link":"/2022/02/10/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E9%85%8D%E7%BD%AE%E5%92%8C%E5%88%9D%E8%AF%86%E7%AF%87/"},{"title":"Pytorch数据处理和可视化篇","text":"hljs.initHighlightingOnLoad(); 本文主要从Pytorch数据加载的初识、数据集Dataset的代码实践、TensorBoard基础教程、transforms进行图片的预处理、Torchvision里数据集的使用以及DataLoader的使用五个大方面来介绍Pytorch在运行过程中数据的修饰和预处理过程。主要的视频教程 。 Pytorch数据加载的初识 数据：可以比作垃圾的海洋，从数据(垃圾)中获取由于的东西。 Dataset：取出其中的某一类有用的数据(垃圾)，并获取对应的label值。 Dateset的几种组织形式： 第一种是文件夹的名称为对应的label。 第二种是图片和文件夹分属在不同的文件夹中，label以.txt形式保存。 DataLoader：将数据进行打包和压缩，为后面的网络提供不同的数据形式。 Dataset类的使用： 1234567from torch.utils.data import Dataset# 导入dataset类help(Dataset)# 获取简易的把帮助Dataset??# 在jupyter里获取详细的帮助# 在pycharm里一般是按住ctrl再点击dataset dataset是一个抽象类，所有的子类都应该重写__getitme__方法，这个方法是用来获取数据集对应的label的。 也可以选择重写__len__用来获取数据的长度。 12345def __getitem__(self, index): raise NotImplementedErrordef __add__(self, other): return ConcatDataset([self, other]) getitem的作用：使得数据集可以支持下标的引用 len的作用：使得数据集类可以通过len()函数来返回我们的长度 数据集Dataset的代码实践图片的读取12345# 测试图片的属性from PIL import Imageimg_path=&quot;flower_photos\\\\daisy\\\\5673551_01d1ea993e_n.jpg&quot;img.size # 测试证明读取数据成功img.show() # 展示图片 路径的读取12345678910import os# 用来读取文件夹地址的包dir_path=&quot;flower_photos/roses&quot;img_path_list=os.listdir(dir_path)# 读取指定的文件夹下的文件名，以list的形式读取文件名root_dir=&quot;flower_photos&quot;label_dir=&quot;roses&quot;path=os.path.join(root_dir,label_dir)# 该函数可以按照不同的系统来分配适当的连接符# 对于windows 用\\\\连接 对于linux使用一般的 / 连接 构建数据集类1234567891011121314151617class MyData(Dataset): def __init__(self, root_dir, label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir, self.label_dir) self.img_path = os.listdir(self.path) # 得到文件下面所有图片的名称，组成一个列表 def __getitem__(self, idx): img_name = self.img_path[idx] img_item = os.path.join(self.root_dir, self.label_dir, img_name) img = Image.open(img_item) label = self.label_dir return img, label def __len__(self): return len(self.img_path) 带双下划线的是属性函数，是自动调用的函数，不需要显式的调用。如__len__()起作用是当我们调用len(MyData)时。 测试数据集的测试123456789# 图片的读取root_dir = &quot;flower_photos&quot;label_list = os.listdir(root_dir)label_dir = label_list[1]flower_set = MyData(root_dir, label_dir)# flower_set对象其实是一个图像集，靠索引来读取数据img, label = flower_set[1]img.show() __getitem__起作用时我们以索引的方式调用数据集对象时。 程序的输出如下图所示： 1234567# 数据集的拼接roses_dir = label_list[3]daisy_dir = label_list[0]rose_set = MyData(root_dir, roses_dir)daisy_set = MyData(root_dir, daisy_dir)flower_set = rose_set + daisy_setprint(len(rose_set), len(daisy_set), len(flower_set)) 通过加号可以直接实现两个数据集的合并。 以下代码创建第二类，标签和图片分里的数据集的代码(和本实验不同，大概读懂意思即可，可以作为模板来套用)： 12345678910root_dir = &quot;dataset/train&quot;target_dir = &quot;ants_image&quot;img_path = os.listdir(os.path.join(root_dir，arget_dir))label = target_dir.split('_')[0]# 相当于去除ants_image的第一个字antsout_dir = &quot;ants_label&quot;for i in img_path: file_name = i.split(&quot;.jpg&quot;)[0] with open(os.path.join(root_dir,out_dir,&quot;{}.txt&quot;.format(file_name)),'w') as f; f.write(label) TensorBoard基础教程TensorBoard的安装tensorboard起初只能tensorflow才能使用，后来pytorch1.1之后也可以使用了。 SummaryWriter类的使用TensorBoard的帮助文档如下图所示： 主要是想log_dir中写内容的事件文件，主要的参数是前两个参数，后面的参数直接使用默认值就好了。 1234567891011121314Examples::from torch.utils.tensorboard import SummaryWriter# create a summary writer with automatically generated folder name.writer = SummaryWriter()# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/# create a summary writer using the specified folder name.writer = SummaryWriter(&quot;my_experiment&quot;)# folder location: my_experiment# create a summary writer with comment appended.writer = SummaryWriter(comment=&quot;LR_0.1_BATCH_16&quot;)# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/ 这里初始化类的主要过程，可以直接初始化，此时文件保存在默认的路径下面，location: runs/May04_22-14-54_s-MacBook-Pro.local/下。 也可以设置保存的路径为my_experiment ，此时文件保存在my_experiment下。 pycharm 和 jupyter中的注释使用ctrl+/ texstdio中的注释是ctrl+t add_scalar()的使用add_scalar()经常用来绘制train/val_loss。 tag：表示的是图表的数值。 scalar_value：表示需要去保存的数值的规模。(Y轴的规模) global_step：表示需要保存多少步。(X轴的规模) 1234567from torch.utils.tensorboard import SummaryWriterwriter = SummaryWriter(&quot;logs&quot;)# 将事件文件保存在logs文件下# y = xfor i in range(100): writer.add_scalar(tag=&quot;y=x&quot;, scalar_value=i, global_step=i)writer.close() tensorboard的打开，这里的logdir=事件文件所在文件夹名,切记不可以有中文字符 12345tensorboard --logdir=logs# logdir=事件文件所在文件夹名# 不可以有中文字符# 有时实验室服务器的6006端口比较紧张，可以自行设计打开的端口tensorboard --logdir=logs --port=6007 tensorboard打不开的原因：浏览器的问题，换成原始的Edge就没问题了。 如果tag=“ ”忘记改名，就可能出现如下多个内容画到了同一图上面的情况。 add_image()的使用 其他的参数大概都是和add_scalar相同，但是图片的数据类型要注意，这里img_tensor要是tensor类型和numpy.array类型的。 如图所示，PIL读取的数据类型不符合要求，因此这里使用opencv来读取图片数据，opencv读取得到的类型是array类型的。 也可以直接将PIL的类型转换为arrry。 但是，这里要注意，输入图片的维度也是有要求的，默认是(3,H,W)类型的，如果要改成(3,H,W)需要修改dataformats='CHW'。 12345678910111213141516from torch.utils.tensorboard import SummaryWriterimport numpy as npimg = np.zeros((3, 100, 100))img[0] = np.arange(0, 10000).reshape(100, 100) / 10000img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000img_HWC = np.zeros((100, 100, 3))img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000writer = SummaryWriter()writer.add_image('my_image', img, 0)# If you have non-default dimension setting, set the dataformats argument.writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')writer.close() 123456789101112131415from torch.utils.tensorboard import SummaryWriterimport numpy as npfrom PIL import Imagewriter = SummaryWriter(&quot;logs&quot;)# 将事件文件保存在logs文件下img_path = &quot;flower_photos/roses/145862135_ab710de93c_n.jpg&quot;img_PIL = Image.open(img_path)img_array = np.array(img_PIL)print(type(img_array))# 是np.array类型，但是数据的维度排列不是很匹配print(img_array.shape)writer.add_image(&quot;roses&quot;, img_array, 1, dataformats='HWC')writer.close() 从PIL到numpy，需要在add_image()中指定shape中每一个数字/维表示的含义 如果tag不变，所有的图像都会画在同一个tag的下面，注意部署一定要修改，不然就会直接覆盖，通过左右滑动来切换。 transforms进行图片的预处理 transforms的结构和用法 整理，在pycharm中打开文档，利用structure来分析器结构。 快捷键设置：到setting-&gt;keymap-&gt;搜索目标关键字即可 小贴士：alt+7可以调出structure的栏目，主要可以用来显示我们的函数里所包含的类（功能）和变量 该包相当于一个工具的模具（模板） （ctrl+p用来显示数该函数的变量信息） tensor数据类型整理通过tensorforms.ToTensor()去理解tensors的数据类型的问题。 这里的__call__代表的是传入的参数应该的什么类型。 1234567891011from torchvision.transforms import transformsfrom PIL import Imageimg_path = &quot;flower_photos/tulips/10791227_7168491604.jpg&quot;img = Image.open(img_path)# 使用Totensortensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)print(tensor_img) 这里Totensor()是一个类，需要实例化才能使用。 以下为我们转移到交互窗口的进行查看过程的数据。tensor的数据类型一共包含以下数据的属性和类型。 pycharm 出现问题时，使用alt+enter可以弹出相应的解决方案。 常见的transforms ToTensor()的使用 1234567891011from torchvision.transforms import transformsfrom PIL import Imageimg_path = &quot;flower_photos/tulips/10791227_7168491604.jpg&quot;img = Image.open(img_path)# 使用Totensortensor_trans = transforms.ToTensor()tensor_img = tensor_trans(img)print(tensor_img) Normalized()的用法 求图片张量的每一个channel的平均值和标准差。 1234trans_norm = transforms.Normalize([.5, .5, .5], [.5, .5, .5])img_norm = trans_norm(tensor_img)writer.add_image(&quot;Normalized&quot;, img_norm) 公式主要的作用相当于是改变数据分布的范围。 $output[channel] = \\dfrac {input[channel] - mean[channel]} {std[channel]}$ PyCharm小技巧设置:忽略大小写，进行提示匹配一般情况下，你需要输入R，才能提示出Resize 我们想设置，即便你输入的是r，也能提示出Resize 也就是忽略了大小写进行匹配提示。 Resize()的作用 主要的作用是按照指定的长宽来缩放图片。 如果输入(h,w)，则将图片变为指定的 (h,w) 如果输入 int a (小于最短的边)，则将图片的最小边变为a，长边按照比例来缩放。如果大于最长的边，图片最长边为a，最短边就按比例缩放。 参数必须为一个图片PIL，不可以是一个tensor，因此在compose里排列的顺序一定要先Resize后Totensor。 代码如下： 123456789101112# Reszie# 注意先后的顺序，先Reszie，后ToTensor# img PIL -&gt; resize -&gt; img_resize PILtrans_resize = transforms.Resize((512, 512))img_resize = trans_resize(img)# img_resize PIL -&gt; totensor -&gt; img_resizetensortensor_trans = transforms.ToTensor()img_resize = tensor_trans(img_resize)writer.add_image(&quot;Reszie&quot;, img_resize)# ------------------------------------------------- Compose()的使用 这个类的主要作用是将不同的transforms类进行合并操作。 Compose()中的参数需要是一个列表Python中，列表的表示形式为[数据1，数据2，…] 在Compose中，数据需要是transforms类型所以得到，Compose([transforms参数1, transforms参数2, …]) 1234567891011# Compose# 注意先后的顺序，先Reszie，后ToTensortrans_resize = transforms.Resize(512)tensor_trans = transforms.ToTensor()trans_compose = transforms.Compose([trans_resize, tensor_trans])# 也可以不用实例化，直接生成trans_compose = transforms.Compose([ transforms.Resize(512), transforms.ToTensor()])img_resize_2 = trans_compose(img) RandomCrop的用法 主要的作用：对图像进行随机裁剪 参数：size的如果是序列(h,w)，则将图片变为指定的 (h,w)，如果输入 int a (小于最短的边)，则将图片的最小边变为a，长边按照比例来缩放。如果大于最长的边，图片最长边为a，最短边就按比例缩放。 123456# Randomcroptrans_crop = transforms.RandomCrop(100, 80)trans_compose1 = transforms.Compose([trans_crop, tensor_trans])for i in range(10): img_crop = trans_compose1(img) writer.add_image(&quot;RandomCrop100,80&quot;, img_crop, i) python__call__的用法1234567891011121314class Person: def __call__(self, name): print(&quot;__call__&quot;+&quot;HELLO &quot;+name) def hello(self, name): print(&quot;hello &quot;+name)person = Person()person(&quot;Tom&quot;)person.hello(&quot;Tom&quot;)# output:&gt;&gt;&gt; __call__HELLO Tom&gt;&gt;&gt; hello Tom call的意思其实就是调用，使用对象加()即可作为一个函数来调用，是一个内置的方法。 hello()是对象配置的一个属性，只能用.hello()来显式调用。 总结：首先，注意输入和输出；其次，关注官方文档；然后，关注这个方法需要的参数。 Torchvision里数据集的使用这里Torchvision是主要服务计算机视觉的包，主要提供一些数据集和模型来以及一些预训练号的参数来方便训练。 CIFARCLASStorchvision.datasets.CIFAR10(root: str, train: bool = True, transform:Optional[Callable] = None, target_transform: Optional[Callable] = None, download:bool = False)[SOURCE] CIFAR10 Dataset的参数： root (string) – Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. 主要是保存文件夹的位置。 train (bool, optional) – If True, creates dataset from training set, otherwise creates from test set.是否为训练集True代表是训练集。 transform (callable**, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop使用的transforms的格式。 target_transform (callable**, optional) – A function/transform that takes in the target and transforms it. download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.是否下载。 这里如果下载速度过慢，可以使用迅雷进行加速。 12345678910111213141516171819202122232425262728import torchvisionfrom torch.utils.tensorboard import SummaryWriterdataset_trans = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()])train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=dataset_trans, download=True)test_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=False, transform=dataset_trans, download=True)# 读取测试img, target = train_set[0]print(train_set[0])# 转换字典的键值idx_classes = {}idx = 0for name in train_set.class_to_idx: idx_classes[idx] = name idx = idx + 1print(idx_classes[target])# tensorboard可视化writer = SummaryWriter(&quot;p10&quot;)for i in range(10): img, target = test_set[i] writer.add_image(&quot;test&quot;, img, i)writer.close() 源代码里有相关的下载链接。 DataLoader的使用dataset相当于一组牌，dataloader相当于如何抽一组牌。 torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=2, persistent_workers=False)[SOURCE]** Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See torch.utils.data documentation page for more details. Parameters主要参数： dataset (Dataset) – dataset from which to load the data.之前我们自己创建或者导入的数据集。 batch_size (int, optional) – how many samples per batch to load (default: 1).每一批取到的数据量，每一批的数据量越大越好。 shuffle (bool, optional) – set to True to have the data reshuffled at every epoch (default: False).是否拉乱顺序，对于训练集，最好打乱顺序 sampler (Sampler or Iterable**, optional) – defines the strategy to draw samples from the dataset. Can be any Iterable with __len__ implemented. If specified, shuffle must not be specified. batch_sampler (Sampler or Iterable**, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)进程数，但是windows只能设为0 collate_fn (callable**, optional) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset. pin_memory (bool, optional) – If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below. drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) 每次按照批量来存取，最后不能除尽的几张是否舍去。 timeout (numeric**, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) worker_init_fn (callable**, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None) generator (torch.Generator, optional) – If not None, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate base_seed for workers. (default: None) prefetch_factor (int, optional**, keyword-only arg) – Number of samples loaded in advance by each worker. 2 means there will be a total of 2 * num_workers samples prefetched across all workers. (default: 2) persistent_workers (bool, optional) – If True, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: False) dataloader是将img和target分别进行打包，在dim=0上增加一个维度。 12345678910111213141516171819202122232425262728293031323334import torchvisionfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoaderdataset_trans = torchvision.transforms.Compose([ torchvision.transforms.ToTensor()])train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;,train=True,transform=dataset_trans,download=True)test_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;,train=False,transform=dataset_trans,download=True)## 设置DataLoader类test_loader = DataLoader(dataset=test_set, batch_size=4, num_workers=0, drop_last=False, shuffle=False)# 测试数据中的每一张图片及targetimg, target = test_set[0]print(img.shape)print(target)# 测试dataloaderfor data in test_loader: imgs, targets = data print(imgs.shape) print(targets)&gt;&gt;&gt;output:torch.Size([3, 32, 32])3torch.Size([4, 3, 32, 32])tensor([3, 8, 8, 0]) 这里我们通过tensorboard来测试其效果 12345678910writer = SummaryWriter(&quot;dataloader&quot;)# 测试dataloaderfor epoch in range(2): for idx, data in enumerate(test_loader): imgs, targets = data # print(imgs.shape) # print(targets) # 由于每次输出为多张图片，这里将其平铺起来 writer.add_images(&quot;Epoch:{}&quot;.format(epoch), imgs, idx)writer.close() 这里，我们选择留下未除尽的内容，shuffle=False时，二者一模一样。反之，两次就不相同了。","link":"/2022/02/10/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/pytorch%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E7%AF%87/"},{"title":"DCGAN 的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要根据视频教程来搭建的一个卷积对抗网路DCGAN模型，相比之前的网络是将线性层换成卷积层和反卷积层，数据集只要需用的是Mnist手写数据集，主要从模型搭建、参数设置和模型训练三个方面进行讲解。 原论文要点 为了保证网络的稳定性，DCGAN去掉了最大池化层和平均池化层。 再判别器和生成器中使用了BatchNorm。 去掉的全连接隐藏层。 在生成器中使用ReLU激活函数，除了最后一个输出层。 在判别器中使用LeakyReLU激活函数。 导入相关包12345678import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torchvision.datasets as datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterimport torchvision.transforms as transforms 模型搭建判别器1234567891011121314151617181920212223242526272829303132333435363738class Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 nn.Sigmoid(), ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x): return self.disc(x) 主要的区别是将线性连接换成了卷积，其他不变。 生成器123456789101112131415161718192021222324252627282930313233343536class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x) 这里生成器主要使用的是反卷积操作。 初始化和测试1234567891011121314151617181920def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02)def test(): N, in_channels, H, W = 8, 3, 64, 64 z_dim = 100 x = torch.randn((N, in_channels, H, W)) disc = Discriminator(in_channels, 8) initialize_weights(disc) tmp = disc(x).shape == (N, 1, 1, 1) assert tmp gen = Generator(z_dim, in_channels, 8) z = torch.randn((N, z_dim, 1, 1)) tmp = gen(z).shape == (N, in_channels, H, W) assert tmp print(&quot;Success&quot;) 参数设置超参数设计1234567891011121314# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 2e-4# 3e-4 是对于Adam 来说最好的学习率z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 128num_epoch = 50channels_img = 1features_disc = 64features_gen = 64 features_disc、features_gen分别为判别器和生成器卷积层中的特征通道数。 channels_img = 1数据的通道数。 数据初始化123456789101112131415161718transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True) 初始化模型123disc = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) 损失函数与与优化器的初始化123456789# 设置优化器opt_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))# 可以考虑设置betas=的参数opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数criterion = nn.BCELoss() 模型训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869writer_fake = SummaryWriter(f&quot;runs/DCGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/DCGAN_MNIST/real&quot;)step = 0# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size batch_size = real.shape[0] # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) disc_real = disc(real).reshape(-1) # view()也可以 lossD_real = criterion(disc_real, torch.ones_like(disc_real)) disc_fake = disc(fake).reshape(-1) # view()也可以 lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_fake + lossD_real) / 2 # 初始化梯度 disc.zero_grad() # 反向传播 lossD.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc(fake).view(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 lossG = criterion(output, torch.ones_like(output)) gen.zero_grad() lossG.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)}\\ &quot; f&quot;Loss D: {lossD:.4f}, Loss G: {lossG:.4f}&quot; ) with torch.no_grad(): fake = gen(fixed_noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 这里主要和Simple GAN的区别是，数据的维度上的处理，之前是将图片展成一组向量，而且batch_size也有一定的改变。","link":"/2022/02/11/Generative%20Adversarial%20Networks/DCGAN-%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Simple NN-GAN的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要根据视频教程来搭建的一个只有线性层的简单神经网络，数据集只要需用的是Mnist手写数据集，主要从模型搭建、参数设置和模型训练三个方面进行讲解。 导入相关包12345678import torchimport torch.nn as nnimport torch.optim as optimimport torchvisionimport torchvision.datasets as datasetsfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterimport torchvision.transforms as transforms 模型搭建判别器12345678910111213141516class Discriminator(nn.Module): def __init__(self, img_dim): super().__init__() self.disc = nn.Sequential( nn.Linear(in_features=img_dim, out_features=128), nn.LeakyReLU(0.1), # 在GAN里面LeakyReLU的效果比ReLU好 nn.Linear(128, 1), nn.Sigmoid(), # 因为输出是0和1的布尔值 # 因此这里的激活函数选用Sigmoid ) def forward(self, x): return self.disc(x) 这里使用的是最简单的现象模型来搭建的，基本的框架就是这样。 在GAN里面激活函数LeakyReLU的效果比ReLU好。 因为输出是0和1的布尔值， 因此这里的激活函数选用Sigmoid。 生成器1234567891011121314151617class Generator(nn.Module): def __init__(self, z_dim, img_dim): # z_dim 生成噪声的维度 # img_dim 图片的维度 super().__init__() self.gen = nn.Sequential( nn.Linear(z_dim, 256), nn.LeakyReLU(0.1), nn.Linear(256, img_dim), # 28x28x1 -&gt; 784 nn.Tanh() # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def forward(self, x): return self.gen(x) 参数设置超参数设计1234567891011121314151617181920# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 3e-4# lr=1e-4# 3e-4 是对于Adam 来说最好的学习率z_dim = 64# 同样的可以尝试 128, 256image_dim = 28 * 28 * 1 # 784# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 32num_epoch = 50transforms = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数]) 超参数里最基本有敏感度的是学习率，lr=3e-4 是对于Adam 来说最好的学习率，有时候也会用lr=1e-4。 其次就是标准化参数的影响是最大的。 然后再考虑生成噪声数据的维度、每一批次的数据量batch_size(根据自己的内存来考虑，量力而行) 最后考虑总的循环数，一般是迭代次数越多越好。 实例化模型123disc = Discriminator(image_dim).to(device)gen = Generator(z_dim, image_dim).to(device)fixed_noise = torch.randn((batch_size, z_dim)).to(device) 数据预处理、损失函数和优化器123456789101112131415161718dataset = datasets.MNIST(root=&quot;/dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)# 设置优化器opt_disc = optim.Adam(disc.parameters(), lr=lr)# 可以考虑设置betas=的参数opt_gen = optim.Adam(gen.parameters(), lr=lr)# 损失函数criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/GAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/GAN_MNIST/real&quot;) GAN的优化器选择的是带有动量的Adam优化器。 损失函数选择的是二元交叉熵函数。 Parameters weight (Tensor, optional) – a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.权重参数。 size_average (bool, optional) – Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) – Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string**, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean' 模型训练1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465step = 0# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.view(-1, 784).to(device) # 将图片后两维展平 第一维为batch_size batch_size = real.shape[0] # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn(batch_size, z_dim).to(device) fake = gen(noise) # G(z) disc_real = disc(real).view(-1) lossD_real = criterion(disc_real, torch.ones_like(disc_real)) disc_fake = disc(fake).view(-1) lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_fake + lossD_real) / 2 # 初始化梯度 disc.zero_grad() # 反向传播 lossD.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max log(D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc(fake).view(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 lossG = criterion(output, torch.ones_like(output)) gen.zero_grad() lossG.backward() opt_gen.step() if batch_idx == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] \\ &quot; f&quot;Loss D: {lossD:.4f} ,Loss G: {lossG:.4f}&quot; ) with torch.no_grad(): fake = gen(fixed_noise).reshape(-1, 1, 28, 28) data = real.reshape(-1, 1, 28, 28) img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( data, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 对于判别器来说，其目标函数为： \\max \\{\\log(D(x_{real})) + \\log(1-D(G(z))\\}\\\\ \\Leftrightarrow \\min \\{ -\\log(D(x_{real})) - \\log(1-D(G(z))\\} 这里我们选择的损失函数是BCLoss： L(x,y) = -[y\\log(x) + (1-y)\\log(1-x)]\\\\ L(x,y)= \\begin{cases} -\\log(x), & if\\quad y=1 \\\\ -\\log(1-x), & if\\quad y=0 \\end{cases}因此这里选择torch.ones_like()和torch.zeros_like()来作为y参数。 对于生成器来说，其目标函数为： \\min \\log(1-D(G(z)))由于训练开始时的梯度较小，可能会导致生成器训练不动，生成器过早收敛。于是将目标函数转换为： \\min -\\log(D(G(z))) 同时生成器中的D时固定的，利用的是上一轮中D的参数，因此这里要将其固定： 12345# 方法1:保持动态图不变lossD.backward(retain_grap=True)# 方法2:保持计算出来的fake&lt;-&gt;G(z)保留fake = gen(noise)disc_fake = disc(fake.detach()).view(-1) detach()返回一个新的tensor，是从当前计算图中分离下来的，但是仍指向原变量的存放位置，其grad_fn=None且requires_grad=False，得到的这个tensor永远不需要计算其梯度，不具有梯度grad，即使之后重新将它的requires_grad置为true,它也不会具有梯度grad。 注意：返回的tensor和原始的tensor共享同一内存数据。in-place函数修改会在两个tensor上同时体现(因为它们共享内存数据)，此时当要对其调用backward()时可能会导致错误。 测试是将生成器作用到一个固定的噪声数据中。 1fake = gen(fixed_noise).reshape(-1, 1, 28, 28) 这里是将数据拼接成为网格形状 1img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)","link":"/2022/02/11/Generative%20Adversarial%20Networks/Simple-NN-GAN%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"WGAN-GP的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要介绍了WGAN-GP的理论和pytorch实现，主要的相比之前WGAN修改了生成器的归一化层、新加入了一些超参数、增加了新的计算梯度惩罚项的函数、去掉了原来的损失函数并在模型训练中定义了新的损失函数。 WGAN-GP理论简介WGAN-GP是WGAN的改进版，因为截断操作一方面浪费时间，其次效果实在一般，还不如DCGAN，因此这里带有梯度惩罚项的函数来进行优化操作。 这里的主要参数如下： $\\lambda=10$ 是惩罚项的系数，其代表的是梯度惩罚项前面的权重。 $\\alpha=1e-4$ 是学习率，$\\beta_1=0 \\ ,\\beta_2=0.9$ 是Adam优化器的动量参数。 每个循环里生成器训练一次对应的判别器训练的次数$n_{critic} = 5$ $\\hat{\\boldsymbol{x}} \\leftarrow \\epsilon \\boldsymbol{x}+(1-\\epsilon) \\tilde{\\boldsymbol{x}}$ 代表的是计算真实图片和生成图片的加权值。 $\\lambda\\left(\\left|\\nabla_{\\hat{\\boldsymbol{x}}} D_{w}(\\hat{\\boldsymbol{x}})\\right|_{2}-1\\right)^{2}$ 之后将加权值求范数，如果范数值为1，则此时满足$1-Lipschitz$ 稳定性。否则，作为惩罚项加入到损失函数中。 生长器也是使用Adam优化器，其他的地方不变。 模型搭建生成器和判别器这里使用的是将生成器的BatchNorm2d换成了InstanceNorm2d，关于InstanceNorm2d的参考CSDN博文 。 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import torchimport torch.nn as nnclass Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 # nn.Sigmoid(), 判别器取消了Sigmoid() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), # 整理修改为InstanceNorm2d # LayerNorm &lt;-&gt; InstanceNorm nn.InstanceNorm2d(out_channels, affine=True), nn.LeakyReLU(0.2), ) def forward(self, x): return self.disc(x)class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x)def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02) 梯度惩罚项函数为了求梯度惩罚，这里增加了求梯度惩罚项的函数如下： 1234567891011121314151617181920212223242526import torchimport torch.nndef gradient_penalty(critic, real, fake, device=&quot;cpu&quot;): batch_size, channels, H, W = real.shape epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, channels, H, W).to(device) interpolated_images = real * epsilon + fake * (1 - epsilon) # 计算判别器输出 mix_scores = critic(interpolated_images) # 求导 gradient = torch.autograd.grad( inputs=interpolated_images, outputs=mix_scores, grad_outputs=torch.ones_like(mix_scores), create_graph=True, retain_graph=True, )[0] gradient = gradient.view(gradient.shape[0], -1) gradient_norm = gradient.norm(2, dim=1) # 求2-范数 gradient_penalty_value = torch.mean((gradient_norm - 1) ** 2) return gradient_penalty_value 参数设计超参数的设计这里参数选择参考理论简介部分。以下为具体的代码： 123456789101112131415161718# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 1e-4# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 6channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5# weight_clip = 0.01LAMBEDA_GP = 10 优化器和损失函数的设置注意WGAN的损失函数都是自己写的，因此不设置损失函数了。 123456789101112# 设置优化器opt_disc = optim.RMSprop(disc_critic.parameters(), lr=lr)opt_gen = optim.RMSprop(gen.parameters(), lr=lr)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/WGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/WGAN_MNIST/real&quot;) 模型初始化和数据预处理这边也保持不变。 12345678910111213141516171819202122232425transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen) 模型训练主要的修改是给生成器去掉的参数截断功能，损失函数加入了求梯度惩罚项的部分，具体原理参考理论简介。 损失函数： L^{(i)} \\leftarrow D_{w}(\\tilde{\\boldsymbol{x}})-D_{w}(\\boldsymbol{x})+\\lambda\\left(\\left\\|\\nabla_{\\hat{\\boldsymbol{x}}} D_{w}(\\hat{\\boldsymbol{x}})\\right\\|_{2}-1\\right)^{2}主要的代码如下： 12345678910noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)fake = gen(noise) # G(z)critic_real = disc_critic(real).reshape(-1)critic_fake = disc_critic(fake).reshape(-1)gp = gradient_penalty(disc_critic, real, fake, device=device)# 这里自定义了损失函数# (f-GAN论文讲了，其实损失函数的设置对结果的影响不大)loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp) 完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) critic_real = disc_critic(real).reshape(-1) critic_fake = disc_critic(fake).reshape(-1) gp = gradient_penalty(disc_critic, real, fake, device=device) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp ) # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(fixed_noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train() 相比WGAN，加入梯度惩罚项以后的效果还是比较好的。","link":"/2022/02/12/Generative%20Adversarial%20Networks/WGAN-GP%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"WGAN的搭建","text":"hljs.initHighlightingOnLoad(); 本文主要介绍了WGAN的理论和pytorch实现，主要的相比之前DCGAN修改了模型搭建部分的激活函数、新加入了一些超参数、去掉了原来的损失函数并在模型训练中定义了损失函数。 WGAN理论简介这里有时间去复习李宏毅的WGAN相关理论，这里直接从代码部分讲起。 参考公式如图所示： 学习率$\\alpha$=5e-5, 截断系数c=0.01, 每一批的数据batch_size=64, 以及每个循环里生成器训练一次对应的判别器训练的次数$n_{critic} = 5$ 这里的 $\\theta$ 代表生成器的参数，$\\omega$ 代表判别器的参数。循环结束的条件是生成器的参数 $\\theta$ 是否收敛。 为了保证数学上的具有$1-Lipschitz$ 连续性(保证函数是光滑的)， 于是增加了截断误差，就是将判别器的参数 $\\omega$ 裁剪到一个小的范围之内，[-0.01, 0.01]。 优化器使用的是RMSProp优化器，替代了原来的Adma优化器。 模型搭建模型的结构是基本不变的，判别器的最后一层去掉了Sigmoid()函数。 其他的地方均和DCGAN相同。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import torchimport torch.nn as nnclass Discriminator(nn.Module): def __init__(self, channel_img, features_d): super(Discriminator, self).__init__() self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img, out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 # nn.Sigmoid(), 判别器取消了Sigmoid() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x): return self.disc(x)class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.gen = nn.Sequential( # Input: N x z_dim x 1 x 1 self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x): return self.gen(x)def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, mean=0.0, std=.02) 参数设置超参数的设置这里参数选择参考理论简介部分。以下为具体的代码： 1234567891011121314151617# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 5e-5# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 6channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5weight_clip = 0.05 优化器和损失函数的设置注意WGAN的损失函数都是自己写的，因此不设置损失函数了。 123456789101112# 设置优化器opt_disc = optim.RMSprop(disc_critic.parameters(), lr=lr)opt_gen = optim.RMSprop(gen.parameters(), lr=lr)fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/WGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/WGAN_MNIST/real&quot;) 模型初始化和数据预处理这边也保持不变。 12345678910111213141516171819202122232425transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc).to(device)gen = Generator(z_dim, channels_img, features_gen).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen) 模型训练 这边也不变，注意这里有BN层，需要加model.train() 函数。 损失函数： 判别器： g_{w} \\leftarrow \\nabla_{w}\\left[\\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(x^{(i)}\\right)-\\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(g_{\\theta}\\left(z^{(i)}\\right)\\right)\\right]\\\\ w \\leftarrow w+\\alpha \\cdot \\mathrm{RMSProp}\\left(w, g_{w}\\right)​ 生成器： g_{\\theta} \\leftarrow \\nabla_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} f_{w}\\left(g_{\\theta}\\left(z^{(i)}\\right)\\right)12345678noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)fake = gen(noise) # G(z)critic_real = disc_critic(real).reshape(-1)critic_fake = disc_critic(fake).reshape(-1)# 这里自定义了损失函数# (f-GAN论文讲了，其实损失函数的设置对结果的影响不大)loss_critic = - (torch.mean(critic_real) - torch.mean(critic_fake))# - 负号为了保证最小化 这里损失函数为判别器计算出来的数值取平均，即可。 训练的顺序是每次先训练$n_{critic}$次判别器，之后再训练1次生成器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise) # G(z) critic_real = disc_critic(real).reshape(-1) critic_fake = disc_critic(fake).reshape(-1) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = - (torch.mean(critic_real) - torch.mean(critic_fake)) # - 负号为了保证最小化 # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # 参数进行截断 for p in disc_critic.parameters(): p.data.clamp_(-weight_clip, weight_clip) # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(noise) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train() 总结： 其实WGAN的效果在一般的Mnist数据集上的效果不是很好，因此尽管数学上的效果很好，但是试验证实的效果还是不理想。","link":"/2022/02/12/Generative%20Adversarial%20Networks/WGAN%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Condition_WGAN_GP的搭建","text":"hljs.initHighlightingOnLoad(); 这个模型和ACGAN有相似之处，都增加了标签的embedding层，用来辅助训练。这样可以实现训练的结果和原来标签实现一一对应。 本文主要是在WGAN-GP的基础上修改，主要增加了和label与embedding相关的参数，并修改了模型。但是训练的效果不是很满意，收敛的速度不如WGAN-GP。 模型搭建 在模型搭建上，该模型主要增加了label的embedding层，并将embedding层和原来的输入层进行堆叠。 判别器 增加了embedding层： 增加了与labels和embedding层相关的参数： 前向传播函数增加了embedding的内容： 完整代码如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Discriminator(nn.Module): def __init__(self, channel_img, features_d, num_classes, img_size): super(Discriminator, self).__init__() self.img_size = img_size self.disc = nn.Sequential( # Input: N x channels_img x 64 x 64 nn.Conv2d( in_channels=channel_img+1, # 由于增加了一个编码层 因此要加1 out_channels=features_d, kernel_size=4, stride=2, padding=1 ), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d * 2, 4, 2, 1), # 16 x 16 self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8 self._block(features_d * 4, features_d * 8, 4, 2, 1), # 4 x 4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 nn.Sigmoid(), ) # 增加一个编码器 将一个label编码为一个图片 self.embed = nn.Embedding( num_embeddings=num_classes, embedding_dim=img_size*img_size ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.LeakyReLU(0.2), nn.BatchNorm2d(out_channels), ) def forward(self, x, labels): # label -&gt; [batch_size, 1] -&gt; N x 1 embedding = self.embed(labels).view([labels.shape[0], 1, self.img_size, self.img_size]) # embedding -&gt; N x 1 x img_size(64) x img_size x = torch.cat([x, embedding], dim=1) # x -&gt; N x channel_img x img_size(64) x img_size # dim=1 代表在第二维上进行堆叠 就是在channel上堆叠 return self.disc(x) 生成器 embed_size：编码器将num_classes维转换为embed_size。 新增embedding层： 修改前向传播函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size): # z_dim 生成噪声的维度 # channels_img 图片的维度 super(Generator, self).__init__() self.img_size = img_size self.gen = nn.Sequential( # Input: N x (z_dim + embed_size) x 1 x 1 self._block(z_dim + embed_size, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g*8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g*4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g*2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, kernel_size=4, stride=2, padding=1 ), # 64 x 64 nn.Tanh(), # [-1, 1] # 由于mnist数据集的特点，我们输入标准化后的数据是[-1,1] # 最后的输出要在[-1,1]之间 ) # 编码的维度要跟噪声的维度相同 self.embed = nn.Embedding(num_embeddings=num_classes, embedding_dim=embed_size) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU(), ) def forward(self, x, labels): embedding = self.embed(labels).unsqueeze(2).unsqueeze(3) # unsqueeze()的作用是增加维度，这里增加了第3，4维 # embedding -&gt; N x embed_size x 1 x 1 # x -&gt; N x z_dim x 1 x 1 x = torch.cat([x, embedding], dim=1) return self.gen(x) 梯度惩罚项增加了和label相关的参数即可。 12345678910111213141516171819202122def gradient_penalty(critic, real, labels, fake, device=&quot;cpu&quot;): batch_size, channels, H, W = real.shape epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, channels, H, W).to(device) interpolated_images = real * epsilon + fake * (1 - epsilon) # 计算判别器输出 mix_scores = critic(interpolated_images, labels) # 求导 gradient = torch.autograd.grad( inputs=interpolated_images, outputs=mix_scores, grad_outputs=torch.ones_like(mix_scores), create_graph=True, retain_graph=True, )[0] gradient = gradient.view(gradient.shape[0], -1) gradient_norm = gradient.norm(2, dim=1) # 求2-范数 gradient_penalty_value = torch.mean((gradient_norm - 1) ** 2) return gradient_penalty_value 参数设置123# 新增编码参数num_classes = 10gen_embedding = 100 其他参数和原来相似，完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# Hyperparameters etc (超参数)device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;lr = 5e-5# 也可以使用两组学习率，一个是生成器的、一个是判别器的。z_dim = 64# 同样的可以尝试 128, 256image_size = 64# gan对于超参数相当的敏感，# 换一个其他的参数可能就not work了batch_size = 64num_epoch = 3channels_img = 1features_disc = 64features_gen = 64# WGAN特有的参数critic_iteration = 5# weight_clip = 0.01LAMBEDA_GP = 10# 新增编码参数num_classes = 10gen_embedding = 100transforms = transforms.Compose([ transforms.Resize(image_size), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)] ) # 自适应我们的通道数，几个通道都为0.5 # transforms.Normalize((0.1307,), (0.3081,)) # 参数谷歌的参数，计算mnist的偏差值 # 但是这组参数可能会导致不收敛，因此我们换用原来的参数])dataset = datasets.MNIST(root=&quot;./dataset&quot;, transform=transforms, download=True)loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)disc_critic = Discriminator(channels_img, features_disc, num_classes, image_size).to(device)gen = Generator(z_dim, channels_img, features_gen, num_classes, image_size, gen_embedding).to(device)# 初始化模型initialize_weights(disc_critic)initialize_weights(gen)# 设置优化器opt_disc = optim.Adam(disc_critic.parameters(), lr=lr, betas=(0.0, 0.9))opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.9))fixed_noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)# 损失函数# criterion = nn.BCELoss()# Binary Cross Entropywriter_fake = SummaryWriter(f&quot;runs/CWGAN_MNIST/fake&quot;)writer_real = SummaryWriter(f&quot;runs/CWGAN_MNIST/real&quot;) 模型训练 主要的变化是在增加了和label相关的参数，其他部分不变。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778step = 0gen.train()disc_critic.train()# trainfor epoch in range(num_epoch): for batch_idx, (real, labels) in enumerate(loader): real = real.to(device) labels = labels.to(device) # 将图片后两维展平 第一维为batch_size # batch_size = real.shape[0] for _ in range(critic_iteration): # ------------------------------------------------------------ # Train Discriminator: # max {log(D(real)) + log(1 - D(G(Z))} # 等价于 min{-log(D(real)) - log(1 - D(G(Z))} noise = torch.randn((batch_size, z_dim, 1, 1)).to(device) fake = gen(noise, labels) # G(z) critic_real = disc_critic(real, labels).reshape(-1) critic_fake = disc_critic(fake, labels).reshape(-1) gp = gradient_penalty(disc_critic, real, labels, fake, device=device) # 这里自定义了损失函数 # (f-GAN论文讲了，其实损失函数的设置对结果的影响不大) loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBEDA_GP*gp ) # 初始化梯度 disc_critic.zero_grad() # 反向传播 loss_critic.backward(retain_graph=True) # 优化器优化 opt_disc.step() # ------------------------------------------------------------ # Train Generator # min log(1 - D(G(z)) -&gt; min -log(D(G(z)) # -&gt; max D(G(z)) # 开始的时候会因为梯度饱和训练不动 # 因此换了损失函数 # 我们在训练生成器时还想用之前判别器的梯度和参数 # 这里有两种办法 # lossD.backward(retain_grap=True) # disc_fake = disc(fake.detach()).view(-1) output = disc_critic(fake, labels).reshape(-1) # 由于上面retain_graph=True, 因此相当于此时的判别器时固定的 loss_gen = - torch.mean(output) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f&quot;Epoch [{epoch}/{num_epoch}] Batch {batch_idx}/{len(loader)} &quot; f&quot;loss_critic: {loss_critic:.4f}, loss_gen: {loss_gen:.4f}&quot; ) gen.eval() disc_critic.eval() with torch.no_grad(): fake = gen(noise, labels) # 每次测试的噪声是不变的，都是将一组固定的噪声转换的fake 假的图片 img_grid_fake = torchvision.utils.make_grid( fake, normalize=True) img_grid_real = torchvision.utils.make_grid( real, normalize=True) writer_fake.add_image( &quot;Mnist Fake Images&quot;, img_grid_fake, global_step=step ) writer_real.add_image( &quot;Mnist real Images&quot;, img_grid_real, global_step=step ) step = step + 1 gen.train() disc_critic.train()","link":"/2022/02/12/Generative%20Adversarial%20Networks/Condition-WGAN-GP%E7%9A%84%E6%90%AD%E5%BB%BA/"},{"title":"Kalman Filter小项目","text":"hljs.initHighlightingOnLoad(); 本文是通过一个位置判断的项目，来测试kalman filter的简单使用，了解其在opencv中的具体实现。 实现的简单的kalman filter123456789101112class KalmanFilter: kf = cv2.KalmanFilter(4, 2) kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32) kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) def predict(self, coordX, coordY): ''' This function estimates the position of the object''' measured = np.array([[np.float32(coordX)], [np.float32(coordY)]]) self.kf.correct(measured) predicted = self.kf.predict() x, y = int(predicted[0]), int(predicted[1]) return x, y 这个kalman滤波器主要是用来预测位置的。 绘制坐标点并预测 1234567891011121314img = cv2.imread(&quot;Pysource Kalman filter/blue_background.webp&quot;)ball_positions = [(50, 100), (100, 100), (150, 100), (200, 100), (250, 100), (300, 100), (350, 100)]for pt in ball_positions: cv2.circle(img, pt, 15, (0, 20, 220), -1) # 参数 绘图对象 位置 大小 颜色BGR 粗细 -1代表实心 # 绘制预测点 predicted = kf.predict(pt[0], pt[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4)cv2.imshow(&quot;Img&quot;, img)cv2.waitKey(0) 1234567891011121314# 根据上述值向后预测10代for i in range(10): predicted = kf.predict(predicted[0], predicted[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4) print(predicted)&gt;&gt;&gt;(550, 99)(600, 98)(649, 97)(698, 96)(747, 95)(796, 94)(845, 92)(894, 90)(942, 88) 123456789101112131415ball2_positions = [(4, 300), (61, 256), (116, 214), (170, 180), (225, 148), (279, 120), (332, 97), (383, 80), (434, 66), (484, 55), (535, 49), (586, 49), (634, 50),(683, 58), (731, 69), (778, 82), (824, 101), (870, 124), (917, 148),(962, 169), (1006, 212), (1051, 249), (1093, 290)]for pt in ball2_positions: cv2.circle(img, pt, 15, (0, 20, 220), -1) # 参数 绘图对象 位置 大小 颜色BGR 粗细 -1代表实心 # 绘制预测点 predicted = kf.predict(pt[0], pt[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4)# 根据上述值向后预测10代for i in range(10): predicted = kf.predict(predicted[0], predicted[1]) cv2.circle(img, predicted, 15, (20, 220, 0), 4) print(predicted) 同理，预测第二个球的轨迹。 orange轨迹预测12# 创建橘子检测器od = OrangeDetector() 1234# 实时的检测orange的轨迹orange_bbbox = od.detect(frame)x1, y1, x2, y2 = orange_bbboxcv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4) 之后我们将矩形框转换为中心点 12345678# 视频播完就退出orange_bbbox = od.detect(frame)x1, y1, x2, y2 = orange_bbbox# cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4)# 确定orange的中心cx = int((x1 + x2) / 2)cy = int((y1 + y2) / 2)cv2.circle(frame, (cx, cy), 20, (0, 0, 255), -1) 以上是根据目标检测得到的实时orange的位置，之后我们来用kalman filter预测Orange位置。 123# 滤波器预测下一时刻的位置predicted = kf.predict(cx, cy)cv2.circle(frame, predicted, 20, (220, 20, 0), 5) 完整代码： 1234567891011121314151617181920212223242526272829303132333435import cv2from orange_detector import OrangeDetectorfrom kalmanfilter import KalmanFilter# 读取视频cap = cv2.VideoCapture(&quot;Pysource Kalman filter/orange.mp4&quot;)# cap = cv2.VideoCapture(0)# 创建橘子检测器od = OrangeDetector()# 创建kfkf = KalmanFilter()while True: ret, frame = cap.read() if ret is False: break # 视频播完就退出 orange_bbbox = od.detect(frame) x1, y1, x2, y2 = orange_bbbox # cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 4) # 确定orange的中心 cx = int((x1 + x2) / 2) cy = int((y1 + y2) / 2) cv2.circle(frame, (cx, cy), 20, (0, 0, 255), -1) # 滤波器预测下一时刻的位置 predicted = kf.predict(cx, cy) cv2.circle(frame, predicted, 20, (220, 20, 0), 5) # print(orange_bbbox) cv2.imshow(&quot;Frame&quot;, frame) key = cv2.waitKey(50) if key == 27: break","link":"/2022/02/13/opencv/kalman-filter%E5%B0%8F%E9%A1%B9%E7%9B%AE/"},{"title":"车流统计项目","text":"hljs.initHighlightingOnLoad(); 本项目主要是来识别某路段的车流数量，主要分为目标识别和车流数量统计两个部分，对于车流数量计算部分还是需要很强的算法基础，在写的过程中不断会出现新的问题，需要一步一步的调试。但是该项目还有一定的BUG，还有待改进。 目标识别12345678910from object_detection import ObjectDetection# 初始化目标检测od = ObjectDetection()# Detect objects on frame(class_ids, scores, boxes) = od.detect(frame)for box in boxes: (x, y, w, h) = box cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2) 车流数量识别识别两帧中的相同车辆下面要判断连续两帧中的汽车是否为同一个汽车，来避免重复计数。 这里通过连续绘制车辆运行过程的中心点来进行识别是否为同一辆车： 1234567891011for box in boxes: (x, y, w, h) = box # print(&quot;FRAME N°&quot;, count, &quot; &quot;, x, y, w, h) cx = int((x + x + w) / 2) cy = int((y + y + h) / 2) center_point.append((cx, cy)) cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1) cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)for pt in center_point: cv2.circle(frame, pt, 5, (0, 0, 255), -1) 如上面的轨迹检测将会造成我们的中心点的数据越来有多，因此当我们判断好这两量车是同一辆是，可以释放掉同一量车前面时刻的路径。 12345&gt;&gt;&gt;CUR[(567, 905), (434, 750), (1724, 626), (930, 534), (857, 567), (1019, 650), (761, 655), (612, 474), (881, 473), (753, 463), (1867, 590), (641, 436), (687, 451), (943, 459), (1347, 984), (1114, 435), (1417, 465)]PRE[(571, 891), (437, 742), (1750, 635), (761, 648), (1018, 647), (928, 533), (856, 565), (612, 473), (881, 473), (1336, 980), (753, 463), (1118, 436), (942, 458), (1877, 605), (642, 435), (688, 450), (1268, 433), (1424, 466)] 以上为两次的输出结果，由此可以看出来有几对是一一对应的。 12345678910111213141516tracking_objects = {}# 检测车辆的序号track_id = 0for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 10: tracking_objects[track_id] = pt track_id += 1 # 再图中绘制出每一个图片对应的位置for object_id, pt in tracking_objects.items(): cv2.circle(frame, pt, 5, (0, 0, 255), -1) cv2.putText(frame, str(object_id), (pt[0], pt[1] - 7), 0, 1, (0, 0, 255), 2) 但是出现了以下异常的情况，一些车辆被识别但没有被统计。 原因可能是，近处的车移动量会大一点，造成违背计算在内。将distance &lt; 10 改为 distance &lt; 20. 统计车辆的id但是每张图的id都不相同，会不断的添加新的id，于是出现以下的BUG： 其原因是，第一次pre还没有数据，cur也是刚刚才有数据，于是给新的项目赋予了track_id， 下一次的比较又会给旧的目标赋予新的track_id, 于是出现以上错误。 第二次比较时，要将tracking_object的数据和新的目标点数据数据进行比较，代码如下： 123456789101112131415if count &lt;= 2: for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[track_id] = pt track_id += 1else: for pt in center_points_cur_frame: for object_id, pt2 in tracking_objects.items(): distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt 之后，如图所示没有出现新的问题，就是运行过程中的id掉了一地，没有跟着一起移动。 123456789101112131415161718for object_id, pt2 in tracking_objects.items(): object_exist = False # 判断这个对象是否存在 for pt in center_points_cur_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt object_exist = True # 已经证实器存在之后，就不用进行下面的操作了 # 就是跳出本次for循环，这样可以提高效率 if object_exist == False: # 如果这个id遍历了一圈还没有结果，那么就直接删除 tracking_objects.pop(object_id) # 但是，循环过程中时不可以随便的改变里面的元素的 # 因此要创建副本 接下来的问题就是循环过程中时不可以随便的改变里面的元素的，因此要创建副本，具体的操作如下，参与循环的是副本，进行改变的是真实参数： 下面我们为新识别出来的车辆增加标签。 完整代码以下是完整的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import cv2import numpy as npfrom object_detection import ObjectDetectionimport math# 初始化目标检测od = ObjectDetection()cap = cv2.VideoCapture(&quot;los_angeles.mp4&quot;)# 初始化计算器count = 0# center_point = []center_points_pre_frame = []tracking_objects = {}# 检测车辆的序号track_id = 0while True: ret, frame = cap.read() count += 1 if not ret: break # 当前窗口内的点 center_points_cur_frame = [] # Detect objects on frame (class_ids, scores, boxes) = od.detect(frame) for box in boxes: (x, y, w, h) = box # print(&quot;FRAME N°&quot;, count, &quot; &quot;, x, y, w, h) cx = int((x + x + w) / 2) cy = int((y + y + h) / 2) center_points_cur_frame.append((cx, cy)) cv2.circle(frame, (cx, cy), 5, (0, 0, 255), -1) cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2) if count &lt;= 2: for pt in center_points_cur_frame: for pt2 in center_points_pre_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[track_id] = pt track_id += 1 else: tracking_objects = tracking_objects.copy() center_points_cur_frame_copy = center_points_cur_frame.copy() for object_id, pt2 in tracking_objects.copy().items(): object_exist = False # 判断这个对象是否存在 for pt in center_points_cur_frame: distance = math.hypot(pt2[0] - pt[0], pt2[1] - pt[1]) if distance &lt; 20: tracking_objects[object_id] = pt object_exist = True # 已经证实器存在之后，就不用进行下面的操作了 # 就是跳出本次for循环，这样可以提高效率 if pt in center_points_cur_frame: center_points_cur_frame.remove(pt) # 如果存在，就是个旧的点，将其删去后就是新的点了 # 但是多次循环会造成一个点删除了多次 if object_exist == False: # 如果这个id遍历了一圈还没有结果，那么就直接删除 tracking_objects.pop(object_id) # 但是，再循环过程中时不可以随便的改变里面的元素的 # 因此要创建副本 # 增加新的点 for pt in center_points_cur_frame: tracking_objects[track_id] = pt track_id += 1 # 再图中绘制出每一个图片对应的位置 for object_id, pt in tracking_objects.items(): cv2.circle(frame, pt, 5, (0, 0, 255), -1) cv2.putText(frame, str(object_id), (pt[0], pt[1] - 7), 0, 1, (0, 0, 255), 2) print(&quot;CUR&quot;) print(center_points_cur_frame) print(&quot;PRE&quot;) print(center_points_pre_frame) # 保留上一帧中的数据 center_points_pre_frame = center_points_cur_frame.copy() cv2.imshow('Frame', frame) key = cv2.waitKey(1) if key == 27: breakcap.release()cv2.destroyAllWindows()","link":"/2022/02/13/opencv/%E8%BD%A6%E6%B5%81%E7%BB%9F%E8%AE%A1%E9%A1%B9%E7%9B%AE/"},{"title":"Object_detection_app项目","text":"hljs.initHighlightingOnLoad(); 本项目主要是利用yolov4目标检测网络，实现对固定目标的识别功能，最后将其打包形成.exe文件。但是，最后编译的过程还有一定的bug,还未能实现封装。 初始化相机12345678910cap = cv2.VideoCapture(0)# 修改相机参数 整理修改的是相机的frame的H,Wcap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)while True: # 如果不加while循环的话，生成的程序会一闪而过 ret, frame = cap.read() cv2.imshow(&quot;Frame&quot;, frame) cv2.waitKey(1) # 单位是ms # 等待时间设为0的话，视像头会会静止，关闭一次窗口才刷新一次 导入目标检测网络1234567# opencv DNNnet = cv2.dnn.readNet(&quot;object_detection_crash_course/dnn_model/yolov4-tiny.weights&quot;, &quot;object_detection_crash_course/dnn_model/yolov4-tiny.cfg&quot;)model = cv2.dnn_DetectionModel(net)# 导入模型model.setInputParams(size=(320, 320), scale=1/255)# size越大，清晰度越大，但是可以速度比较慢# scale的功能是缩放 [0,255]-&gt;[0,1] 导入类别文件123456# 导入类别文件classes=[]with open(&quot;object_detection_crash_course/dnn_model/classes.txt&quot;,&quot;r&quot;) as file_object: for class_name in file_object: class_name = class_name.strip() classes.append(class_name) 目标检测这部分的代码放到相加的whie循环中： 1234567891011121314151617# 目标检测class_idx, scores, bboxs = model.detect(frame)# 返回值依次是类别的idx， 所得的分数， boundingboxs边界框# print(f&quot;class_idx:{class_idx}\\n scores:{scores} \\n bboxs:{bboxs}&quot;)for class_id, score, bbox in zip(class_idx, scores, bboxs): (x, y, w, h) = bbox class_name = classes[class_id] # class_name = classes[class_id[0]] # 4.4.0的版本 # 绘制目标检测三角形, 在窗口对象上绘制 cv2.rectangle(frame, (x, y), (x+w, y+h), (200, 0, 50), 3) # 参数为绘制的对象, 绘制长方形的左上点合右下点, 颜色RGB, 框的粗细 # 识别的种类合coco数据集有关 cv2.putText(frame, str(class_name), (x, y-5), cv2.FONT_HERSHEY_PLAIN, fontScale=2, color=(200, 0, 50), thickness=2) 设计鼠标点击1234567891011121314151617181920button_person = Falsedef click_button(event, x, y, flags, params): global button_person if event == cv2.EVENT_LBUTTONDOWN: # print(x, y) polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) is_inside = cv2.pointPolygonTest(polygan, (x,y), False) if is_inside: print(&quot;inside the button&quot;) if button_person is False: button_person = True else: button_person = False # 相当于一个开关，点击以下开始识别，再点一下就关闭识别# 产生一个新的窗口cv2.namedWindow(&quot;Frame&quot;)cv2.setMouseCallback(&quot;Frame&quot;, click_button) 之后就可以来设计窗口的打开的开关： 绘制类别字体1234567# 创建按钮# 这里使用多边行来绘制polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]])cv2.fillPoly(frame, polygan, (0, 0, 200))# cv2.rectangle(frame, (20, 20), (220, 70), (0, 0, 200), -1)# 厚度为-1代表布满cv2.putText(frame, &quot;Person&quot;, (30, 60), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 3) 由于绘制字体类别和设计鼠标点击的逻辑的内容有些复杂，因此我们直接导入写好的文件。 设置按钮这里定义了一个Button类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import cv2import numpy as npclass Buttons: def __init__(self): # Font self.font = cv2.FONT_HERSHEY_PLAIN self.text_scale = 3 self.text_thick = 3 self.x_margin = 20 self.y_margin = 10 # Buttons self.buttons = {} self.button_index = 0 self.buttons_area = [] np.random.seed(0) self.colors = [] self.generate_random_colors() def generate_random_colors(self): for i in range(91): random_c = np.random.randint(256, size=3) self.colors.append((int(random_c[0]), int(random_c[1]), int(random_c[2]))) def add_button(self, text, x, y): # Get text size textsize = cv2.getTextSize(text, self.font, self.text_scale, self.text_thick)[0] right_x = x + (self.x_margin * 2) + textsize[0] bottom_y = y + (self.y_margin * 2) + textsize[1] self.buttons[self.button_index] = {&quot;text&quot;: text, &quot;position&quot;: [x, y, right_x, bottom_y], &quot;active&quot;: False} self.button_index += 1 def display_buttons(self, frame): for b_index, button_value in self.buttons.items(): button_text = button_value[&quot;text&quot;] (x, y, right_x, bottom_y) = button_value[&quot;position&quot;] active = button_value[&quot;active&quot;] if active: button_color = (0, 0, 200) text_color = (255, 255, 255) thickness = -1 else: button_color = (0, 0, 200) text_color = (0, 0, 200) thickness = 3 # Get text size cv2.rectangle(frame, (x, y), (right_x, bottom_y), button_color, thickness) cv2.putText(frame, button_text, (x + self.x_margin, bottom_y - self.y_margin), self.font, self.text_scale, text_color, self.text_thick) return frame def button_click(self, mouse_x, mouse_y): for b_index, button_value in self.buttons.items(): (x, y, right_x, bottom_y) = button_value[&quot;position&quot;] active = button_value[&quot;active&quot;] area = [(x, y), (right_x, y), (right_x, bottom_y), (x, bottom_y)] inside = cv2.pointPolygonTest(np.array(area, np.int32), (int(mouse_x), int(mouse_y)), False) if inside &gt; 0: print(&quot;IS Ac&quot;, active) new_status = False if active is True else True self.buttons[b_index][&quot;active&quot;] = new_status def active_buttons_list(self): active_list = [] for b_index, button_value in self.buttons.items(): active = button_value[&quot;active&quot;] text = button_value[&quot;text&quot;] if active: active_list.append(str(text).lower()) return active_list 之后开始实例化Button: 显示button： 设计点击激活操作： 完整的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import cv2import numpy as npfrom object_detection_crash_course.gui_buttons import Buttons# 初始化Buttonbutton = Buttons()button.add_button(&quot;person&quot;, 20, 20)button.add_button(&quot;cell phone&quot;, 20, 100)button.add_button(&quot;keyboard&quot;, 20, 180)button.add_button(&quot;remote&quot;, 20, 260)button.add_button(&quot;scissors&quot;, 20, 340)colors = button.colors# opencv DNNnet = cv2.dnn.readNet(&quot;object_detection_crash_course/dnn_model/yolov4-tiny.weights&quot;, &quot;object_detection_crash_course/dnn_model/yolov4-tiny.cfg&quot;)model = cv2.dnn_DetectionModel(net)# 导入模型model.setInputParams(size=(416, 416), scale=1/255)# size越大，清晰度越大，但是可以速度比较慢# scale的功能是缩放 [0,255]-&gt;[0,1]# 导入类别文件classes=[]with open(&quot;object_detection_crash_course/dnn_model/classes.txt&quot;,&quot;r&quot;) as file_object: for class_name in file_object: class_name = class_name.strip() classes.append(class_name)# 初始化相机cap = cv2.VideoCapture(0)# 修改相机参数 整理修改的是相机的frame的H,Wcap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)# FULL HD = 1920x1080# button_person = Falsedef click_button(event, x, y, flags, params): # global button_person if event == cv2.EVENT_LBUTTONDOWN: button.button_click(x, y) # print(x, y) # polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) # # is_inside = cv2.pointPolygonTest(polygan, (x,y), False) # if is_inside: # print(&quot;inside the button&quot;) # if button_person is False: # button_person = True # else: # button_person = False # # 相当于一个开关，点击以下开始识别，再点一下就关闭识别# 产生一个新的窗口cv2.namedWindow(&quot;Frame&quot;)cv2.setMouseCallback(&quot;Frame&quot;, click_button)while True: # 如果不加while循环的话，生成的程序会一闪而过 ret, frame = cap.read() # 导出激活的button active_button = button.active_buttons_list() # 目标检测 class_idx, scores, bboxs = model.detect(frame) # 返回值依次是类别的idx， 所得的分数， boundingboxs边界框 # print(f&quot;class_idx:{class_idx}\\n scores:{scores} \\n bboxs:{bboxs}&quot;) for class_id, score, bbox in zip(class_idx, scores, bboxs): (x, y, w, h) = bbox class_name = classes[class_id] # class_name = classes[class_id[0]] # 4.4.0的版本 color = colors[class_id] # 根据id分配颜色 # if button_person and class_name == &quot;person&quot;: if class_name in active_button: # 绘制目标检测三角形, 在窗口对象上绘制 cv2.rectangle(frame, (x, y), (x+w, y+h), color, 3) # 参数为绘制的对象, 绘制长方形的左上点合右下点, 颜色RGB, 框的粗细 # 识别的种类合coco数据集有关 cv2.putText(frame, str(class_name), (x, y-5), cv2.FONT_HERSHEY_PLAIN, fontScale=2, color=color, thickness=2) # # 创建按钮 # # 这里使用多边行来绘制 # polygan = np.array([[(20, 20), (220, 20), (220, 70), (20, 70)]]) # cv2.fillPoly(frame, polygan, (0, 0, 200)) # # cv2.rectangle(frame, (20, 20), (220, 70), (0, 0, 200), -1) # # 厚度为-1代表布满 # cv2.putText(frame, &quot;Person&quot;, (30, 60), cv2.FONT_HERSHEY_PLAIN, 3, (255, 255, 255), 3) # 显示Button button.display_buttons(frame) cv2.imshow(&quot;Frame&quot;, frame) cv2.waitKey(1) # 单位是ms # 等待时间设为0的话，视像头会会静止，关闭一次窗口才刷新一次 项目部署12345678import sysfrom cx_Freeze import setup, Executablesetup(name=&quot;Simple Object Detection Software&quot;, version=&quot;0.1&quot;, description=&quot;This software detects objects in realtime&quot;, executables=[Executable(&quot;main.py&quot;)] ) 结果出现了报错应该是cx_Freeze的问题，目前没解决。","link":"/2022/02/13/opencv/object-detection-app%E9%A1%B9%E7%9B%AE/"},{"title":"Pytorch中常用的高级操作与搭建完整流程","text":"hljs.initHighlightingOnLoad(); 本文主要从模型的保存和修改、如何导入和加载现有的模型、如何使用GPU以及完整的训练和测试过程进行介绍，主要的视频教程 。 本教程以官方文档为主，这里关注最多的是图像，因此可以使用跟图像处理相关的包，打开torchvision的文档。 现有网络的使用和修改打开torchvision文档，选择model可以得出我们的一些常用的模型，如图所示。 主要的模型涉及了图像分类、目标检测、语义识别和视频处理。 这里以分类模型的VGG为例，数据集使用的时CFIAR10 如果pretained=true则使用的时预训练好的参数，为False则直接重新初始化参数来训练。 progress(bool)代表是否显示预训练模型下载的进度条。 1234567import torchvisiontrain_data = torchvision.datasets.ImageNet(&quot;./data_image_net&quot;, split='train', download=True)# ../ 表示上级目录下导入 ./ 表示当前的目录下导入&gt;&gt;&gt;RuntimeError: The dataset is no longer publicly accessible. You need to download the archives externally and place them in the root directory. 目前这个数据集不适合公开的被下载，要自己手动下载，导入到对于的目录下，但是TM 147G也太大了，所以入门还是找一些小一点的数据集吧。 现有模型的使用下载的时候默认会下载到我们C盘中C:\\Users\\dell/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth 123456789101112131415161718192021# 导入网络的模型vgg16_pretrain = vgg16(pretrained=True)# 有预训练权重的,会下载一会，528vgg16_normal = vgg16(pretrained=False)# 没有权重，仅模型本身print(vgg16_pretrain)&gt;&gt;&gt;VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ...... (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) ...... (6): Linear(in_features=4096, out_features=1000, bias=True) )) 现有模型的修改模型层数的增加add_module() 1234567891011121314151617181920212223# 模型层的增加print(vgg16_normal.classifier[6])# 直接在模型最后增加vgg16_normal.add_module('linear1', nn.Linear(1000, 10))# 加入到最后一个模型块中vgg16_normal.classifier.add_module('linear0', nn.Linear(1000, 10))print(vgg16_normal)&gt;&gt;&gt; ( ......(classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) (1): ReLU(inplace=True) (2): Dropout(p=0.5, inplace=False) (3): Linear(in_features=4096, out_features=4096, bias=True) (4): ReLU(inplace=True) (5): Dropout(p=0.5, inplace=False) (6): Linear(in_features=4096, out_features=1000, bias=True) (linear0): Linear(in_features=1000, out_features=10, bias=True) ) (linear1): Linear(in_features=1000, out_features=10, bias=True)) 模型的修改 12345# -----------------------------------------------# 模型的修改vgg16_normal.classifier[7] = nn.Linear(1000, 256)vgg16_normal.linear0 = nn.Linear(256, 10)print(vgg16_normal) 注意： 在 Sequential里的模型，索引都是按照0、1、2、… 的索引来选取的，因此金国最后一层的名字为linear0，但是要求改时还是用数值7来索引vgg16_normal.classifier[7] 其次如果vgg16_normal.linear0不存在时，就会在模型的最后重新建立一个vgg16_normal.linear0的层。 至于Sequential()的一些操作，后期可以参考李沐的课程。 模型的保存和模型的加载这里将展示两种模型的保存和加载的方式，一种将网络的参数和结构全部保存下来，另一种只是保存模型的参数（我感觉模型的参数占用的内存应该达到90%以上，只保存参数感觉不怎么节省空间其实，不知道为什么官方推荐。） 保存和加载方式112345678910111213141516171819202122# 保存方式1，模型结构+模型参数vgg16_1 = vgg16(pretrained=False)torch.save(vgg16_1, &quot;vgg16_method1.pth&quot;)# 对应方式1的加载model = torch.load(&quot;vgg16_method1.pth&quot;)print(model)&gt;&gt;&gt;VGG( (features): Sequential( (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) ...... (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) ) (avgpool): AdaptiveAvgPool2d(output_size=(7, 7)) (classifier): Sequential( (0): Linear(in_features=25088, out_features=4096, bias=True) ...... (6): Linear(in_features=4096, out_features=1000, bias=True) )) 因为模型和权重的数据都保存了，打印的时候只是打印模型的结构，而不打印权重数据。 注意： 对于方式1，如果模型是自己定义，那么torch.load()时，如果模型定义的文件不是同一个文件，那么会报错。 解决的办法就是将我们我们自定义的模型和torch.load()放在用一个文件(这样有点蠢)，或者就是将定义的模型文件from model import *进来即可。 保存和加载方式2123456789# 保存方式2，模型参数(官方推荐)vgg16_2 = vgg16(pretrained=False)torch.save(vgg16_2.state_dict(), &quot;vgg16_method2.pth&quot;)# 对应方式1的加载model = torch.load(&quot;vgg16-397923af.pth&quot;)print(model)model = torch.load(&quot;vgg16_method2.pth&quot;)print(model) 官方带有预训练参数的模型和这种方法一样，都是采用的是保存参数而不是保存模型结构的方式。 直接torch.load()的话结果，打印出来都是参数字典的形式。 12345678# 对应方式2的加载# 先初始化模型vgg16_3 = vgg16(pretrained=False)# 获取参数存储dictdict_para = torch.load(&quot;vgg16-397923af.pth&quot;)# 将字典参数导入模型vgg16_3.load_state_dict(dict_para)print(vgg16_3) 完整的训练套路导入数据集1234567891011121314151617181920212223from torchvision.transforms import transformsfrom torchvision.datasets import CIFAR10# 准备数据集train_data = CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=False)test_data = CIFAR10(root=&quot;./root&quot;, train=False, transform=transforms.ToTensor(), download=False)# length长度train_data_size = len(train_data)test_data_size = len(test_data)# 打印信息print(&quot;训练数据集长度为：{}&quot;.format(train_data_size))print(&quot;测试数据集长度为：{}&quot;.format(test_data_size))# 利用DataLoader来加载数据库集train_loader = DataLoader(train_data, batch_size=64)test_loader = DataLoader(test_data, batch_size=64) 搭建网络1234567891011121314151617181920212223242526272829from torch import nnimport torch# 搭建神经网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(64*4*4, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)# 验证正确性if __name__ == '__main__': net = Net() inputs = torch.ones((64, 3, 32, 32)) outputs = net(inputs) print(outputs.shape) 在train.py中导入网络，创建网络模型： 123from model import Net# 创建网络模型net = Net() 优化器和损失函数123456# 损失函数loss_fn = nn.CrossEntropyLoss()# 优化器learning_rate = 1e-2optimizer = optim.SGD(net.parameters(), learning_rate) 基本的训练过程12345678910111213141516171819202122232425262728293031# 设置训练网络和一些参数# 记录训练的次数total_train_step = 0# 记录测试次数total_test_step = 0# 训练的轮数epoch = 10# 添加tensorboardwriter = SummaryWriter(&quot;train_logs&quot;)for i in range(epoch): print(&quot;----------第{}轮训练开始----------&quot;.format(i+1)) # 训练步骤开始 for data in train_loader: imgs, targets = data outputs = net(imgs) loss = loss_fn(outputs, targets) # 优化器优化模型 optimizer.zero_grad() loss.backward() optimizer.step() total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(&quot;训练次数：{}，Loss:{}&quot;.format(total_train_step, loss.item())) writer.add_scalar(&quot;train_loss&quot;, loss.item(), total_train_step) # loss -&gt; tensor([xxx]) # loss.item() -&gt; xxx 直接返回真实的数据 这里的loss.item()就是将tensor类型变量里具体的数字输出出来。 以上的代码是对模型进行训练，那么如何评价训练的好坏情况?——那就是直接在测试集合进行测试。 12345678910111213141516# 测试步骤开始total_test_loss = 0.0with torch.no_grad(): for data in test_loader: imgs, targets = data outputs = net(imgs) loss = loss_fn(outputs, targets) total_test_loss = total_test_loss + loss.item() print(&quot;整体测试集上的Loss:{}&quot;.format(total_test_loss)) writer.add_scalar(&quot;test_loss&quot;, total_test_loss, total_test_step) total_test_loss = total_test_step + 1 # 模型的保存 这里是将每轮的模型都进行保存 torch.save(net, &quot;net_{}.pth&quot;.format(i)) print(&quot;模型已经保存&quot;) 模型的优化过程对于目标检测和语义分割来说，可以直接使用测试误差来作为训练过程的评价指标。但是分类问题还有其他的方法，比如我们的outputs=[0.1, 0.2, 0.3, 0.4]其实是该图片属于各种情况的概率，而真实的输出可以是outputs=[3] 即为最后的判别类别的编号。 12345678910111213total_accuracy = 0for xxx ... ... accuracy = (outputs.argmax(1) == targets).sum() # 也可以使用max函数 pred = torch.max(outputs, dim=1)[1] accuracy0 = (pred == targets).sum() total_accuracy = total_accuracy + accuracy # total_accuracy = total_accuracy + accuracy0 print(&quot;整体测试数据的正确率为:{}&quot;.format(total_accuracy/len(test_loader))) 通过上述方法来实现输出准确率。 模型搭建的细节12net.train()net.eval() 由于我们的net是Net的实例化的对象，同时Net是继承nn.Module的一个子类。要去nn.Containers-&gt;nn.Module帮助文档里查阅。 其实，这两个层就是在有dropout和BatchNorm是才主要起作用。 使用GPU训练模型的方法方法一只要把对于的网络模型、数据(输入imgs、标注labels)和损失函数进行.cuda()，然后覆盖原来的即可。 12345678if torch.cuda.is_available(): # 修改模型 net = net.cuda() # 修改数据 imgs = imgs.cuda() targets = targets.cuda() # 修改损失函数 loss_fn = loss_fn.cuda() 土堆的提醒： colab的GPU免费版每周可以使用免费30小时。 对于colab,如果想要运行在terminal上的语言，需要加在命令前加！。 up测试结果，colab的速度是一般他自己电脑的4倍。运行一轮只要0.7s，而up的电脑要2.5s 但是上述方法不是很常用。 方法二这是比较常用的方法。 1234567891011121314151617# Device是torch.device类型的数据Device = torch.device(&quot;cpu&quot;)Device0 = torch.device(&quot;cuda&quot;)# 可以选择不同的显卡Device1 = torch.device(&quot;cuda:0&quot;)Device2 = torch.device(&quot;cuda:1&quot;)device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)# 导入到网络中net = net.to(Device)# 修改数据imgs = imgs.to(Device)targets = targets.to(Device)# 修改损失函数loss_fn = loss_fn.to(Device)# 对于未来和损失函数来说直接to(device)即可 其他与方法一相同，也是对网络、数据以及损失函数进行设置。 以下为我在字节电脑上训练的速度结果。 123456789101112131415161718-------------CPU------------------------第2轮训练开始----------消耗的时间为:0.21944713592529297训练次数：800，Loss:1.8308976888656616消耗的时间为:1.417208194732666训练次数：900，Loss:1.782684564590454消耗的时间为:2.627971887588501训练次数：1000，Loss:1.8661954402923584消耗的时间为:3.8746509552001953训练次数：1100，Loss:1.9508962631225586消耗的时间为:5.333733797073364训练次数：1200，Loss:1.6595017910003662消耗的时间为:6.635249376296997训练次数：1300，Loss:1.6205933094024658消耗的时间为:7.973670244216919训练次数：1400，Loss:1.7197237014770508消耗的时间为:9.35597276687622训练次数：1500，Loss:1.8009873628616333 以下是使用cpu训练的效果： 123456789101112131415161718192021-------------GPU------------------------第2轮训练开始----------消耗的时间为:1.0422132015228271训练次数：800，Loss:1.8765838146209717消耗的时间为:6.844691276550293训练次数：900，Loss:1.856195330619812消耗的时间为:12.637196063995361训练次数：1000，Loss:1.9804853200912476消耗的时间为:18.599247694015503训练次数：1100，Loss:1.9813154935836792消耗的时间为:24.597208261489868训练次数：1200，Loss:1.7226444482803345消耗的时间为:30.361780405044556训练次数：1300，Loss:1.6570385694503784消耗的时间为:36.181212425231934训练次数：1400，Loss:1.7347835302352905消耗的时间为:41.99465990066528训练次数：1500，Loss:1.8110555410385132整体测试集上的Loss:304.21112048625946整体测试数据的正确率为:19.764331817626953模型已经保存 以下是colab的cpu与GPU的效果： 完整的验证/测试(demo)步骤主要的核心就是使用已经训练的好的模型。 主要提醒： Resize()的特点：输入的图格式和输出的格式是一一对应的。 12345678910111213141516171819202122232425262728293031323334353637import torchfrom PIL import Imagefrom torchvision.transforms import transformsfrom model import Net# 导入图片image_path = &quot;&quot;image = Image.open(image_path)print(image)# 数据增强处理transform = transforms.Compose([ transforms.Resize((32, 32)), transforms.ToTensor(),])image = transform(image)image = torch.reshape(image, (1, 3, 32, 32))# 导入模型# 注意使用第一种模型保存时要将模型导入# from model import Net 即可torch.load(&quot;net_0.pth&quot;)# 第二种导入方法net = Net()net.load_state_dict(torch.load(&quot;net_0.pth&quot;))# 养成练好得到习惯net.eval()with torch.no_grad(): # 节约内存和使用性能 outputs = net(image) pred_label = outputs.argmax(1) # 输出预测的类别print(pred_label) 比较标准的步骤请见教程。 实际项目中的参数问题 可以将require的参数改成defalut的，先让代码跑起来。","link":"/2022/02/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C%E4%B8%8E%E6%90%AD%E5%BB%BA%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/"},{"title":"Pytorch基本网络的搭建过程","text":"hljs.initHighlightingOnLoad(); 本文主要从网络的搭建、损失函数和优化器三个方面进行介绍，并且以下教程主要还是出自于官方的文档，因此还是有时间多看看官方文档，主要的视频教程 。 神经网络基本骨架 本节主要讲的是nn.Containers内的工具。 nn.Module这是一个基类，所有的神经网络的搭建都需要去集成这个类，将整个类的__init__()和forward()进行重写即可。这里我们以官网上的一个教程为例。 123456789101112import torch.nn as nnimport torch.nn.functional as Fclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5) self.conv2 = nn.Conv2d(20, 20, 5) def forward(self, x): x = F.relu(self.conv1(x)) return F.relu(self.conv2(x)) 通过调试可以发现，进入类中第一步就会进入到我们的forward()函数中，进行运行，再通过return返回到类的外部。 卷积操作Conv2d()torch提供了两个一个是torch.nn一个是torch.nn.functional前者可以看作是后者的一个封装，调研起来会更方便，但是后者实现的更加基础，相当于前者的一些具体的实现。 这里是nn.Conv2d的帮助文档，相比之下比functional.conv2d的教程要详细很多。 123456789101112131415161718192021222324252627282930313233343536373839404142import torchimport torch.nn.functional as Finput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]])# 养成良好的习惯，将矩阵写成以上的格式kernel = torch.tensor([[1, 2, 1], [0, 1, 0], [2, 1, 0]])input = torch.reshape(input, (1, 1, 5, 5))kernel = torch.reshape(kernel, (1, 1, 3, 3))print(input.shape)print(kernel.shape)output = F.conv2d(input=input, weight=kernel, stride=1)print(output)output2 = F.conv2d(input=input, weight=kernel, stride=2)print(output2)output3 = F.conv2d(input=input, weight=kernel, stride=1, padding=1)print(output3)&gt;&gt;&gt;torch.Size([1, 1, 5, 5])torch.Size([1, 1, 3, 3])tensor([[[[10, 12, 12], [18, 16, 16], [13, 9, 3]]]])tensor([[[[10, 12], [13, 3]]]])tensor([[[[ 1, 3, 4, 10, 8], [ 5, 10, 12, 12, 6], [ 7, 18, 16, 16, 8], [11, 13, 9, 3, 4], [14, 13, 9, 7, 4]]]]) 以下主要来讲解的是nn.Conv2d()的操作。 Parameters参数（这里面最常用的是上面的是前五个） in_channels (int) – Number of channels in the input image 输入的通道数 out_channels (int) – Number of channels produced by the convolution 输出的通道数 kernel_size (int or tuple) – Size of the convolving kernel 可以自定义卷积核的大小，可以不是方形的结构。 stride (int or tuple, optional) – Stride of the convolution. 指的是卷积的步长 Default: 1 padding (int, tuple or str, optional) – Padding added to all four sides of the input. 是否再边缘进行填充Default: 0 padding_mode (string**, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1 groups (int, optional) – Number of blocked connections from input channels to output channels. 分组卷积，在ResNeXt 网络中多见到。 Default: 1 bias (bool, optional) – If True, adds a learnable bias to the output. Default: True 详细的理解可以参考Link kernel_size 是我们实际不断去学习的参数。 以下为输入和输出的形状。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import torchimport torchvisionimport torch.nn as nnfrom torch.nn import Conv2dfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformstrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)# 设置网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0) def forward(self, x): x = self.conv1(x) return xnet = Net()print(net)# 打印网络的相关的参数step = 0writer = SummaryWriter(&quot;conv&quot;)for data in dataloader: img, target = data output = net(img) # print(img.shape) # print(output.shape) # torch.Size([64, 3, 32, 32]) writer.add_images(&quot;input&quot;, img, step) # torch.Size([64, 6, 30, 30]) # tensorboard 是没有办法显示6 channel的图片的 output = torch.reshape(output, (-1, 3, 30, 30)) writer.add_images(&quot;output&quot;, output, step) step += 1 如图为卷积处理前后图片的对比情况。 池化层 Parameters kernel_size – the size of the window to take a max over stride – the stride of the window. Default value is kernel_size，默认是和kernel_size一样的。 padding – implicit zero padding to be added on both sides dilation – a parameter that controls the stride of elements in the window 空洞卷积。 return_indices – if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later ceil_mode – when True, will use ceil instead of floor to compute the output shape 取整方式，上取证还是下取整。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import torchimport torchvisionimport torch.nn as nnfrom torch.nn import MaxPool2dfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformsinput = torch.tensor([[1, 2, 0, 3, 1], [0, 1, 2, 3, 1], [1, 2, 1, 0, 0], [5, 2, 3, 1, 1], [2, 1, 0, 1, 1]], dtype=torch.float32)# 养成良好的习惯，将矩阵写成以上的格式# 设置网络class Net(nn.Module): def __init__(self, ceil_mode): super(Net, self).__init__() self.ceil_mode = ceil_mode self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=self.ceil_mode) def forward(self, x): output = self.maxpool1(x) return outputnet1 = Net(ceil_mode=True)net2 = Net(ceil_mode=False)print(net1)print(net2)# 打印网络的相关的参数input = torch.reshape(input, (-1, 1, 5, 5))output1 = net1(input)output2 = net2(input)print(&quot;ceil_mode=true\\n&quot;, output1)print(&quot;ceil_mode=False\\n&quot;, output2)&gt;&gt;&gt;ceil_mode=true tensor([[[[2., 3.], [5., 1.]]]])ceil_mode=False tensor([[[[2.]]]]) 最大池化是保留主要的图片特征，会变模糊，eg.1080P-&gt;720P 12345678910111213141516171819train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)step = 0writer = SummaryWriter(&quot;maxpooling&quot;)for data in dataloader: img, target = data output = net1(img) writer.add_images(&quot;input&quot;, img, step) writer.add_images(&quot;output&quot;, output, step) step += 1 这里padding层的作用跟padding是意义的，可以用0或者任意的一个常数去进行，用的比较少，此处不在详细介绍。 非线性激活函数比较常用的nn.ReLU(inplace=Flase)，nn.Sigmoid()。 inplace的作用其实就是是否在原来的位置进行替换。 默认inplace=False , 这样可以保留原始的数据不丢失。 以下我们使用Sigmoid激活函数为例，来观察处理前后的情况。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import torchimport torchvisionimport torch.nn as nnfrom torch.nn import ReLUfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformsinput = torch.tensor([[1, -0.5], [-1, 3]])input = torch.reshape(input, (-1, 1, 2, 2))class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.relu = ReLU() def forward(self, input): output = self.relu(input) return outputnet = Net()print(net)output = net(input)print(output)# 图片操作情况# ----------------------------------------------------train_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)class Net2(nn.Module): def __init__(self): super(Net2, self).__init__() self.sigmoid = nn.Sigmoid() def forward(self, input): output = self.sigmoid(input) return outputnet2 = Net2()print(net2)step = 0writer = SummaryWriter(&quot;Sigmoid&quot;)for data in dataloader: img, target = data output = net2(img) writer.add_images(&quot;input&quot;, img, step) writer.add_images(&quot;output&quot;, output, step) step += 1 线性层nn.Linear() 12345678910111213141516171819202122232425262728293031323334353637383940414243import torchimport torchvisionimport torch.nn as nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterfrom torchvision.transforms import transformstrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=64, num_workers=0, drop_last=False, shuffle=True)# 设置网络class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.linear1 = nn.Linear(in_features=196608, out_features=10) def forward(self, x): output = self.linear1(x) return outputnet = Net()step = 0writer = SummaryWriter(&quot;maxpooling&quot;)for data in dataloader: img, target = data print(img.shape) # output = torch.reshape(img, (1, 1, 1, -1)) output = torch.flatten(img) print(output.shape) output = net(output) print(output.shape) 使用torch.flatten()得到的结果为 123torch.Size([64, 3, 32, 32])torch.Size([196608])torch.Size([10]) 就是最后直接展平成为一维。 nn.Sequential()实战这里提供一个实际的例子，来展示nn.Sequential()的使用过程。 1234567891011121314151617181920212223242526272829303132333435import torchimport torch.nn as nnfrom torch.utils.tensorboard import SummaryWriterclass Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), # 默认式和kernel_size相等的 nn.Linear(1024, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)net = Net()print(net)input = torch.ones((64, 3, 32, 32))print(input.shape)output = net(input)writer = SummaryWriter(&quot;Sequential&quot;)writer.add_graph(net, input)writer.close() 如图所示，为使用tensorboard后的基本结果，其可以显示基本的网络连接和维度上的变化。 损失函数 损失函数根据定义来讲一定是越小越好。 Loss function 衡量的是实际计算和目标之间的差距。 为我们更新参数提供依据，即反向传播。 L1Loss以下以L1Loss来举例： 要重点注意的时输入与输出的Shape: Input: (), where means any number of dimensions. Target: (*), same shape as the input. Output: scalar. If reduction is 'none', then (*), same shape as the input. 12345678910111213141516import torchimport torch.nn as nnfrom torch.nn import L1Loss# 损失函数一般在torch.nn里inputs = torch.tensor([1, 2, 3], dtype=torch.float32)targets = torch.tensor([1, 2, 5], dtype=torch.float32)inputs = torch.reshape(inputs, (1, 1, 1, 3))targets = torch.reshape(targets, (1, 1, 1, 3))loss = L1Loss(reduction='mean')# reduction='mean' 时我们的inputs和targets要都是float类型才对result = loss(inputs, targets)print(result) MSELoss之后，我们以最小二乘法常用的MSELoss为例： 其主要的使用方法和上面的基本相同，这里不在赘述。 CrossEntropyLoss然后，我们以分类任务中比较常用的交叉熵损失函数为例： 以上公式和视频里的公式不同，因为这里将交叉熵softmax进行了内置处理。 Shape: Input: (N, C) where C = number of classes, or$ (N, C, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. 这里输入需要连两个参数，第二个参数为分类数(就是其所示类别的编号数) ； 如上面的例子，狗的分类数为1，C=[0, 1, 0] 这里要输入的必须为one-hot编码 Target:（N） If containing class indices, shape (N)where each value is $ 0 \\leq \\text{targets}[i] \\leq C-10$, or $(N, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. If containing class probabilities, same shape as the input. Output: If reduction is 'none', shape (N) or $(N, d_1, d_2, …, d_K)$ with $K \\geq 1$ in the case of K-dimensional loss. Otherwise, scalar. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import torch.nn as nnfrom torch.utils.tensorboard import SummaryWriterfrom torch.utils.data import DataLoaderfrom torchvision.transforms import transformsimport torchvisiontrain_set = torchvision.datasets.CIFAR10(root=&quot;./root&quot;, train=True, transform=transforms.ToTensor(), download=True)dataloader = DataLoader(dataset=train_set, batch_size=1, num_workers=0, drop_last=False, shuffle=True)class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.models = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(kernel_size=2, stride=2), nn.Flatten(), # 默认式和kernel_size相等的 nn.Linear(1024, 64), nn.Linear(64, 10), ) def forward(self, x): return self.models(x)net = Net()loss = nn.CrossEntropyLoss()for i, data in enumerate(dataloader): imgs, targets = data outputs = net(imgs) # print(outputs) # print(targets) result = loss(outputs, targets) print(result) result.backward() if i == 10: break 提供反向传播使其拥有grad参数。 优化器优化器的构件过程12optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)optimizer = optim.Adam([var1, var2], lr=0.0001) 优化器的使用过程123456for input, target in dataset: optimizer.zero_grad() output = model(input) loss = loss_fn(output, target) loss.backward() optimizer.step() 这里有很多优化器，不同的优化器所使用的参数是不同的，但是共有的参数是para=和 lr= (即要优化的参数和学习率)","link":"/2022/02/23/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/Pytorch%E5%9F%BA%E6%9C%AC%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA%E8%BF%87%E7%A8%8B/"}],"tags":[{"name":"Typora","slug":"Typora","link":"/tags/Typora/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"机械振动","slug":"机械振动","link":"/tags/%E6%9C%BA%E6%A2%B0%E6%8C%AF%E5%8A%A8/"},{"name":"Latex","slug":"Latex","link":"/tags/Latex/"},{"name":"图像分类","slug":"图像分类","link":"/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"},{"name":"控制理论","slug":"控制理论","link":"/tags/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"control theory","slug":"control-theory","link":"/tags/control-theory/"},{"name":"auto-encoder","slug":"auto-encoder","link":"/tags/auto-encoder/"},{"name":"生成模型","slug":"生成模型","link":"/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"},{"name":"无监督学习","slug":"无监督学习","link":"/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"Generative Adversarial Networks","slug":"Generative-Adversarial-Networks","link":"/tags/Generative-Adversarial-Networks/"},{"name":"GAN实战","slug":"GAN实战","link":"/tags/GAN%E5%AE%9E%E6%88%98/"},{"name":"opencv","slug":"opencv","link":"/tags/opencv/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"object detection","slug":"object-detection","link":"/tags/object-detection/"}],"categories":[{"name":"Typora","slug":"Typora","link":"/categories/Typora/"},{"name":"PyTorch教程与源码讲解","slug":"PyTorch教程与源码讲解","link":"/categories/PyTorch%E6%95%99%E7%A8%8B%E4%B8%8E%E6%BA%90%E7%A0%81%E8%AE%B2%E8%A7%A3/"},{"name":"振动力学","slug":"振动力学","link":"/categories/%E6%8C%AF%E5%8A%A8%E5%8A%9B%E5%AD%A6/"},{"name":"Latex","slug":"Latex","link":"/categories/Latex/"},{"name":"图像分类网络及Pytorch实现","slug":"图像分类网络及Pytorch实现","link":"/categories/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E7%BD%91%E7%BB%9C%E5%8F%8APytorch%E5%AE%9E%E7%8E%B0/"},{"name":"现代控制理论","slug":"现代控制理论","link":"/categories/%E7%8E%B0%E4%BB%A3%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"auto-encoder","slug":"auto-encoder","link":"/categories/auto-encoder/"},{"name":"Generative Adversarial Networks","slug":"Generative-Adversarial-Networks","link":"/categories/Generative-Adversarial-Networks/"},{"name":"opencv","slug":"opencv","link":"/categories/opencv/"}]}